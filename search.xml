<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[jstat命令查看jvm的GC情况]]></title>
    <url>%2F2020%2F05%2F15%2Fjstat%E5%91%BD%E4%BB%A4%E6%9F%A5%E7%9C%8Bjvm%E7%9A%84GC%E6%83%85%E5%86%B5%2F</url>
    <content type="text"><![CDATA[1、 jstat [-命令选项] [vmid] [间隔时间/毫秒] [查询次数]2、类加载统计123456jstat -class 9675Loaded:加载class的数量Bytes：所占用空间大小Unloaded：未加载数量Bytes:未加载占用空间Time：时间 3、编译统计1234567jstat -compiler 9675Compiled：编译数量。Failed：失败数量Invalid：不可用数量Time：时间FailedType：失败类型FailedMethod：失败的方法 4、 垃圾回收统计123456789101112131415161718jstat -gc 9675 1000 10 #每秒查询一次查询10次S0C：第一个幸存区的大小S1C：第二个幸存区的大小S0U：第一个幸存区的使用大小S1U：第二个幸存区的使用大小EC：伊甸园区的大小EU：伊甸园区的使用大小OC：老年代大小OU：老年代使用大小MC：方法区大小MU：方法区使用大小CCSC:压缩类空间大小CCSU:压缩类空间使用大小YGC：年轻代垃圾回收次数YGCT：年轻代垃圾回收消耗时间FGC：老年代垃圾回收次数FGCT：老年代垃圾回收消耗时间GCT：垃圾回收消耗总时间 5、堆内存统计12345678910111213141516171819jstat -gccapacity 9675NGCMN：新生代最小容量NGCMX：新生代最大容量NGC：当前新生代容量S0C：第一个幸存区大小S1C：第二个幸存区的大小EC：伊甸园区的大小OGCMN：老年代最小容量OGCMX：老年代最大容量OGC：当前老年代大小OC:当前老年代大小MCMN:最小元数据容量MCMX：最大元数据容量MC：当前元数据空间大小CCSMN：最小压缩类空间大小CCSMX：最大压缩类空间大小CCSC：当前压缩类空间大小YGC：年轻代gc次数FGC：老年代GC次数 6、新生代垃圾回收统计/老年代垃圾回收统计123456789101112131415161718192021222324jstat -gcnew 9675jstat -gcold 9675S0C：第一个幸存区大小S1C：第二个幸存区的大小S0U：第一个幸存区的使用大小S1U：第二个幸存区的使用大小TT:对象在新生代存活的次数MTT:对象在新生代存活的最大次数DSS:期望的幸存区大小EC：伊甸园区的大小EU：伊甸园区的使用大小YGC：年轻代垃圾回收次数YGCT：年轻代垃圾回收消耗时间MC：方法区大小MU：方法区使用大小CCSC:压缩类空间大小CCSU:压缩类空间使用大小OC：老年代大小OU：老年代使用大小YGC：年轻代垃圾回收次数FGC：老年代垃圾回收次数FGCT：老年代垃圾回收消耗时间GCT：垃圾回收消耗总时间 7、新生代内存统计/老年代内存统计12345678910111213141516171819202122jstat -gcnewcapacity 9675jstat -gcoldcapacity 9675NGCMN：新生代最小容量NGCMX：新生代最大容量NGC：当前新生代容量S0CMX：最大幸存1区大小S0C：当前幸存1区大小S1CMX：最大幸存2区大小S1C：当前幸存2区大小ECMX：最大伊甸园区大小EC：当前伊甸园区大小YGC：年轻代垃圾回收次数FGC：老年代回收次数OGCMN：老年代最小容量OGCMX：老年代最大容量OGC：当前老年代大小OC：老年代大小YGC：年轻代垃圾回收次数FGC：老年代垃圾回收次数FGCT：老年代垃圾回收消耗时间GCT：垃圾回收消耗总时间 8、元数据空间统计1234567891011jstat -gcmetacapacity 9675MCMN: 最小元数据容量MCMX：最大元数据容量MC：当前元数据空间大小CCSMN：最小压缩类空间大小CCSMX：最大压缩类空间大小CCSC：当前压缩类空间大小YGC：年轻代垃圾回收次数FGC：老年代垃圾回收次数FGCT：老年代垃圾回收消耗时间GCT：垃圾回收消耗总时间 参考 ：https://www.jianshu.com/p/beb59929bbfc 转载请注明出处，谢谢！]]></content>
      <categories>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx下后端节点realserverweb健康检测模块ngx_http_upstream_check_module]]></title>
    <url>%2F2020%2F04%2F21%2Fnginx%E4%B8%8B%E5%90%8E%E7%AB%AF%E8%8A%82%E7%82%B9realserverweb%E5%81%A5%E5%BA%B7%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9D%97ngx_http_upstream_check_module%2F</url>
    <content type="text"><![CDATA[目前，nginx对后端节点健康检查的方式主要有3种，这里列出： 1234561、ngx_http_proxy_module 模块和ngx_http_upstream_module模块（自带） 官网地址：https://nginx.org/cn/docs/http/ngx_http_proxy_module.html#proxy_next_upstream2、nginx_upstream_check_module模块 官网网址：https://github.com/yaoweibin/nginx_upstream_check_module3、ngx_http_healthcheck_module模块 官网网址：https://wiki.nginx.org/NginxHttpHealthcheckModule 公司业务线上对后端节点的健康检查是通过nginx_upstream_check_module模块做的，这里我将分别介绍这三种实现方式以及之间的差异性。 一、ngx_http_proxy_module 模块和ngx_http_upstream_module模块（自带）严格来说，nginx自带是没有针对负载均衡后端节点的健康检查的，但是可以通过默认自带的ngx_http_proxy_module 模块和ngx_http_upstream_module模块中的相关指令来完成当后端节点出现故障时，自动切换到健康节点来提供访问。 这里列出这两个模块中相关的指令：123语法: proxy_connect_timeout time;默认值: proxy_connect_timeout 60s;上下文: http, server, location ngx_http_proxy_module 模块中的 proxy_connect_timeout 指令、proxy_read_timeout指令和proxy_next_upstream指令. 设置与后端服务器建立连接的超时时间。应该注意这个超时一般不可能大于75秒。 123语法: proxy_read_timeout time;默认值: proxy_read_timeout 60s;上下文: http, server, location 定义从后端服务器读取响应的超时。此超时是指相邻两次读操作之间的最长时间间隔，而不是整个响应传输完成的最长时间。如果后端服务器在超时时间段内没有传输任何数据，连接将被关闭。123语法: proxy_next_upstream error | timeout | invalid_header | http_500 | http_502 | http_503 | http_504 |http_404 | off ...;默认值: proxy_next_upstream error timeout;上下文: http, server, location 指定在何种情况下一个失败的请求应该被发送到下一台后端服务器： 123456789error # 和后端服务器建立连接时，或者向后端服务器发送请求时，或者从后端服务器接收响应头时，出现错误timeout # 和后端服务器建立连接时，或者向后端服务器发送请求时，或者从后端服务器接收响应头时，出现超时invalid_header # 后端服务器返回空响应或者非法响应头http_500 # 后端服务器返回的响应状态码为500http_502 # 后端服务器返回的响应状态码为502http_503 # 后端服务器返回的响应状态码为503http_504 # 后端服务器返回的响应状态码为504http_404 # 后端服务器返回的响应状态码为404off # 停止将请求发送给下一台后端服务器 需要理解一点的是，只有在没有向客户端发送任何数据以前，将请求转给下一台后端服务器才是可行的。也就是说，如果在传输响应到客户端时出现错误或者超时，这类错误是不可能恢复的。 范例如下：123http &#123;proxy_next_upstream http_502 http_504 http_404 error timeout invalid_header;&#125; ngx_http_upstream_module模块中的server指令 123语法: server address [parameters];默认值: —上下文: upstream 范例如下：1234upstream name &#123; server 10.1.1.110:8080 max_fails=1 fail_timeout=10s; server 10.1.1.122:8080 max_fails=1 fail_timeout=10s;&#125; 下面是每个指令的介绍：123456max_fails=number # 设定Nginx与服务器通信的尝试失败的次数。在fail_timeout参数定义的时间段内，如果失败的次数达到此值，Nginx就认为服务器不可用。在下一个fail_timeout时间段，服务器不会再被尝试。 失败的尝试次数默认是1。设为0就会停止统计尝试次数，认为服务器是一直可用的。 你可以通过指令proxy_next_upstream、fastcgi_next_upstream和 memcached_next_upstream来配置什么是失败的尝试。 默认配置时，http_404状态不被认为是失败的尝试。fail_timeout=time # 设定服务器被认为不可用的时间段以及统计失败尝试次数的时间段。在这段时间中，服务器失败次数达到指定的尝试次数，服务器就被认为不可用。默认情况下，该超时时间是10秒。 在实际应用当中，如果你后端应用是能够快速重启的应用，比如nginx的话，自带的模块是可以满足需求的。但是需要注意。如果后端有不健康节点，负载均衡器依然会先把该请求转发给该不健康节点，然后再转发给别的节点，这样就会浪费一次转发。 可是，如果当后端应用重启时，重启操作需要很久才能完成的时候就会有可能拖死整个负载均衡器。此时，由于无法准确判断节点健康状态，导致请求handle住，出现假死状态，最终整个负载均衡器上的所有节点都无法正常响应请求。由于公司的业务程序都是java开发的，因此后端主要是nginx集群和tomcat集群。由于tomcat重启应部署上面的业务不同，有些业务启动初始化时间过长，就会导致上述现象的发生，因此不是很建议使用该模式。 并且ngx_http_upstream_module模块中的server指令中的max_fails参数设置值，也会和ngx_http_proxy_module 模块中的的proxy_next_upstream指令设置起冲突。比如如果将max_fails设置为0，则代表不对后端服务器进行健康检查，这样还会使fail_timeout参数失效（即不起作用）。此时，其实我们可以通过调节ngx_http_proxy_module 模块中的 proxy_connect_timeout 指令、proxy_read_timeout指令，通过将他们的值调低来发现不健康节点，进而将请求往健康节点转移。 以上就是nginx自带的两个和后端健康检查相关的模块。 二、nginx_upstream_check_module模块除了自带的上述模块，还有一个更专业的模块，来专门提供负载均衡器内节点的健康检查的。这个就是淘宝技术团队开发的 nginx 模块 nginx_upstream_check_module，通过它可以用来检测后端 realserver 的健康状态。如果后端 realserver 不可用，则所以的请求就不会转发到该节点上。 在淘宝自己的 tengine 上是自带了该模块的，大家可以访问淘宝tengine的官网来获取该版本的nginx，官方地址：https://tengine.taobao.org[/。](https://note.youdao.com/) 如果我们没有使用淘宝的 tengine 的话，可以通过补丁的方式来添加该模块到我们自己的 nginx 中。我们业务线上就是采用该方式进行添加的。 下面是部署流程！ 1、下载nginx_upstream_check_module模块12345[root@localhost ~]# cd /usr/local/srcwget https://codeload.github.com/yaoweibin/nginx_upstream_check_module/zip/masterunzip master[root@localhost /usr/local/src]# ll -d nginx_upstream_check_module-masterdrwxr-xr-x. 6 root root 4096 Dec 1 02:28 nginx_upstream_check_module-master 2、为nginx打补丁12345678[root@localhost /usr/local/src]# cd nginx-1.6.0 # 进入nginx的源码目录[root@localhost nginx-1.6.0]# patch -p1 &amp;lt; ../nginx_upstream_check_module-master/check_1.5.12+.patch[root@localhost nginx-1.6.0]# ./configure --user=nginx --group=nginx --prefix=/usr/local/nginx-1.6.0 --with-http_ssl_module --with-openssl=/usr/local/src/openssl-0.9.8q --with-pcre=/usr/local/src/pcre-8.32 --add-module=/usr/local/src/nginx_concat_module/ --add-module=../nginx_upstream_check_module-master/make (注意：此处只make，编译参数需要和之前的一样)[root@localhost nginx-1.6.0]# mv /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx-1.6.0.bak[root@localhost nginx-1.6.0]# cp ./objs/nginx /usr/local/nginx/sbin/[root@localhost nginx-1.6.0]# /usr/local/nginx/sbin/nginx -t # 检查下是否有问题[root@localhost nginx-1.6.0]# kill -USR2 `cat /usr/local/nginx/logs/nginx.pid` 3、在nginx.conf配置文件里面的upstream加入健康检查，如下：123456789101112upstream name&#123;keepalive 8;server ip1:80;server ip2:80;server ip3:80;server ip4:80;check interval=3000 rise=2 fall=3 timeout=1000 type=http default_down=false port=80;check_http_send &quot;GET /test/health_check HTTP/1.1\r\nHost: request_header\r\n\r\n&quot;;check_http_expect_alive http_2xx http_3xx http_4xx;&#125; 上面配置的意思是，对name这个负载均衡条目中的所有节点，每个3秒检测一次，请求2次正常则标记 realserver状态为up，如果检测 3 次都失败，则标记 realserver的状态为down，超时时间为1秒。 这里列出nginx_upstream_check_module模块所支持的指令意思：123Syntax: check interval=milliseconds [fall=count] [rise=count] [timeout=milliseconds] [default_down=true|false] [type=tcp|http|ssl_hello|mysql|ajp] [port=check_port]Default: 如果没有配置参数，默认值是：interval=30000 fall=5 rise=2 timeout=1000 default_down=true type=tcpContext: upstream 该指令可以打开后端服务器的健康检查功能。 指令后面的参数意义是： - interval：向后端发送的健康检查包的间隔。 - fall(fall_count): 如果连续失败次数达到fall_count，服务器就被认为是down。 - rise(rise_count): 如果连续成功次数达到rise_count，服务器就被认为是up。 - timeout: 后端健康请求的超时时间。 - default_down: 设定初始时服务器的状态，如果是true，就说明默认是down的，如果是false，就是up的。默认值是true，也就是一开始服务器认为是不可用，要等健康检查包达到一定成功次数以后才会被认为是健康的。 - type：健康检查包的类型，现在支持以下多种类型 - tcp：简单的tcp连接，如果连接成功，就说明后端正常。 - ssl_hello：发送一个初始的SSL hello包并接受服务器的SSL hello包。 - http：发送HTTP请求，通过后端的回复包的状态来判断后端是否存活。 - mysql: 向mysql服务器连接，通过接收服务器的greeting包来判断后端是否存活。 - ajp：向后端发送AJP协议的Cping包，通过接收Cpong包来判断后端是否存活。 - port: 指定后端服务器的检查端口。你可以指定不同于真实服务的后端服务器的端口，比如后端提供的是443端口的应用，你可以去检查80端口的状态来判断后端健康状况。默认是0，表示跟后端server提供真实服务的端口一样。该选项出现于Tengine-1.4.0。 123Syntax: check_keepalive_requests request_numDefault: 1Context: upstream 该指令可以配置一个连接发送的请求数，其默认值为1，表示Tengine完成1次请求后即关闭连接。 Syntax: check_http_send http_packetDefault: “GET / HTTP/1.0\r\n\r\n”Context: upstream该指令可以配置http健康检查包发送的请求内容。为了减少传输数据量，推荐采用”HEAD”方法。 当采用长连接进行健康检查时，需在该指令中添加keep-alive请求头，如：”HEAD / HTTP/1.1\r\nConnection: keep-alive\r\n\r\n”。 同时，在采用”GET”方法的情况下，请求uri的size不宜过大，确保可以在1个interval内传输完成，否则会被健康检查模块视为后端服务器或网络异常。123Syntax: check_http_expect_alive [ http_2xx | http_3xx | http_4xx | http_5xx ]Default: http_2xx | http_3xxContext: upstream 该指令指定HTTP回复的成功状态，默认认为2XX和3XX的状态是健康的。123Syntax: check_shm_size sizeDefault: 1MContext: http 所有的后端服务器健康检查状态都存于共享内存中，该指令可以设置共享内存的大小。默认是1M，如果你有1千台以上的服务器并在配置的时候出现了错误，就可能需要扩大该内存的大小。123Syntax: check_status [html|csv|json]Default: check_status htmlContext: location 显示服务器的健康状态页面。该指令需要在http块中配置。在Tengine-1.4.0以后，你可以配置显示页面的格式。支持的格式有: html、csv、 json。默认类型是html。你也可以通过请求的参数来指定格式，假设‘/status’是你状态页面的URL， format参数改变页面的格式，比如：123/status?format=html/status?format=csv/status?format=json 同时你也可以通过status参数来获取相同服务器状态的列表，比如：1232/status?format=html&amp;amp;status=down/status?format=csv&amp;amp;status=up 下面是一个状态也配置的范例：12345678910http &#123; server &#123; location /nstatus &#123; check_status; access_log off; #allow IP; #deny all; &#125; &#125;&#125; 配置完毕后，重启nginx。此时通过访问定义好的路径，就可以看到当前 realserver 实时的健康状态啦。效果如下图：realserver 都正常的状态： 一台 realserver 故障的状态： OK，以上nginx_upstream_check_module模块的相关信息，更多的信息大家可以去该模块的淘宝tengine页面和github上该项目页面去查看，下面是访问地址： https://tengine.taobao.org/document_cn/http_upstream_check_cn.html https://github.com/yaoweibin/nginx_upstream_check_module 在生产环境的实施应用中，需要注意的有 2 点： 1、主要定义好type。由于默认的type是tcp类型，因此假设你服务启动，不管是否初始化完毕，它的端口都会起来，所以此时前段负载均衡器为认为该服务已经可用，其实是不可用状态。 2、注意check_http_send值的设定。由于它的默认值是”GET / HTTP/1.0\r\n\r\n”。假设你的应用是通过https://ip/name访问的，那么这里你的check_http_send值就需要更改为”GET /name HTTP/1.0\r\n\r\n”才可以。针对采用长连接进行检查的，这里增加keep-alive请求头，即”HEAD /name HTTP/1.1\r\nConnection: keep-alive\r\n\r\n”。如果你后端的tomcat是基于域名的多虚拟机，此时你需要通过check_http_send定义host，不然每次访问都是失败，范例：check_http_send “GET /mobileapi HTTP/1.0\r\n HOST www.redhat.sx\r\n\r\n”; 三、ngx_http_healthcheck_module模块除了上面两个模块，nginx官方在早期的时候还提供了一个 ngx_http_healthcheck_module 模块用来进行nginx后端节点的健康检查。nginx_upstream_check_module模块就是参照该模块的设计理念进行开发的，因此在使用和效果上都大同小异。但是需要注意的是，ngx_http_healthcheck_module 模块仅仅支持nginx的1.0.0版本，1.1.0版本以后都不支持了！因此，对于目前常见的生产环境上都不会去用了，这里仅仅留个纪念，给大家介绍下这个模块！ 原文链接：https://www.21yunwei.com/archives/50871]]></content>
      <categories>
        <category>linux</category>
        <category>负载均衡</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查看linux中的TCP连接数]]></title>
    <url>%2F2020%2F01%2F01%2F%E6%9F%A5%E7%9C%8Blinux%E4%B8%AD%E7%9A%84TCP%E8%BF%9E%E6%8E%A5%E6%95%B0%2F</url>
    <content type="text"><![CDATA[一、查看哪些IP连接本机 netstat -an 二、查看TCP连接数 1)统计80端口连接数1netstat -nat|grep -i &quot;80&quot;|wc -l 2）统计httpd协议连接数1ps -ef|grep httpd|wc -l 3）、统计已连接上的，状态为“established1netstat -na|grep ESTABLISHED|wc -l 4)、查出哪个IP地址连接最多,将其封了.123netstat -na|grep ESTABLISHED|awk &#123;print $5&#125;|awk -F: &#123;print $1&#125;|sort|uniq -c|sort -r +0nnetstat -na|grep SYN|awk &#123;print $5&#125;|awk -F: &#123;print $1&#125;|sort|uniq -c|sort -r +0n 1、查看apache当前并发访问数：1netstat -an | grep ESTABLISHED | wc -l 对比httpd.conf中MaxClients的数字差距多少。 2、查看有多少个进程数：1ps aux|grep httpd|wc -l 3、可以使用如下参数查看数据1234server-status?auto#ps -ef|grep httpd|wc -l1388 统计httpd进程数，连个请求会启动一个进程，使用于Apache服务器。表示Apache能够处理1388个并发请求，这个值Apache可根据负载情况自动调整。12#netstat -nat|grep -i &quot;80&quot;|wc -l4341 netstat -an会打印系统当前网络链接状态，而grep -i “80”是用来提取与80端口有关的连接的，wc -l进行连接数统计。最终返回的数字就是当前所有80端口的请求总数。12#netstat -na|grep ESTABLISHED|wc -l376 netstat -an会打印系统当前网络链接状态，而grep ESTABLISHED 提取出已建立连接的信息。 然后wc -l统计。最终返回的数字就是当前所有80端口的已建立连接的总数。1netstat -nat||grep ESTABLISHED|wc - 可查看所有建立连接的详细记录 查看Apache的并发请求数及其TCP连接状态： Linux命令：123456netstat -n | awk &apos;/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;&apos;（netstat -n | awk ‘/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;’TIME_WAIT 8947 等待足够的时间以确保远程TCP接收到连接中断请求的确认FIN_WAIT1 15 等待远程TCP连接中断请求，或先前的连接中断请求的确认FIN_WAIT2 1 从远程TCP等待连接中断请求ESTABLISHED 55 代表一个打开的连接SYN_RECV 21 再收到和发送一个连接请求后等待对方对连接请求的确认CLOSING 2 没有任何连接状态LAST_ACK 4 等待原来的发向远程TCP的连接中断请求的确认 TCP连接状态详解LISTEN： 侦听来自远方的TCP端口的连接请求SYN-SENT： 再发送连接请求后等待匹配的连接请求SYN-RECEIVED：再收到和发送一个连接请求后等待对方对连接请求的确认ESTABLISHED： 代表一个打开的连接FIN-WAIT-1： 等待远程TCP连接中断请求，或先前的连接中断请求的确认FIN-WAIT-2： 从远程TCP等待连接中断请求CLOSE-WAIT： 等待从本地用户发来的连接中断请求CLOSING： 等待远程TCP对连接中断的确认LAST-ACK： 等待原来的发向远程TCP的连接中断请求的确认TIME-WAIT： 等待足够的时间以确保远程TCP接收到连接中断请求的确认CLOSED： 没有任何连接状态123456 LAST_ACK 5 SYN_RECV 30 ESTABLISHED 1597 FIN_WAIT1 51 FIN_WAIT2 504 TIME_WAIT 1057 其中的SYN_RECV表示正在等待处理的请求数；ESTABLISHED表示正常数据传输状态；TIME_WAIT表示处理完毕，等待超时结束的请求数。 查看Apache并发请求数及其TCP连接状态 查看httpd进程数（即prefork模式下Apache能够处理的并发请求数）： Linux命令：1ps -ef | grep httpd | wc -l 返回结果示例： 1388 表示Apache能够处理1388个并发请求，这个值Apache可根据负载情况自动调整，我这组服务器中每台的峰值曾达到过2002。 查看Apache的并发请求数及其TCP连接状态： Linux命令：1netstat -n | awk &apos;/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;&apos; 返回结果示例：123456 LAST_ACK 5 SYN_RECV 30 ESTABLISHED 1597 FIN_WAIT1 51 FIN_WAIT2 504 TIME_WAIT 1057 其中的SYN_RECV表示正在等待处理的请求数；ESTABLISHED表示正常数据传输状态；TIME_WAIT表示处理完毕，等待超时结束的请求数。 状态：描述 CLOSED：无连接是活动 的或正在进行 LISTEN：服务器在等待进入呼叫 SYN_RECV：一个连接请求已经到达，等待确认 SYN_SENT：应用已经开始，打开一个连接 ESTABLISHED：正常数据传输状态 FIN_WAIT1：应用说它已经完成 FIN_WAIT2：另一边已同意释放 ITMED_WAIT：等待所有分组死掉 CLOSING：两边同时尝试关闭 TIME_WAIT：另一边已初始化一个释放 LAST_ACK：等待所有分组死掉 如发现系统存在大量TIME_WAIT状态的连接，通过调整内核参数解决，1vim /etc/sysctl.conf 编辑文件，加入以下内容：1234net.ipv4.tcp_syncookies = 1net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_fin_timeout = 30 然后执行1/sbin/sysctl -p 让参数生效。1net.ipv4.tcp_syncookies = 1 表示开启SYN cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭；net.ipv4.tcp_tw_reuse = 1 表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；net.ipv4.tcp_tw_recycle = 1 表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。net.ipv4.tcp_fin_timeout 修改系統默认的 TIMEOUT 时间 下面附上TIME_WAIT状态的意义： 客户端与服务器端建立TCP/IP连接后关闭SOCKET后，服务器端连接的端口状态为TIME_WAIT 是不是所有执行主动关闭的socket都会进入TIME_WAIT状态呢？有没有什么情况使主动关闭的socket直接进入CLOSED状态呢？ 主动关闭的一方在发送最后一个 ack 后就会进入 TIME_WAIT 状态 停留2MSL（max segment lifetime）时间这个是TCP/IP必不可少的，也就是“解决”不了的。 也就是TCP/IP设计者本来是这么设计的主要有两个原因1。防止上一次连接中的包，迷路后重新出现，影响新连接（经过2MSL，上一次连接中所有的重复包都会消失）2。可靠的关闭TCP连接在主动关闭方发送的最后一个 ack(fin) ，有可能丢失，这时被动方会重新发fin, 如果这时主动方处于 CLOSED 状态 ，就会响应 rst 而不是 ack。所以主动方要处于 TIME_WAIT 状态，而不能是 CLOSED 。 TIME_WAIT 并不会占用很大资源的，除非受到攻击。 还有，如果一方 send 或 recv 超时，就会直接进入 CLOSED 状态 如何合理设置apache httpd的最大连接数？ 手头有一个网站在线人数增多，访问时很慢。初步认为是服务器资源不足了，但经反复测试，一旦连接上，不断点击同一个页面上不同的链接，都能迅速打开，这种现象就是说明apache最大连接数已经满了，新的访客只能排队等待有空闲的链接，而如果一旦连接上，在keeyalive 的存活时间内（KeepAliveTimeout，默认5秒）都不用重新打开连接，因此解决的方法就是加大apache的最大连接数。 1.在哪里设置？apache 2.24，使用默认配置（FreeBSD 默认不加载自定义MPM配置），默认最大连接数是250 在/usr/local/etc/apache22/httpd.conf中加载MPM配置（去掉前面的注释）：123# Server-pool management (MPM specific)Include etc/apache22/extra/httpd-mpm.conf 可见的MPM配置在/usr/local/etc/apache22/extra/httpd-mpm.conf，但里面根据httpd的工作模式分了很多块，哪一部才是当前httpd的工作模式呢？可通过执行1apachectl -l 来查看：12345Compiled in modules: core.c prefork.c http_core.c mod_so.c 看到prefork 字眼，因此可见当前httpd应该是工作在prefork模式，prefork模式的默认配置是：1234567&lt;IfModule mpm_prefork_module&gt; StartServers 5 MinSpareServers 5 MaxSpareServers 10 MaxClients 150 MaxRequestsPerChild 0&lt;/IfModule&gt; 2.要加到多少？ 连接数理论上当然是支持越大越好，但要在服务器的能力范围内，这跟服务器的CPU、内存、带宽等都有关系。 查看当前的连接数可以用：1ps aux | grep httpd | wc -l 或：1pgrep httpd|wc -l 计算httpd占用内存的平均数:1ps aux|grep -v grep|awk &apos;/httpd/&#123;sum+=$6;n++&#125;;END&#123;print sum/n&#125;&apos; 由于基本都是静态页面，CPU消耗很低，每进程占用内存也不算多，大约200K。 服务器内存有2G，除去常规启动的服务大约需要500M（保守估计），还剩1.5G可用，那么理论上可以支持1.510241024*1024/200000 = 8053.06368 约8K个进程，支持2W人同时访问应该是没有问题的（能保证其中8K的人访问很快，其他的可能需要等待1、2秒才能连上，而一旦连上就会很流畅） 控制最大连接数的MaxClients ，因此可以尝试配置为：12345678&lt;IfModule mpm_prefork_module&gt; StartServers 5 MinSpareServers 5 MaxSpareServers 10 ServerLimit 5500 MaxClients 5000 MaxRequestsPerChild 100&lt;/IfModule&gt; 注意，MaxClients默认最大为250，若要超过这个值就要显式设置ServerLimit，且ServerLimit要放在MaxClients之前，值要不小于MaxClients，不然重启httpd时会有提示。 重启httpd后，通过反复执行pgrep httpd|wc -l 来观察连接数，可以看到连接数在达到MaxClients的设值后不再增加，但此时访问网站也很流畅，那就不用贪心再设置更高的值了，不然以后如果网站访问突增不小心就会耗光服务器内存，可根据以后访问压力趋势及内存的占用变化再逐渐调整，直到找到一个最优的设置值。 (MaxRequestsPerChild不能设置为0，可能会因内存泄露导致服务器崩溃） 更佳最大值计算的公式：12apache_max_process_with_good_perfermance &lt; (total_hardware_memory / apache_memory_per_process ) * 2apache_max_process = apache_max_process_with_good_perfermance * 1.5 附： 实时检测HTTPD连接数：1watch -n 1 -d &quot;pgrep httpd|wc -l&quot; ———————————————— 原文链接：https://blog.csdn.net/he_jian1/article/details/40787269 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[提高性能，MySQL 读写分离环境搭建]]></title>
    <url>%2F2019%2F08%2F30%2F%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD%EF%BC%8CMySQL%20%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[MySQL 读写分离在互联网项目中应该算是一个非常常见的需求了。受困于 Linux 和 MySQL 版本问题，很多人经常会搭建失败，今天给大伙举一个成功的例子。 CentOS 安装 MySQL环境： CentOS7MySQL5.7具体的安装步骤如下： 检查是否安装了 mariadb，如果已经安装了则卸载：1yum list installed | grep mariadb 如果执行结果如下，表示已经安装了 mariadb，将之卸载：1mariadb-libs.x86_64 1:5.5.52-1.el7 @anaconda 卸载命令如下：1yum -y remove mariadb* 接下来下载官方提供的 rpm 包如果 CentOS 上没有 wget 命令，首先通过如下命令安装 wget：1yum install wget 然后执行如下操作下载 rpm 包：1wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm 下载完成后，安装rpm包：1rpm -ivh mysql57-community-release-el7-11.noarch.rpm 检查 MySQL 的 yum 源是否安装成功：1yum repolist enabled | grep &quot;mysql.*-community.*&quot; 执行结果如下表示安装成功： 安装 MySQL1yum install mysql-server 安装完成后，启动MySQL：1systemctl start mysqld.service 停止MySQL：1systemctl stop mysqld.service 登录 MySQL：1mysql -u root -p 默认无密码。有的版本有默认密码，查看默认密码，首先去 /etc/my.cnf 目录下查看 MySQL 的日志位置，然后打开日志文件，可以看到日志中有一个提示，生成了一个临时的默认密码，使用这个密码登录，登录成功后修改密码即可。 改密码首先修改密码策略(这一步不是必须的，如果不修改密码策略，需要取一个比较复杂的密码，松哥这里简单起见，就修改下密码策略)：1set global validate_password_policy=0; 然后重置密码：12set password=password(&quot;123&quot;);flush privileges; 授权远程登录同方式一：12grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;123&apos; with grant option;flush privileges; 授权远程登录同方式二：修改 mysql 库中的 user 表，将 root 用户的 Host 字段的值改为 % ，然后重启 MySQL 即可。 关闭防火墙 MySQL 要能远程访问，还需要关闭防火墙：1systemctl stop firewalld.service 禁止firewall开机启动:1systemctl disable firewalld.service MySQL 读写分离环境搭建。准备工作 我这里有一张简单的图向大伙展示 MySQL 主从的工作方式： 这里，我们准备两台机器： 主机：192.168.248.128 •从机：192.168.248.139 主机配置主机的配置就三个步骤，比较容易： 1、授权给从机服务器12GRANT REPLICATION SLAVE ON *.* to &apos;rep1&apos;@&apos;192.168.248.139&apos; identified by &apos;123&apos;;FLUSH PRIVILEGES; 这里表示配置从机登录用户名为 rep1，密码为 123，并且必须从 192.168.248.139这个地址登录，登录成功之后可以操作任意库中的任意表。其中，如果不需要限制登录地址，可以将 IP 地址更换为一个 %。 2.修改主库配置文件，开启 binlog ，并设置 server-id ，每次修改配置文件后都要重启 MySQL 服务才会生效1vi /etc/my.cnf 修改的文件内容如下：1234[mysqld]log-bin=/var/lib/mysql/binlogserver-id=128binlog-do-db = cmdb 如下图： log-bin：同步的日志路径及文件名，一定注意这个目录要是 MySQL 有权限写入的（我这里是偷懒了，直接放在了下面那个datadir下面 binlog-do-db：要同步的数据库名，当从机连上主机后，只有这里配置的数据库才会被同步，其他的不会被同步 server-id: MySQL 在主从环境下的唯一标志符，给个任意数字，注意不能和从机重复。 配置完成后重启 MySQL 服务端：1systemctl restart mysqld 3、查看主服务器当前二进制日志名和偏移量，这个操作的目的是为了在从数据库启动后，从这个点开始进行数据的恢复：1show master status; 至此，主机配置完成。 从机配置从机的配置也比较简单，我们一步一步来看：1.在/etc/my.cnf 添加下面配置： 注意从机这里只需要配置一下 server-id 即可。 注意：如果从机是从主机复制来的，即我们通过复制 CentOS 虚拟机获取了 MySQL 实例 ，此时两个 MySQL 的 uuid 一样（正常安装是不会相同的），这时需要手动修改，修改位置在 /var/lib/mysql/auto.cnf ，注意随便修改这里几个字符即可，但也不可太过于随意，例如修改了 uuid 的长度。 2.使用命令来配置从机：1change master to master_host=&apos;192.168.248.128&apos;,master_port=3306,master_user=&apos;rep1&apos;,master_password=&apos;123&apos;,master_log_file=&apos;binlog.000001&apos;,master_log_pos=120; 这里配置了主机地址、端口以及从机登录主机的用户名和密码，注意最后两个参数要和 master 中的保持一致。 3.启动 slave 进程1start slave; 启动之后查看从机状态：1show slave status\G; 4.查看 slave 的状态 主要是下面两项值都要为为 YES，则表示配置正确：12Slave_IO_Running: YesSlave_SQL_Running: Yes 至此，配置完成，主机创建库，添加数据，从机会自动同步。 如果这两个有一个不为 YES ，表示主从环境搭建失败，此时可以阅读日志，查看出错的原因，再具体问题具体解决。 原文链接：https://mp.weixin.qq.com/s/QapdsWt5KOi1L0JKX4B0pg 转载请注明出处，谢谢！]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ActiveMQ之VirtualTopic]]></title>
    <url>%2F2019%2F06%2F17%2FActiveMQ%E4%B9%8BVirtualTopic%E6%98%AF%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[一句话总结： VirtualTopic是为了解决持久化模式下多消费端同时接收同一条消息的问题。 Topic订阅模式缺点，虽然满足1对多同时接收，然而持久化模式下只能有一个持有clientID的消费者连接，不满足持久化需求 简单说来，就是将Topic和Queue相结合，各取所长。 VirtualTopic 就是这样一种存在，对生产者而言它是Topic，对消费者而言它是Queue，内部的处理机制就是由Broker将接收到的消息二次分发给每一个Queue，然后由不同的Queue对应不同的应用实现持久化，不同的消费端只关心并连接到自己的Queue接收消息即可。 VirtualTopic 因为不同消费者会同事收到所有消息，所以会存在重复消费的问题，如果业务上要求只消费一次，需要在业务上做幂等处理。 总结一下： 虚拟Topic是一种特殊命名的Topic，系统根据命名规则将该Topic内的消息分发给当前存在的名称对应的Queue，分发是非持久化的，新加入的Queue是接收不到过去的消息的。 虚拟Topic还是Topic，不是什么新的存在，具有普通Topic的所有功能，只是名字特殊而已。 虚拟Topic的功能完全是中间件本身额外附加的机制，对于生产者和消费者都是无感知的。 对于运维人员来说，还是正常监控队列即可，虚拟Topic是非持久化的，不存在积压。 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>MQ</category>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>MQ</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RestController和Controller区别]]></title>
    <url>%2F2019%2F05%2F08%2FRestController%E5%92%8CController%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[@RestController注解相当于@ResponseBody ＋ @Controller合在一起的作用。RestController使用的效果是将方法返回的对象直接在浏览器上展示成json格式，而如果单单使用@Controller会报错，需要ResponseBody配合使用。注意1、如果只是使用@RestController注解Controller类，则方法无法返回jsp页面，配置的视图解析器InternalResourceViewResolver不起作用，返回的内容就是Return 里的内容。例如：本来应该到success.jsp页面的，则其显示success.2、如果需要返回到指定页面，则需要用 @Controller配合视图解析器InternalResourceViewResolver才行。3、如果需要返回JSON，XML或自定义mediaType内容到页面，则需要在对应的方法上加上@ResponseBody注解。有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[idea破解]]></title>
    <url>%2F2019%2F05%2F06%2FIDEA%E7%A0%B4%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[破解方法license server 填写 https://idea.toocruel.net/ 简单粗暴搞定 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>软件破解</category>
      </categories>
      <tags>
        <tag>idea</tag>
        <tag>软件破解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yum安装更新java sdk]]></title>
    <url>%2F2019%2F05%2F06%2Fyum%E5%AE%89%E8%A3%85%E6%9B%B4%E6%96%B0java%20sdk%2F</url>
    <content type="text"><![CDATA[安装之前先检查一下系统有没有自带open-jdk123rpm -qa |grep java rpm -qa |grep jdk rpm -qa |grep gcj 如果没有输入信息表示没有安装。如果安装可以使用1rpm -qa | grep java | xargs rpm -e –nodeps (报错失败也无所谓 可以直接跳过) 批量卸载所有带有Java的文件 这句命令的关键字是java首先检索包含java的列表1yum list java* 检索1.8的列表1yum list java-1.8* 安装1.8.0的所有文件1yum install java-1.8.0-openjdk* -y 使用命令检查是否安装成功1java -version 到此安装结束了。这样安装有一个好处就是不需要对path进行设置，自动就设置好了 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>java</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx添加ipv6模块并配置ipv6端口监听]]></title>
    <url>%2F2018%2F12%2F20%2Fnginx%E6%B7%BB%E5%8A%A0ipv6%E6%A8%A1%E5%9D%97%E5%B9%B6%E9%85%8D%E7%BD%AEipv6%E7%AB%AF%E5%8F%A3%E7%9B%91%E5%90%AC%2F</url>
    <content type="text"><![CDATA[一为Nginx添加ipv6模块一：检查nginx是否已经包含ipv6模块1[root@iz2ze3oyrjbxg32wecre15z /]# /alidata/server/nginx/sbin/nginx -V 从结果看出，输出结果没有 ipv6支持 如果支持ipv6，则输出结果会包含 “–with-ipv6”。 如果各位同学的环境已经支持，则可以不用往下看啦！ 二：准备nginx源码1、本文以1.4.4版本为例，其他版本没有测试，不过我想基本类似，具体请各位同学亲测。 2、如果本地没有源码，则需要到如下地址下载：https://nginx.org/download/nginx-1.4.4.tar.gz其他版本，也可以在https://nginx.org下载，上面会列出所有版本以供下载。 3、将下载的源码上传到linux 在root 根目录新建 tmp目录123[root@iz2ze3oyrjbxg32wecre15z /]# cd /root[root@iz2ze3oyrjbxg32wecre15z /]# mkdir tmp 将安装包上传到该目录，上传工具推荐同学们使用Xftp 三：编译nginx源代码，加入ipv6支持1、跳转到tmp目录，执行命令解压安装文件123[root@iz2ze3oyrjbxg32wecre15z /]# cd tmp[root@iz2ze3oyrjbxg32wecre15z /]# tar xvzf nginx-1.4.4.tar.gz 2、跳转到解压过后的目录，执行命令配置新的参数，加入 ipv6模块。 注意：在配置参数时，一定要在原来的参数的基础上追加，否则有可能丢失原来的支持模块，切记！切记！切记！123[root@iz2ze3oyrjbxg32wecre15z /]# cd nginx-1.4.4[root@iz2ze3oyrjbxg32wecre15z/]#./configure --user=www --group=www --prefix=/alidata/server/nginx --with-http_stub_status_module --without-http-cache --with-http_ssl_module --with-http_gzip_static_module --with-ipv6 注意：‘–’符号之间有空格。 ….中间省略若干输出 3、配置完毕后，执行make 命令，重新编译 ….中间省略若干输出 4、编译完毕后，会在当前目录下创建objs目录，新的nginx执行文件将生成在该目录中 四：替换nginx执行文件1、同学们要养成好习惯，首先备份原来的nginx123[root@iz2ze3oyrjbxg32wecre15z /]#cp /alidata/server/nginx/sbin/nginx /alidata/server/nginx/sbin/nginx.bak 2、停止nginx 这里必须停止nginx，否则无法覆盖的哦！1[root@iz2ze3oyrjbxg32wecre15z /]# service nginx stop 3、覆盖nginx执行文件1[root@iz2ze3oyrjbxg32wecre15z /]# cp nginx /alidata/server/nginx/sbin/nginx 3、测试新的nginx程序是否正确1[root@iz2ze3oyrjbxg32wecre15z /]# /alidata/server/nginx/sbin/nginx -t 4、启动nginx1[root@iz2ze3oyrjbxg32wecre15z /]# service nginx start 五：再次检查nginx是否已经支持ipv61[root@iz2ze3oyrjbxg32wecre15z /]# /alidata/server/nginx/sbin/nginx -V 二.配置IPv6端口监听Nginx可以同时支持ipv4与 ipv6的监听，但为了一致性的考虑，新版本Nginx推荐使用分开监听，下面我们开始进入正题。 一、默认IPV4配置下面我们先来看一看默认的ipv4配置： 二、加入ipv6监听1、从Nginx 1.3的某个版本起，默认ipv6only是打开的，所以，我们只需要在监听中加入ipv6监听即可，不过推荐都手动加上比较好，代码如下：1listen [::]:80 ipv6only=on; 2、编辑完毕后保存，然后使用命令检测配置是否正确1[root@iz2ze3oyrjbxg32wecre15z /]# /alidata/server/nginx/sbin/nginx -t 如果出现 test is successful代表配置检测成功。 3、重启nginx1[root@iz2ze3oyrjbxg32wecre15z /]# service nginx reload 4、检测监听是否已经成功1[root@iz2ze3oyrjbxg32wecre15z /]# netstat -tuln 如果列表出现 :::80的监听代表ipv6的监听已经成功。 13三、其他个性化操作1、如果只想监听ipv6，则去掉ipv4的配置，然后将ipv6设置为默认即可。 2、如果想监听指定ipv6地址，则将中括号中的::换成 指定ipv6地址即可。 3、在前面的配置中，很多同学看到有监听443端口，这个是对ssl的监听，监听方式和默认80监听类似，大家可以举一反三嘛！ 四、我们使用ipv6网站来测试是否已经可以正常访问我们常用的网站是：https://ipv6-test.com/validate.php 结果显示，已经正确访问到网站。 转载请注明出处，谢谢！参考：https://blog.csdn.net/shenxianfeng/article/details/72859830！]]></content>
      <categories>
        <category>nginx</category>
        <category>ipv6</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>ipv6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java之并发上锁精简分析]]></title>
    <url>%2F2018%2F09%2F14%2Fjava%E4%B9%8B%E5%B9%B6%E5%8F%91%E4%B8%8A%E9%94%81%E7%B2%BE%E7%AE%80%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[“锁”在我们日常的生活工作中经常会用到，这充分保证了个人财产安全和隐私安全。 同样，在程序的世界里，也有一把锁，保证程序不会崩溃，保证我们手机钱包里的钱不会无缘无故变多变少。 这把锁（lock）作为用于保护临界区（critical section）的一种机制，被广泛应用在多线程程序中。比如 Java 应用程序出现的 synchronized 关键字，就是锁实现的一种方式。 下面我们从一个计数器的应用说起，分析下锁在 Java 程序中的应用。 一个计数器 非线程安全，value++ 不是原子性操作，状态不稳定，需要同步。 同步方法锁定实例 线程安全，锁的对象是当前 SafeCounter 的实例，不会影响到另一个 SafeCounter 实例的 getNext() 方法。 锁定类 线程安全，锁的对象是 SafeCounterWithClass 类，任何实例都会受锁的影响，一般不这么用。 lock 线程安全，锁的对象是 lock，可以灵活控制，只锁真正需要同步的代码。注意 lock 不能是 null，也不能改变。 更好的 lock 线程安全，灵活控制锁竞争时的处理，拥有更好的性能。 atomic 最佳实践：使用原子操作类，不阻塞，获得最好的性能。 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>java</category>
        <category>并发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下解压命令大全]]></title>
    <url>%2F2018%2F07%2F17%2Flinux%E5%8E%8B%E7%BC%A9%E8%A7%A3%E5%8E%8B%2F</url>
    <content type="text"><![CDATA[linux下解压命令大全 .tar 解包：tar xvf FileName.tar 打包：tar cvf FileName.tar DirName （注：tar是打包，不是压缩！） ——————————————— .gz 解压1：gunzip FileName.gz 解压2：gzip -d FileName.gz 压缩：gzip FileName .tar.gz 和 .tgz 解压：tar zxvf FileName.tar.gz 压缩：tar zcvf FileName.tar.gz DirName ——————————————— .bz2 解压1：bzip2 -d FileName.bz2 解压2：bunzip2 FileName.bz2 压缩： bzip2 -z FileName .tar.bz2 解压：tar jxvf FileName.tar.bz2 压缩：tar jcvf FileName.tar.bz2 DirName ——————————————— .bz 解压1：bzip2 -d FileName.bz 解压2：bunzip2 FileName.bz 压缩：未知 .tar.bz 解压：tar jxvf FileName.tar.bz 压缩：未知——————————————— .Z 解压：uncompress FileName.Z 压缩：compress FileName .tar.Z 解压：tar Zxvf FileName.tar.Z 压缩：tar Zcvf FileName.tar.Z DirName ——————————————— .zip 解压：unzip FileName.zip 压缩：zip FileName.zip DirName ——————————————— .rar 解压：rar x FileName.rar 压缩：rar a FileName.rar DirName ——————————————— .lha 解压：lha -e FileName.lha 压缩：lha -a FileName.lha FileName ——————————————— .rpm 解包：rpm2cpio FileName.rpm | cpio -div ——————————————— .deb 解包：ar p FileName.deb data.tar.gz | tar zxf - ——————————————— .tar .tgz .tar.gz .tar.Z .tar.bz .tar.bz2 .zip .cpio .rpm .deb .slp .arj .rar .ace .lha .lzh .lzx .lzs .arc .sda .sfx .lnx .zoo .cab .kar .cpt .pit .sit .sea 解压：sEx x FileName.* 压缩：sEx a FileName.* FileName sEx只是调用相关程序，本身并无压缩、解压功能，请注意！ gzip 命令 减少文件大小有两个明显的好处，一是可以减少存储空间，二是通过网络传输文件时，可以减少传输的时间。gzip 是在 Linux 系统中经常使用的一个对文件进行压缩和解压缩的命令，既方便又好用。 语法：gzip [选项] 压缩（解压缩）的文件名该命令的各选项含义如下： -c 将输出写到标准输出上，并保留原有文件。 -d 将压缩文件解压。 -l 对每个压缩文件，显示下列字段： 压缩文件的大小；未压缩文件的大小；压缩比；未压缩文件的名字 -r 递归式地查找指定目录并压缩其中的所有文件或者是解压缩。 -t 测试，检查压缩文件是否完整。 -v 对每一个压缩和解压的文件，显示文件名和压缩比。 -num 用指定的数字 num 调整压缩的速度， -1 或 –fast 表示最快压缩方法（低压缩比）， -9 或–best表示最慢压缩方法（高压缩比）。系统缺省值为 6。指令实例： gzip *% 把当前目录下的每个文件压缩成 .gz 文件。 gzip -dv *% 把当前目录下每个压缩的文件解压，并列出详细的信息。 gzip -l *% 详细显示例1中每个压缩的文件的信息，并不解压。 gzip usr.tar% 压缩 tar 备份文件 usr.tar，此时压缩文件的扩展名为.tar.gz。 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins安装配置 linux]]></title>
    <url>%2F2018%2F07%2F06%2Fjenkins%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[环境：CentOS 6.8 + java 8 下载安装jenkins方式1: yum方式安装Jenkins，最简单方便yum的repos中默认是没有Jenkins的，需要先将Jenkins存储库添加到yum repos。12345sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.reposudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.keyyum install jenkins 如果连接超时(内网虚机可能无法链接)采用 方式2： 直接下载war包jenkins.war，下载地址https://jenkins.io/download 使用rpm包安装命令：1sudo rpm -ih jenkins-2.119-1.1.noarch.rpm 自动安装完成之后： /usr/lib/jenkins/jenkins.war WAR包 /etc/sysconfig/jenkins 配置文件 /var/lib/jenkins/ 默认的JENKINS_HOME目录 /var/log/jenkins/jenkins.log Jenkins日志文件 启动jenkins1sudo service jenkins start https://IP:8080 登录jenkins 打开红色标记中的路径，取出password，填入上图，continue 可能遇到Jenkenis报错： 该jenkins实例似乎已离线 解决方法： 安装插件那个页面，就是提示你offline的那个页面，不要动。然后打开一个新的tab，输入网址 https://ip:8080/pluginManager/advanced。 这里面最底下有个【升级站点】，把其中的链接改成http的就好了，https://updates.jenkins.io/update-center.json。 然后重启闭jenkins1service jenkins restart 这样就能正常联网了 如果还无法联网，可能是机器不能访问谷歌，因为jenkins检查网络是否连接。直接访问了谷歌 最终解决方案： 1vim /var/lib/jenkins/updates/default.json 编辑、替换： 将www.google.com替换成www.baidu.com 重启jenkins 至此 jenkins成功启动 Python安装参照 https://www.jianshu.com/p/58d106395e45 安装完成后可能出现下面问题 error while loading shared libraries: libpython3.6m.so.1.0 解决方案： 查找一下文件的位置：12find / -name &apos;libpython3.6m.so.1.0&apos;/usr/local/lib/libpython3.6m.so.1.0 之后在目录 /etc/ld.so.conf.d 下，建立 python3.conf并在 python3.conf 中加入：也就是所在的目录1/usr/local/lib/ 然后运行：ldconfig 再运行python3 就可以啦 fabric安装fabric有两个版本1和2，需使用1，不然会报错 安装fabric通过yum安装即可，导入epel yum库 安装fabric命令如下：1yum install fabric -y pip命令使用1pip show fabric 查看fabric安装信息1pip uninstall fabric #卸载fabric maven安装下载maven安装包1wget https://mirror.bit.edu.cn/apache/maven/maven-3/wget https://mirror.bit.edu.cn/apache/maven/maven-3/ 解压缩maven1tar -zxvf apache-maven-3.5.2-bin.tar.gz 将maven解压缩之后的路径为：/var/local/lib 配置maven环境变量1vi /etc/profile 添加环境变量 export MAVEN_HOME=/var/local/apache-maven-3.5.2 export MAVEN_HOME export PATH=$PATH:$MAVEN_HOME/bin 使配置生效1source /etc/profile jenkins插件配置联网状态下 系统管理–&gt;插件管理 可以直接搜索Gitlab Hook Plugin等插件自动安装 断网状态下 jenkins官网下载插件 然后高级 上传.hpi文件 环境变量配置如果虚机maven等工具安装没问题，jenkins执行脚本出现mvn:commond not found等错误，需配置jenkins环境变量 1、控制台执行 echo $PATH 12[root@cartoon-jenkins-dev001-whdx ~]# echo $PATH/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/usr/local/lib/apache-maven-3.5.2/bin:/usr/local/python3.6.0/bin:/root/bin:/usr/local/lib/apache-maven-3.5.2/bin 把输出的这句话复制 2、jenkins-&gt;系统管理-&gt;系统设置 OK了！ 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>jenkins</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac-Operation not permitted]]></title>
    <url>%2F2018%2F06%2F26%2FMac-Operation%20not%20permitted%2F</url>
    <content type="text"><![CDATA[最近在mac上操作文件发现提示 chmod: Unable to change file mode on /usr/bin/cc: Operation not permitted1发现是El Capitan（10.11） 加入了Rootless机制，很多系统目录不再能够随心所欲的读写了，即使设置 root 权限也不行。 以下路径无法写和执行 /System/bin/sbin/usr (except /usr/local)1234加入这个机制主要是为了防止恶意程序的入侵，更多我们可以查看官网 https://developer.apple.com/videos/play/wwdc2015/706/ 如何关闭重启按住 Command+R，进入恢复模式，打开Terminal1csrutil disable 如何开启重启按住 Command+R，进入恢复模式，打开Terminal。1csrutil enable 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python远程部署命令Fabric]]></title>
    <url>%2F2018%2F04%2F22%2FPython%E8%BF%9C%E7%A8%8B%E9%83%A8%E7%BD%B2%E5%91%BD%E4%BB%A4Fabric%2F</url>
    <content type="text"><![CDATA[Python远程部署利器Fabric详解Fabric是一个Python的库，它提供了丰富的同SSH交互的接口，可以用来在本地或远程机器上自动化、流水化地执行Shell命令。因此它非常适合用来做应用的远程部署及系统维护。其上手也极其简单，你需要的只是懂得基本的Shell命令。 本文将为大家详细介绍Fabric的使用。 安装Fabric首先Python的版本必须是2.7以上，可以通过下面的命令查看当前Python的版本：1$ python -V Fabric的官网是www.fabfile.org，源码托管在Github上。你可以clone源码到本地，然后通过下面的命令来安装。1$ python setup.py develop 在执行源码安装前，你必须先将Fabric的依赖包Paramiko装上。所以，个人还是推荐使用pip安装，只需一条命令即可：1$ pip install fabric 第一个例子 万事从Hello World开始 我们创建一个fabfile.py文件，然后写个hello函数：12def hello(): print &quot;Hello Fabric!&quot; 现在，让我们在fabfile.py的目录下执行命令：1$ fab hello 你可以在终端看到”Hello Fabric!”字样。 简单解释下，fabfile.py文件中每个函数就是一个任务，任务名即函数名，上例中是hello。fab命令就是用来执行fabfile.py中定义的任务，它必须显式地指定任务名。你可以使用参数-l来列出当前fabfile.py文件中定义了哪些任务：1$ fab -l 任务可以带参数，比如我们将hello函数改为： 12def hello(name, value): print &quot;Hello Fabric! %s=%s&quot; % (name,value) 此时执行hello任务时，就要传入参数值：1$ fab hello:name=Year,value=2016 Fabric的脚本建议写在fabfile.py文件中，如果你想换文件名，那就要在”fab”命令中用”-f”指定。比如我们将脚本放在script.py中，就要执行：1$ fab -f script.py hello 执行本地命令“fabric.api”包里的”local()”方法可以用来执行本地Shell命令，比如让我们列出本地”/home/bjhee”目录下的所有文件及目录： 1234from fabric.api import local def hello(): local(&apos;ls -l /home/bjhee/&apos;) “local()”方法有一个”capture”参数用来捕获标准输出，比如： 12def hello(): output = local(&apos;echo Hello&apos;, capture=True) 这样，Hello字样不会输出到屏幕上，而是保存在变量output里。”capture”参数的默认值是False。 执行远程命令Fabric真正强大之处不是在执行本地命令，而是可以方便的执行远程机器上的Shell命令。它通过SSH实现，你需要的是在脚本中配置远程机器地址及登录信息： 12345678910from fabric.api import run, env env.hosts = [&apos;example1.com&apos;, &apos;example2.com&apos;]env.user = &apos;bjhee&apos;env.password = &apos;111111&apos; def hello(): run(&apos;ls -l /home/bjhee/&apos;)``` “fabric.api”包里的”run()”方法可以用来执行远程Shell命令。上面的任务会分别到两台服务器example1.com和example2.com上执行”ls -l /home/bjhee/”命令。这里假设两台服务器的用户名都是”bjhee”，密码都是6个1。你也可以把用户直接写在hosts里，比如： env.hosts = [‘bjhee@example1.com’, ‘bjhee@example2.com’]1如果你的”env.hosts”里没有配置某个服务器，但是你又想在这个服务器上执行任务，你可以在命令行中通过”-H”指定远程服务器地址，多个服务器地址用逗号分隔： $ fab -H bjhee@example3.com,bjhee@example4.com hello123另外，多台机器的任务是串行执行的，关于并行任务的执行我们在之后会介绍。如果对于不同的服务器，我们想执行不同的任务，上面的方法似乎做不到，那怎么办？我们要对服务器定义角色： from fabric.api import env, roles, run, execute, cd env.roledefs = { ‘staging’: [‘bjhee@example1.com’,’bjhee@example2.com’], ‘build’: [‘build@example3.com’]} env.passwords = { ‘staging’: ‘11111’, ‘build’: ‘123456’} @roles(‘build’)def build(): with cd(‘/home/build/myapp/‘): run(‘git pull’) run(‘python setup.py’) @roles(‘staging’)def deploy(): run(‘tar xfz /tmp/myapp.tar.gz’) run(‘cp /tmp/myapp /home/bjhee/www/‘) def task(): execute(build) execute(deploy)1现在让我们执行： $ fab task123这时Fabric会先在一台build服务器上执行build任务，然后在两台staging服务器上分别执行deploy任务。”@roles”装饰器指定了它所装饰的任务会被哪个角色的服务器执行。如果某一任务上没有指定某个角色，但是你又想让这个角色的服务器也能运行该任务，你可以通过”-R”来指定角色名，多个角色用逗号分隔： $ fab -R build deploy12345678910111213这样”build”和”staging”角色的服务器都会运行”deploy”任务了。**注**：”staging”是装饰器默认的，因此不用通过”-R”指定。此外，上面的例子中，服务器的登录密码都是明文写在脚本里的。这样做不安全，推荐的方式是设置SSH自动登录，具体方法大家可以去网上搜搜。# SSH功能函数到目前为止，我们介绍了”local()”和”run()”函数分别用来执行本地和远程Shell命令。Fabric还提供了其他丰富的功能函数来辅助执行命令，这里我们介绍几个常用的：## sudo: 以超级用户权限执行远程命令功能类似于”run()”方法，区别是它相当于在Shell命令前加上了”sudo”，所以拥有超级用户的权限。使用此功能前，你需要将你的用户设为sudoer，而且无需输密码。具体操作可参见我的这篇文章。 from fabric.api import env, sudo env.hosts = [‘bjhee@example1.com’, ‘bjhee@example2.com’]env.password = ‘111111’ def hello(): sudo(‘mkdir /var/www/myapp’)get(remote, local):12从远程机器上下载文件到本地它的工作原理是基于scp命令，使用的方法如下： from fabric.api import env, get env.hosts = [‘bjhee@example.com’,]env.password = ‘111111’ def hello(): get(‘/var/log/myapp.log’, ‘myapp-0301.log’)123456上述任务将远程机上”/var/log/myapp.log”文件下载到本地当前目录，并命名为”myapp-0301.log”。## put(local, remote):从本地上传文件到远程机器上同get一样，put方法也是基于scp命令，使用的方法如下： from fabric.api import env, put env.hosts = [‘bjhee@example1.com’, ‘bjhee@example2.com’]env.password = ‘111111’ def hello(): put(‘/tmp/myapp-0301.tar.gz’, ‘/var/www/myapp.tar.gz’)1上述任务将本地”/tmp/myapp-0301.tar.gz”文件分别上传到两台远程机的”/var/www/”目录下，并命名为”myapp.tar.gz”。如果远程机上的目录需要超级用户权限才能放文件，可以在”put()”方法里加上”use_sudo”参数： put(&apos;/tmp/myapp-0301.tar.gz&apos;, &apos;/var/www/myapp.tar.gz&apos;, use_sudo=True) 12## prompt: 提示输入该方法类似于Shell中的”read”命令，它会在终端显示一段文字来提示用户输入，并将用户的输入保存在变量里： from fabric.api import env, get, prompt env.hosts = [‘bjhee@example.com’,]env.password = ‘111111’ def hello(): filename = prompt(‘Please input file name: ‘) get(‘/var/log/myapp.log’, ‘%s.log’ % filename)1现在下载后的文件名将由用户的输入来决定。我们还可以对用户输入给出默认值及类型检查： port = prompt(‘Please input port number: ‘, default=8080, validate=int)1执行任务后，终端会显示： Please input port number: [8080]1234如果你直接按回车，则port变量即为默认值8080；如果你输入字符串，终端会提醒你类型验证失败，让你重新输入，直到正确为止。## reboot: 重启服务器看方法名就猜到了，有时候安装好环境后，需要重启服务器，这时就要用到”reboot()”方法，你可以用”wait”参数来控制其等待多少秒后重启，没有此参数则代表立即重启： from fabric.api import env, reboot env.hosts = [‘bjhee@example.com’,]env.password = ‘111111’ def restart(): reboot(wait=60)1234567上面的restart任务将在一分钟后重启服务器。## 上下文管理器Fabric的上下文管理器是一系列与Python的”with”语句配合使用的方法，它可以在”with”语句块内设置当前工作环境的上下文。让我们介绍几个常用的：cd: 设置远程机器的当前工作目录“cd()”方法在之前的范例中出现过，”with cd()”语句块可以用来设置远程机的工作目录： from fabric.api import env, cd, put env.hosts = [‘bjhee@example1.com’, ]env.password = ‘111111’ def hello(): with cd(‘/var/www/‘): put(‘/tmp/myapp-0301.tar.gz’, ‘myapp.tar.gz’)1234上例中的文件会上传到远程机的”/var/www/”目录下。出了”with cd()”语句块后，工作目录就回到初始的状态，也就是”bjhee”用户的根目录。## lcd: 设置本地工作目录“lcd()”就是”local cd”的意思，用法同”cd()”一样，区别是它设置的是本地的工作目录： from fabric.api import env, cd, lcd, put env.hosts = [‘bjhee@example1.com’, ]env.password = ‘111111’ def hello(): with cd(‘/var/www/‘): with lcd(‘/tmp/‘): put(‘myapp-0301.tar.gz’, ‘myapp.tar.gz’)123这个例子的执行效果跟上个例子一样。## path: 添加远程机的PATH路径 from fabric.api import env, run, path env.hosts = [‘bjhee@example1.com’, ]env.password = ‘111111’ def hello(): with path(‘/home/bjhee/tmp’): run(‘echo $PATH’) run(‘echo $PATH’)1234假设我们的PATH环境变量默认是”/sbin:/bin”，在上述”with path()”语句块内PATH变量将变为”/sbin:/bin:/home/bjhee/tmp”。出了with语句块后，PATH又回到原来的值。## settings: 设置Fabric环境变量参数Fabric环境变量即是我们例子中一直出现的”fabric.api.env”，它支持的参数可以从官方文档中查到。 from fabric.api import env, run, settings env.hosts = [‘bjhee@example1.com’, ]env.password = ‘111111’ def hello(): with settings(warn_only=True): run(‘echo $USER’)1234我们将环境参数”warn_only”暂时设为True，这样遇到错误时任务不会退出。## shell_env: 设置Shell环境变量可以用来临时设置远程和本地机上Shell的环境变量。 from fabric.api import env, run, local, shell_env env.hosts = [‘bjhee@example1.com’, ]env.password = ‘111111’ def hello(): with shell_env(JAVA_HOME=’/opt/java’): run(‘echo $JAVA_HOME’) local(‘echo $JAVA_HOME’)1## prefix: 设置命令执行前缀 from fabric.api import env, run, local, prefix env.hosts = [‘bjhee@example1.com’, ]env.password = ‘111111’ def hello(): with prefix(‘echo Hi’): run(‘pwd’) local(‘pwd’)1在上述”with prefix()”语句块内，所有的”run()”或”local()”方法的执行都会加上”echo Hi &amp;&amp; “前缀，也就是效果等同于： run(‘echo Hi &amp;&amp; pwd’)local(‘echo Hi &amp;&amp; pwd’)123456配合后一节我们会讲到的错误处理，它可以确保在”prefix()”方法上的命令执行成功后才会执行语句块内的命令。# 错误处理默认情况下，Fabric在任务遇到错误时就会退出，如果我们希望捕获这个错误而不是退出任务的话，就要开启”warn_only”参数。在上面介绍”settings()”上下文管理器时，我们已经看到了临时开启”warn_only”的方法了，如果要全局开启，有两个办法：在执行”fab”命令时加上”-w”参数 $ fab -w hello1设置”env.warn_only”环境参数为True from fabric.api import env env.warn_only = True1现在遇到错误时，控制台会打出一个警告信息，然后继续执行后续任务。那我们怎么捕获错误并处理呢？像”run()”, “local()”, “sudo()”, “get()”, “put()”等SSH功能函数都有返回值。当返回值的”succeeded”属性为True时，说明执行成功，反之就是失败。你也可以检查返回值的”failed”属性，为True时就表示执行失败，有错误发生。在开启”warn_only”后，你可以通过”failed”属性检查捕获错误，并执行相应的操作。 from fabric.api import env, cd, put env.hosts = [‘bjhee@example1.com’, ]env.password = ‘111111’ def hello(): with cd(‘/var/www/‘): upload = put(‘/tmp/myapp-0301.tar.gz’, ‘myapp.tar.gz’) if upload.failed: sudo(‘rm myapp.tar.gz’) put(‘/tmp/myapp-0301.tar.gz’, ‘myapp.tar.gz’, use_sudo=True)1234# 并行执行我们在介绍执行远程命令时曾提到过多台机器的任务默认情况下是串行执行的。Fabric支持并行任务，当服务器的任务之间没有依赖时，并行可以有效的加快执行速度。怎么开启并行执行呢？办法也是两个：在执行”fab”命令时加上”-P”参数 $ fab -P hello1设置”env.parallel”环境参数为True from fabric.api import env env.parallel = True1如果，我们只想对某一任务做并行的话，我们可以在任务函数上加上”@parallel”装饰器： from fabric.api import parallel @paralleldef runs_in_parallel(): pass def runs_serially(): pass1这样即便并行未开启，”runs_in_parallel()”任务也会并行执行。反过来，我们可以在任务函数上加上”@serial”装饰器： from fabric.api import serial def runs_in_parallel(): pass @serialdef runs_serially(): pass1234567这样即便并行已经开启，”runs_serially()”任务也会串行执行。**补充**：这个部分用来补充Fabric的一些特别功能：终端输出带颜色我们习惯上认为绿色表示成功，黄色表示警告，而红色表示错误，Fabric支持带这些颜色的输出来提示相应类型的信息： from fabric.colors import * def hello(): print green(“Successful”) print yellow(“Warning”) print red(“Error”)12# 限制任务只能被执行一次通过”execute()”方法，可以在一个”fab”命令中多次调用同一任务，如果想避免这个发生，就要在任务函数上加上”@runs_once”装饰器。 from fabric.api import execute, runs_once @runs_oncedef hello(): print “Hello Fabric!” def test(): execute(hello) execute(hello)```现在不管我们”execute”多少次hello任务，都只会输出一次”Hello Fabric!”字样 参考资料：https://www.bjhee.com/fabric.html 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Shell</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis集群分析]]></title>
    <url>%2F2018%2F04%2F11%2Fredis%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[为什么用集群？通常，为了提高网站响应速度，总是把热点数据保存在内存中而不是直接从后端数据库中读取。Redis是一个很好的Cache工具。大型网站应用，热点数据量往往巨大，几十G上百G是很正常的事儿，在这种情况下，如何正确架构Redis呢？ 首先，无论我们是使用自己的物理主机，还是使用云服务主机，内存资源往往是有限制的，scaleup不是一个好办法，我们需要scaleout横向可伸缩扩展，这需要由多台主机协同提供服务，即分布式多个Redis实例协同运行。 其次，目前硬件资源成本降低，多核CPU，几十G内存的主机很普遍，对于主进程是单线程工作的Redis，只运行一个实例就显得有些浪费。同时，管理一个巨大内存不如管理相对较小的内存高效。因此，实际使用中，通常一台机器上同时跑多个Redis实例。 方案1.Redis官方集群方案 Redis ClusterRedis Cluster是一种服务器Sharding技术，3.0版本开始正式提供。 Redis Cluster中，Sharding采用slot(槽)的概念，一共分成16384个槽，这有点儿类pre sharding思路。对于每个进入Redis的键值对，根据key进行散列，分配到这16384个slot中的某一个中。使用的hash算法也比较简单，就是CRC16后16384取模。 Redis集群中的每个node(节点)负责分摊这16384个slot中的一部分，也就是说，每个slot都对应一个node负责处理。当动态添加或减少node节点时，需要将16384个槽做个再分配，槽中的键值也要迁移。当然，这一过程，在目前实现中，还处于半自动状态，需要人工介入。 Redis集群，要保证16384个槽对应的node都正常工作，如果某个node发生故障，那它负责的slots也就失效，整个集群将不能工作。 为了增加集群的可访问性，官方推荐的方案是将node配置成主从结构，即一个master主节点，挂n个slave从节点。这时，如果主节点失效，Redis Cluster会根据选举算法从slave节点中选择一个上升为主节点，整个集群继续对外提供服务。这非常类似前篇文章提到的Redis Sharding场景下服务器节点通过Sentinel监控架构成主从结构，只是Redis Cluster本身提供了故障转移容错的能力。 Redis Cluster的新节点识别能力、故障判断及故障转移能力是通过集群中的每个node都在和其它nodes进行通信，这被称为集群总线(cluster bus)。它们使用特殊的端口号，即对外服务端口号加10000。例如如果某个node的端口号是6379，那么它与其它nodes通信的端口号是16379。nodes之间的通信采用特殊的二进制协议。 对客户端来说，整个cluster被看做是一个整体，客户端可以连接任意一个node进行操作，就像操作单一Redis实例一样，当客户端操作的key没有分配到该node上时，Redis会返回转向指令，指向正确的node，这有点儿像浏览器页面的302 redirect跳转。 Redis Cluster是Redis 3.0以后才正式推出，时间较晚，目前能证明在大规模生产环境下成功的案例还不是很多，需要时间检验。 2.Redis Sharding集群Redis 3正式推出了官方集群技术，解决了多Redis实例协同服务问题。Redis Cluster可以说是服务端Sharding分片技术的体现，即将键值按照一定算法合理分配到各个实例分片上，同时各个实例节点协调沟通，共同对外承担一致服务。 多Redis实例服务，比单Redis实例要复杂的多，这涉及到定位、协同、容错、扩容等技术难题。这里，我们介绍一种轻量级的客户端Redis Sharding技术。 Redis Sharding可以说是Redis Cluster出来之前，业界普遍使用的多Redis实例集群方法。其主要思想是采用哈希算法将Redis数据的key进行散列，通过hash函数，特定的key会映射到特定的Redis节点上。这样，客户端就知道该向哪个Redis节点操作数据。Sharding架构如图： 庆幸的是，java redis客户端驱动jedis，已支持Redis Sharding功能，即ShardedJedis以及结合缓存池的ShardedJedisPool。 Jedis的Redis Sharding实现具有如下特点： 1.采用一致性哈希算法(consistent hashing)，将key和节点name同时hashing，然后进行映射匹配，采用的算法是MURMUR_HASH。采用一致性哈希而不是采用简单类似哈希求模映射的主要原因是当增加或减少节点时，不会产生由于重新匹配造成的rehashing。一致性哈希只影响相邻节点key分配，影响量小。 2.为了避免一致性哈希只影响相邻节点造成节点分配压力，ShardedJedis会对每个Redis节点根据名字(没有，Jedis会赋予缺省名字)会虚拟化出160个虚拟节点进行散列。根据权重weight，也可虚拟化出160倍数的虚拟节点。用虚拟节点做映射匹配，可以在增加或减少Redis节点时，key在各Redis节点移动再分配更均匀，而不是只有相邻节点受影响。 3.ShardedJedis支持keyTagPattern模式，即抽取key的一部分keyTag做sharding，这样通过合理命名key，可以将一组相关联的key放入同一个Redis节点，这在避免跨节点访问相关数据时很重要。 扩容问题Redis Sharding采用客户端Sharding方式，服务端Redis还是一个个相对独立的Redis实例节点，没有做任何变动。同时，我们也不需要增加额外的中间处理组件，这是一种非常轻量、灵活的Redis多实例集群方法。 当然，Redis Sharding这种轻量灵活方式必然在集群其它能力方面做出妥协。比如扩容，当想要增加Redis节点时，尽管采用一致性哈希，毕竟还是会有key匹配不到而丢失，这时需要键值迁移。 作为轻量级客户端sharding，处理Redis键值迁移是不现实的，这就要求应用层面允许Redis中数据丢失或从后端数据库重新加载数据。但有些时候，击穿缓存层，直接访问数据库层，会对系统访问造成很大压力。有没有其它手段改善这种情况？ Redis作者给出了一个比较讨巧的办法–presharding，即预先根据系统规模尽量部署好多个Redis实例，这些实例占用系统资源很小，一台物理机可部署多个，让他们都参与sharding，当需要扩容时，选中一个实例作为主节点，新加入的Redis节点作为从节点进行数据复制。数据同步后，修改sharding配置，让指向原实例的Shard指向新机器上扩容后的Redis节点，同时调整新Redis节点为主节点，原实例可不再使用。 presharding是预先分配好足够的分片，扩容时只是将属于某一分片的原Redis实例替换成新的容量更大的Redis实例。参与sharding的分片没有改变，所以也就不存在key值从一个区转移到另一个分片区的现象，只是将属于同分片区的键值从原Redis实例同步到新Redis实例。 并不是只有增删Redis节点引起键值丢失问题，更大的障碍来自Redis节点突然宕机。在《Redis持久化》一文中已提到，为不影响Redis性能，尽量不开启AOF和RDB文件保存功能，可架构Redis主备模式，主Redis宕机，数据不会丢失，备Redis留有备份。 这样，我们的架构模式变成一个Redis节点切片包含一个主Redis和一个备Redis。在主Redis宕机时，备Redis接管过来，上升为主Redis，继续提供服务。主备共同组成一个Redis节点，通过自动故障转移，保证了节点的高可用性。则Sharding架构演变成： Redis Sentinel提供了主备模式下Redis监控、故障转移功能达到系统的高可用性。 高访问量下，即使采用Sharding分片，一个单独节点还是承担了很大的访问压力，这时我们还需要进一步分解。通常情况下，应用访问Redis读操作量和写操作量差异很大，读常常是写的数倍，这时我们可以将读写分离，而且读提供更多的实例数。 可以利用主从模式实现读写分离，主负责写，从负责只读，同时一主挂多个从。在Sentinel监控下，还可以保障节点故障的自动监测。 3.利用代理中间件实现大规模Redis集群上面分别介绍了多Redis服务器集群的两种方式，它们是基于客户端sharding的Redis Sharding和基于服务端sharding的Redis Cluster。 客户端sharding技术其优势在于服务端的Redis实例彼此独立，相互无关联，每个Redis实例像单服务器一样运行，非常容易线性扩展，系统的灵活性很强。其不足之处在于： 1.由于sharding处理放到客户端，规模进步扩大时给运维带来挑战。 2.服务端Redis实例群拓扑结构有变化时，每个客户端都需要更新调整。 3.连接不能共享，当应用规模增大时，资源浪费制约优化。 服务端sharding的Redis Cluster其优势在于服务端Redis集群拓扑结构变化时，客户端不需要感知，客户端像使用单Redis服务器一样使用Redis集群，运维管理也比较方便。 不过Redis Cluster正式版推出时间不长，系统稳定性、性能等都需要时间检验，尤其在大规模使用场合。 能不能结合二者优势？即能使服务端各实例彼此独立，支持线性可伸缩，同时sharding又能集中处理，方便统一管理？本篇介绍的Redis代理中间件twemproxy就是这样一种利用中间件做sharding的技术。 twemproxy处于客户端和服务器的中间，将客户端发来的请求，进行一定的处理后(如sharding)，再转发给后端真正的Redis服务器。也就是说，客户端不直接访问Redis服务器，而是通过twemproxy代理中间件间接访问。 参照Redis Sharding架构，增加代理中间件的Redis集群架构如下： twemproxy中间件的内部处理是无状态的，它本身可以很轻松地集群，这样可避免单点压力或故障。 twemproxy又叫nutcracker，起源于twitter系统中redis/memcached集群开发实践，twemproxy后端不仅支持redis，同时也支持memcached，这是twitter系统具体环境造成的。 由于使用了中间件，twemproxy可以通过共享与后端系统的连接，降低客户端直接连接后端服务器的连接数量。同时，它也提供sharding功能，支持后端服务器集群水平扩展。统一运维管理也带来了方便。 当然，也是由于使用了中间件代理，相比客户端直连服务器方式，性能上会有所损耗，实测结果大约降低了20%左右。 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[判断一个单链表是否有环]]></title>
    <url>%2F2018%2F03%2F26%2F%E5%88%A4%E6%96%AD%E4%B8%80%E4%B8%AA%E5%8D%95%E9%93%BE%E8%A1%A8%E6%98%AF%E5%90%A6%E6%9C%89%E7%8E%AF%2F</url>
    <content type="text"><![CDATA[三类情况：（1）（2）（3）1、遇到这个问题，首先想到的是遍历链表，寻找是否有相同地址，借此判断链表中是否有环。123456789101112131415listnode_ptr current =head-&gt;next;while(current)&#123; if(current==head) &#123; printf(&quot;有环！\n&quot;); return 0; &#125; else &#123; current=current-&gt;next; &#125;&#125;printf(&quot;无环！\n&quot;);return 0; 这段代码满足了（1）（链表无环）、（2）（链表头尾相连)两类情况，却没有将（3）情况考虑在内，如果出现（3）类情况，程序会进入死循环。 2、将（3）考虑在内，首先想到我们可能需要一块空间来存储指针，遍历新指针时将其和储存的旧指针比对，若有相同指针，则该链表有环，否则将这个新指针存下来后继续往下读取，直到遇见NULL，这说明这个链表无环。 上述方法虽然可行，可是否还有更简便的算法？ 3、假设有两个学生A和B在跑道上跑步，两人从相同起点出发，假设A的速度为2m/s，B的速度为1m/s,结果会发生什么？ 答案很简单，A绕了跑道一圈之后会追上B！ 将这个问题延伸到链表中，跑道就是链表，我们可以设置两个指针，a跑的快，b跑的慢，如果链表有环，那么当程序执行到某一状态时，a==b。如果链表没有环，程序会执行到a==NULL，结束。 1234567891011121314151617181920212223242526listnode_ptr fast=head-&gt;next; listnode_ptr slow=head;while(fast)&#123; if(fast==slow) &#123; printf(&quot;环！\n&quot;); return 0; &#125; else &#123; fast=fast-&gt;next; if(!fast) &#123; printf(&quot;无环！\n&quot;); return 0; &#125; else &#123; fast=fast-&gt;next; slow=slow-&gt;next; &#125; &#125;&#125;printf(&quot;无环！\n&quot;); return 0; 4、关于算法复杂度： 如图，链表长度为n，环节点个数为m，则循环 t=n-m 次时，slow进入环中，此时，我们假设fast与slow相距x个节点，那么，经过t’次循环，二者相遇时，有：2t’=t’+（m-x） -&gt; t’=m-x -&gt; t’&lt;=m.因此，总共循环了T=t+t’ &lt;= n. 算法复杂度为O(n). 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 和 Memcached比较]]></title>
    <url>%2F2018%2F03%2F26%2FRedis%E5%92%8CMemcached%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[1. MySql+Memcached架构的问题 实际MySQL是适合进行海量数据存储的，通过Memcached将热点数据加载到cache，加速访问，很多公司都曾经使用过这样的架构，但随着业务数据量的不断增加，和访问量的持续增长，我们遇到了很多问题： 1.MySQL需要不断进行拆库拆表，Memcached也需不断跟着扩容，扩容和维护工作占据大量开发时间。 2.Memcached与MySQL数据库数据一致性问题。 3.Memcached数据命中率低或down机，大量访问直接穿透到DB，MySQL无法支撑。 4.跨机房cache同步问题。 众多NoSQL百花齐放，如何选择 最近几年，业界不断涌现出很多各种各样的NoSQL产品，那么如何才能正确地使用好这些产品，最大化地发挥其长处，是我们需要深入研究和思考的问题，实际归根结底最重要的是了解这些产品的定位，并且了解到每款产品的tradeoffs，在实际应用中做到扬长避短，总体上这些NoSQL主要用于解决以下几种问题 1.少量数据存储，高速读写访问。此类产品通过数据全部in-momery 的方式来保证高速访问，同时提供数据落地的功能，实际这正是Redis最主要的适用场景。 2.海量数据存储，分布式系统支持，数据一致性保证，方便的集群节点添加/删除。 3.这方面最具代表性的是dynamo和bigtable 2篇论文所阐述的思路。前者是一个完全无中心的设计，节点之间通过gossip方式传递集群信息，数据保证最终一致性，后者是一个中心化的方案设计，通过类似一个分布式锁服务来保证强一致性,数据写入先写内存和redo log，然后定期compat归并到磁盘上，将随机写优化为顺序写，提高写入性能。 4.Schema free，auto-sharding等。比如目前常见的一些文档数据库都是支持schema-free的，直接存储json格式数据，并且支持auto-sharding等功能，比如mongodb。 面对这些不同类型的NoSQL产品,我们需要根据我们的业务场景选择最合适的产品。 Redis最适合所有数据in-momory的场景，虽然Redis也提供持久化功能，但实际更多的是一个disk-backed的功能，跟传统意义上的持久化有比较大的差别，那么可能大家就会有疑问，似乎Redis更像一个加强版的Memcached，那么何时使用Memcached,何时使用Redis呢? 如果简单地比较Redis与Memcached的区别，大多数都会得到以下观点： 1 、Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。2 、Redis支持数据的备份，即master-slave模式的数据备份。3 、Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。 2. Redis常用数据类型Redis最为常用的数据类型主要有以下：String Hash List Set Sorted set pub/sub Transactions 在具体描述这几种数据类型之前，我们先通过一张图了解下Redis内部内存管理中是如何描述这些不同数据类型的： 首先Redis内部使用一个redisObject对象来表示所有的key和value,redisObject最主要的信息如上图所示： type代表一个value对象具体是何种数据类型， encoding是不同数据类型在redis内部的存储方式， 比如：type=string代表value存储的是一个普通字符串，那么对应的encoding可以是raw或者是int,如果是int则代表实际redis内部是按数值型类存储和表示这个字符串的，当然前提是这个字符串本身可以用数值表示，比如:”123” “456”这样的字符串。 这里需要特殊说明一下vm字段，只有打开了Redis的虚拟内存功能，此字段才会真正的分配内存，该功能默认是关闭状态的，该功能会在后面具体描述。 通过上图我们可以发现Redis使用redisObject来表示所有的key/value数据是比较浪费内存的，当然这些内存管理成本的付出主要也是为了给Redis不同数据类型提供一个统一的管理接口，实际作者也提供了多种方法帮助我们尽量节省内存使用，我们随后会具体讨论。 3. 各种数据类型应用和实现方式 下面我们先来逐一的分析下这7种数据类型的使用和内部实现方式: String:Strings数据结构是简单的key-value类型，value其实不仅是String，也可以是数字.常用命令: set,get,decr,incr,mget 等。 应用场景：String是最常用的一种数据类型，普通的key/ value 存储都可以归为此类.即可以完全实现目前 Memcached 的功能，并且效率更高。还可以享受Redis的定时持久化，操作日志及 Replication等功能。 除了提供与 Memcached 一样的get、set、incr、decr 等操作外，Redis还提供了下面一些操作： 获取字符串长度往字符串append内容 设置和获取字符串的某一段内容 设置及获取字符串的某一位（bit） 批量设置一系列字符串的内容 实现方式： String在redis内部存储默认就是一个字符串，被redisObject所引用，当遇到incr,decr等操作时会转成数值型进行计算，此时redisObject的encoding字段为int。 Hash常用命令：hget,hset,hgetall 等。 应用场景：在Memcached中，我们经常将一些结构化的信息打包成HashMap，在客户端序列化后存储为一个字符串的值，比如用户的昵称、年龄、性别、积分等，这时候在需要修改其中某一项时，通常需要将所有值取出反序列化后，修改某一项的值，再序列化存储回去。这样不仅增大了开销，也不适用于一些可能并发操作的场合（比如两个并发的操作都需要修改积分）。而Redis的Hash结构可以使你像在数据库中Update一个属性一样只修改某一项属性值。 我们简单举个实例来描述下Hash的应用场景，比如我们要存储一个用户信息对象数据，包含以下信息：用户ID为查找的key，存储的value用户对象包含姓名，年龄，生日等信息，如果用普通的key/value结构来存储，主要有以下2种存储方式： 第一种方式将用户ID作为查找key,把其他信息封装成一个对象以序列化的方式存储，这种方式的缺点是，增加了序列化/反序列化的开销，并且在需要修改其中一项信息时，需要把整个对象取回，并且修改操作需要对并发进行保护，引入CAS等复杂问题。 第二种方法是这个用户信息对象有多少成员就存成多少个key-value对儿，用用户ID+对应属性的名称作为唯一标识来取得对应属性的值，虽然省去了序列化开销和并发问题，但是用户ID为重复存储，如果存在大量这样的数据，内存浪费还是非常可观的。那么Redis提供的Hash很好的解决了这个问题，Redis的Hash实际是内部存储的Value为一个HashMap，并提供了直接存取这个Map成员的接口，如下图： 也就是说，Key仍然是用户ID, value是一个Map，这个Map的key是成员的属性名，value是属性值，这样对数据的修改和存取都可以直接通过其内部Map的Key(Redis里称内部Map的key为field), 也就是通过 key(用户ID) + field(属性标签) 就可以操作对应属性数据了，既不需要重复存储数据，也不会带来序列化和并发修改控制的问题。很好的解决了问题。 这里同时需要注意，Redis提供了接口(hgetall)可以直接取到全部的属性数据,但是如果内部Map的成员很多，那么涉及到遍历整个内部Map的操作，由于Redis单线程模型的缘故，这个遍历操作可能会比较耗时，而另其它客户端的请求完全不响应，这点需要格外注意。 实现方式：上面已经说到Redis Hash对应Value内部实际就是一个HashMap，实际这里会有2种不同实现，这个Hash的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，对应的value redisObject的encoding为zipmap,当成员数量增大时会自动转成真正的HashMap,此时encoding为ht。 List常用命令：lpush,rpush,lpop,rpop,lrange等。应用场景：Redis list的应用场景非常多，也是Redis最重要的数据结构之一，比如twitter的关注列表，粉丝列表等都可以用Redis的list结构来实现。 Lists 就是链表，相信略有数据结构知识的人都应该能理解其结构。使用Lists结构，我们可以轻松地实现最新消息排行等功能。Lists的另一个应用就是消息队列，可以利用Lists的PUSH操作，将任务存在Lists中，然后工作线程再用POP操作将任务取出进行执行。Redis还提供了操作Lists中某一段的api，你可以直接查询，删除Lists中某一段的元素。 实现方式：Redis list的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销，Redis内部的很多实现，包括发送缓冲队列等也都是用的这个数据结构。 Set常用命令：sadd,spop,smembers,sunion 等。应用场景：Redis set对外提供的功能与list类似是一个列表的功能，特殊之处在于set是可以自动排重的，当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的。 Sets 集合的概念就是一堆不重复值的组合。利用Redis提供的Sets数据结构，可以存储一些集合性的数据，比如在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。 Redis还为集合提供了求交集、并集、差集等操作，可以非常方便的实现如共同关注、共同喜好、二度好友等功能，对上面的所有集合操作，你还可以使用不同的命令选择将结果返回给客户端还是存集到一个新的集合中。 实现方式：set 的内部实现是一个 value永远为null的HashMap，实际就是通过计算hash的方式来快速排重的，这也是set能提供判断一个成员是否在集合内的原因。 Sorted Set常用命令：zadd,zrange,zrem,zcard等使用场景：Redis sorted set的使用场景与set类似，区别是set不是自动有序的，而sorted set可以通过用户额外提供一个优先级(score)的参数来为成员排序，并且是插入有序的，即自动排序。 当你需要一个有序的并且不重复的集合列表，那么可以选择sorted set数据结构，比如twitter 的public timeline可以以发表时间作为score来存储，这样获取时就是自动按时间排好序的。 另外还可以用Sorted Sets来做带权重的队列，比如普通消息的score为1，重要消息的score为2，然后工作线程可以选择按score的倒序来获取工作任务。让重要的任务优先执行。 实现方式：Redis sorted set的内部使用HashMap和跳跃表(SkipList)来保证数据的存储和有序，HashMap里放的是成员到score的映射，而跳跃表里存放的是所有的成员，排序依据是HashMap里存的score,使用跳跃表的结构可以获得比较高的查找效率，并且在实现上比较简单。 Pub/SubPub/Sub从字面上理解就是发布（Publish）与订阅（Subscribe），在Redis中，你可以设定对某一个key值进行消息发布及消息订阅，当一个key值上进行了消息发布后，所有订阅它的客户端都会收到相应的消息。 这一功能最明显的用法就是用作实时消息系统，比如普通的即时聊天，群聊等功能。 Transactions谁说NoSQL都不支持事务，虽然Redis的Transactions提供的并不是严格的ACID的事务（比如一串用EXEC提交执行的命令，在执行中服务器宕机，那么会有一部分命令执行了，剩下的没执行），但是这个Transactions还是提供了基本的命令打包执行的功能（在服务器不出问题的情况下，可以保证一连串的命令是顺序在一起执行的，中间有会有其它客户端命令插进来执行）。 Redis还提供了一个Watch功能，你可以对一个key进行Watch，然后再执行Transactions，在这过程中，如果这个Watched的值进行了修改，那么这个Transactions会发现并拒绝执行。 4. Redis实际应用场景 Redis在很多方面与其他数据库解决方案不同：它使用内存提供主存储支持，而仅使用硬盘做持久性的存储； 它的数据模型非常独特，用的是单线程。另一个大区别在于，你可以在开发环境中使用Redis的功能，但却不需要转到Redis。转向Redis当然也是可取的，许多开发者从一开始就把Redis作为首选数据库；但设想如果你的开发环境已经搭建好，应用已经在上面运行了，那么更换数据库框架显然不那么容易。另外在一些需要大容量数据集的应用，Redis也并不适合，因为它的数据集不会超过系统可用的内存。所以如果你有大数据应用，而且主要是读取访问模式，那么Redis并不是正确的选择。 然而我喜欢Redis的一点就是你可以把它融入到你的系统中来，这就能够解决很多问题，比如那些你现有的数据库处理起来感到缓慢的任务。这些你就可以通过Redis来进行优化，或者为应用创建些新的功能。在本文中，我就想探讨一些怎样将Redis加入到现有的环境中，并利用它的原语命令等功能来解决 传统环境中碰到的一些常见问题。 在这些例子中，Redis都不是作为首选数据库。 1、显示最新的项目列表下面这个语句常用来显示最新项目，随着数据多了，查询毫无疑问会越来越慢。 1SELECT * FROM foo WHERE ... ORDER BY time DESC LIMIT 10 在Web应用中，“列出最新的回复”之类的查询非常普遍，这通常会带来可扩展性问题。这令人沮丧，因为项目本来就是按这个顺序被创建的，但要输出这个顺序却不得不进行排序操作。 类似的问题就可以用Redis来解决。比如说，我们的一个Web应用想要列出用户贴出的最新20条评论。在最新的评论边上我们有一个“显示全部”的链接，点击后就可以获得更多的评论。 我们假设数据库中的每条评论都有一个唯一的递增的ID字段。 我们可以使用分页来制作主页和评论页，使用Redis的模板，每次新评论发表时，我们会将它的ID添加到一个Redis列表：1LPUSH latest.comments &lt;ID&gt; 我们将列表裁剪为指定长度，因此Redis只需要保存最新的5000条评论： 1LTRIM latest.comments 0 5000 每次我们需要获取最新评论的项目范围时，我们调用一个函数来完成（使用伪代码）： 1234567 FUNCTION get_latest_comments(start, num_items): id_list = redis.lrange(&quot;latest.comments&quot;,start,start+num_items - 1) IF id_list.length &lt; num_items id_list = SQL_DB(&quot;SELECT ... ORDER BY time LIMIT ...&quot;) END RETURN id_list END 这里我们做的很简单。在Redis中我们的最新ID使用了常驻缓存，这是一直更新的。但是我们做了限制不能超过5000个ID，因此我们的获取ID函数会一直询问Redis。只有在start/count参数超出了这个范围的时候，才需要去访问数据库。 我们的系统不会像传统方式那样“刷新”缓存，Redis实例中的信息永远是一致的。SQL数据库（或是硬盘上的其他类型数据库）只是在用户需要获取“很远”的数据时才会被触发，而主页或第一个评论页是不会麻烦到硬盘上的数据库了。 2、删除与过滤我们可以使用LREM来删除评论。如果删除操作非常少，另一个选择是直接跳过评论条目的入口，报告说该评论已经不存在。 有些时候你想要给不同的列表附加上不同的过滤器。如果过滤器的数量受到限制，你可以简单的为每个不同的过滤器使用不同的Redis列表。毕竟每个列表只有5000条项目，但Redis却能够使用非常少的内存来处理几百万条项目。 3、排行榜相关另一个很普遍的需求是各种数据库的数据并非存储在内存中，因此在按得分排序以及实时更新这些几乎每秒钟都需要更新的功能上数据库的性能不够理想。 典型的比如那些在线游戏的排行榜，比如一个Facebook的游戏，根据得分你通常想要： 列出前100名高分选手 列出某用户当前的全球排名 这些操作对于Redis来说小菜一碟，即使你有几百万个用户，每分钟都会有几百万个新的得分。 模式是这样的，每次获得新得分时，我们用这样的代码：1ZADD leaderboard &lt;score&gt; &lt;username&gt; 你可能用userID来取代username，这取决于你是怎么设计的。 得到前100名高分用户很简单：1ZREVRANGE leaderboard 0 99 用户的全球排名也相似，只需要：1ZRANK leaderboard &lt;username&gt; 4、按照用户投票和时间排序排行榜的一种常见变体模式就像Reddit或Hacker News用的那样，新闻按照类似下面的公式根据得分来排序： score = points / time^alpha 因此用户的投票会相应的把新闻挖出来，但时间会按照一定的指数将新闻埋下去。下面是我们的模式，当然算法由你决定。 模式是这样的，开始时先观察那些可能是最新的项目，例如首页上的1000条新闻都是候选者，因此我们先忽视掉其他的，这实现起来很简单。 每次新的新闻贴上来后，我们将ID添加到列表中，使用LPUSH + LTRIM，确保只取出最新的1000条项目。 有一项后台任务获取这个列表，并且持续的计算这1000条新闻中每条新闻的最终得分。计算结果由ZADD命令按照新的顺序填充生成列表，老新闻则被清除。这里的关键思路是排序工作是由后台任务来完成的。 5、处理过期项目另一种常用的项目排序是按照时间排序。我们使用unix时间作为得分即可。 模式如下： 每次有新项目添加到我们的非Redis数据库时，我们把它加入到排序集合中。这时我们用的是时间属性，current_time和time_to_live。 另一项后台任务使用ZRANGE…SCORES查询排序集合，取出最新的10个项目。如果发现unix时间已经过期，则在数据库中删除条目。 6、计数Redis是一个很好的计数器，这要感谢INCRBY和其他相似命令。 我相信你曾许多次想要给数据库加上新的计数器，用来获取统计或显示新信息，但是最后却由于写入敏感而不得不放弃它们。 好了，现在使用Redis就不需要再担心了。有了原子递增（atomic increment），你可以放心的加上各种计数，用GETSET重置，或者是让它们过期。 例如这样操作：1INCR user:&lt;id&gt; EXPIRE user:&lt;id&gt; 60 你可以计算出最近用户在页面间停顿不超过60秒的页面浏览量，当计数达到比如20时，就可以显示出某些条幅提示，或是其它你想显示的东西。 7、特定时间内的特定项目另一项对于其他数据库很难，但Redis做起来却轻而易举的事就是统计在某段特点时间里有多少特定用户访问了某个特定资源。比如我想要知道某些特定的注册用户或IP地址，他们到底有多少访问了某篇文章。 每次我获得一次新的页面浏览时我只需要这样做：1SADD page:day1:&lt;page_id&gt; &lt;user_id&gt; 当然你可能想用unix时间替换day1，比如time()-(time()%3600*24)等等。 想知道特定用户的数量吗？只需要使用1SCARD page:day1:&lt;page_id&gt; 需要测试某个特定用户是否访问了这个页面？1SISMEMBER page:day1:&lt;page_id&gt; 8、实时分析正在发生的情况用于数据统计与防止垃圾邮件等 我们只做了几个例子，但如果你研究Redis的命令集，并且组合一下，就能获得大量的实时分析方法，有效而且非常省力。使用Redis原语命令，更容易实施垃圾邮件过滤系统或其他实时跟踪系统。 9、Pub/SubRedis的Pub/Sub非常非常简单，运行稳定并且快速。支持模式匹配，能够实时订阅与取消频道。 10、队列你应该已经注意到像list push和list pop这样的Redis命令能够很方便的执行队列操作了，但能做的可不止这些：比如Redis还有list pop的变体命令，能够在列表为空时阻塞队列。 现代的互联网应用大量地使用了消息队列（Messaging）。消息队列不仅被用于系统内部组件之间的通信，同时也被用于系统跟其它服务之间的交互。 消息队列的使用可以增加系统的可扩展性、灵活性和用户体验。 非基于消息队列的系统，其运行速度取决于系统中最慢的组件的速度（注：短板效应）。而基于消息队列可以将系统中各组件解除耦合，这样系统就不再受最慢组件的束缚，各组件可以异步运行从而得以更快的速度完成各自的工作。 此外，当服务器处在高并发操作的时候，比如频繁地写入日志文件。可以利用消息队列实现异步处理。从而实现高性能的并发操作。 11、缓存Redis的缓存部分值得写一篇新文章，我这里只是简单的说一下。Redis能够替代memcached，让你的缓存从只能存储数据变得能够更新数据，因此你不再需要每次都重新生成数据了。 数据持久化支持Redis虽然是基于内存的存储系统，但是它本身是支持内存数据的持久化的，而且提供两种主要的持久化策略：RDB快照和AOF日志。 而memcached是不支持数据持久化操作的。 1）RDB快照Redis支持将当前数据的快照存成一个数据文件的持久化机制，即RDB快照。但是一个持续写入的数据库如何生成快照呢？ Redis借助了fork命令的copy on write机制。在生成快照时，将当前进程fork出一个子进程，然后在子进程中循环所有的数据，将数据写成为RDB文件。 我们可以通过Redis的save指令来配置RDB快照生成的时机，比如配置10分钟就生成快照，也可以配置有1000次写入就生成快照，也可以多个规则一起实施。 这些规则的定义就在Redis的配置文件中，你也可以通过Redis的CONFIG SET命令在Redis运行时设置规则，不需要重启Redis。 Redis的RDB文件不会坏掉，因为其写操作是在一个新进程中进行的，当生成一个新的RDB文件时，Redis生成的子进程会先将数据写到一个临时文件中，然后通过原子性rename系统调用将临时文件重命名为RDB文件，这样在任何时候出现故障，Redis的RDB文件都总是可用的。 同时，Redis的RDB文件也是Redis主从同步内部实现中的一环。RDB有他的不足，就是一旦数据库出现问题，那么我们的RDB文件中保存的数据并不是全新的，从上次RDB文件生成到Redis停机这段时间的数据全部丢掉了。在某些业务下，这是可以忍受的。 2）AOF日志AOF日志的全称是append only file，它是一个追加写入的日志文件。与一般数据库的binlog不同的是，AOF文件是可识别的纯文本，它的内容就是一个个的Redis标准命令。只有那些会导致数据发生修改的命令才会追加到AOF文件。每一条修改数据的命令都生成一条日志。 AOF文件会越来越大，所以Redis又提供了一个功能，叫做AOF rewrite。其功能就是重新生成一份AOF文件，新的AOF文件中一条记录的操作只会有一次，而不像一份老文件那样，可能记录了对同一个值的多次操作。其生成过程和RDB类似，也是fork一个进程，直接遍历数据，写入新的AOF临时文件。 在写入新文件的过程中，所有的写操作日志还是会写到原来老的AOF文件中，同时还会记录在内存缓冲区中。当重完操作完成后，会将所有缓冲区中的日志一次性写入到临时文件中。然后调用原子性的rename命令用新的AOF文件取代老的AOF文件。 AOF是一个写文件操作，其目的是将操作日志写到磁盘上，所以它也同样会遇到我们上面说的写操作的流程。在Redis中对AOF调用write写入后，通过appendfsync选项来控制调用fsync将其写到磁盘上的时间，下面appendfsync的三个设置项，安全强度逐渐变强。 appendfsync no 当设置appendfsync为no的时候，Redis不会主动调用fsync去将AOF日志内容同步到磁盘，所以这一切就完全依赖于操作系统的调试了。对大多数Linux操作系统，是每30秒进行一次fsync，将缓冲区中的数据写到磁盘上。 appendfsync everysec 当设置appendfsync为everysec的时候，Redis会默认每隔一秒进行一次fsync调用，将缓冲区中的数据写到磁盘。但是当这一次的fsync调用时长超过1秒时。Redis会采取延迟fsync的策略，再等一秒钟。也就是在两秒后再进行fsync，这一次的fsync就不管会执行多长时间都会进行。这时候由于在fsync时文件描述符会被阻塞，所以当前的写操作就会阻塞。 所以结论就是，在绝大多数情况下，Redis会每隔一秒进行一次fsync。在最坏的情况下，两秒钟会进行一次fsync操作。这一操作在大多数数据库系统中被称为group commit，就是组合多次写操作的数据，一次性将日志写到磁盘。 appednfsync always 当设置appendfsync为always时，每一次写操作都会调用一次fsync，这时数据是最安全的，当然，由于每次都会执行fsync，所以其性能也会受到影响。 对于一般性的业务需求，建议使用RDB的方式进行持久化，原因是RDB的开销并相比AOF日志要低很多，对于那些无法忍数据丢失的应用，建议使用AOF日志。 集群管理的不同Memcached是全内存的数据缓冲系统，Redis虽然支持数据的持久化，但是全内存毕竟才是其高性能的本质。 作为基于内存的存储系统来说，机器物理内存的大小就是系统能够容纳的最大数据量。如果需要处理的数据量超过了单台机器的物理内存大小，就需要构建分布式集群来扩展存储能力。 Memcached本身并不支持分布式，因此只能在客户端通过像一致性哈希这样的分布式算法来实现Memcached的分布式存储。 当客户端向Memcached集群发送数据之前，首先会通过内置的分布式算法计算出该条数据的目标节点，然后数据会直接发送到该节点上存储。但客户端查询数据时，同样要计算出查询数据所在的节点，然后直接向该节点发送查询请求以获取数据。 相较于Memcached只能采用客户端实现分布式存储，Redis更偏向于在服务器端构建分布式存储。最新版本的Redis已经支持了分布式存储功能。 Redis Cluster是一个实现了分布式且允许单点故障的Redis高级版本，它没有中心节点，具有线性可伸缩的功能。 Redis Cluster将整个key的数值域分成4096个哈希槽，每个节点上可以存储一个或多个哈希槽，也就是说当前Redis Cluster支持的最大节点数就是4096。 Redis Cluster使用的分布式算法也很简单：crc16( key ) % HASH_SLOTS_NUMBER。 为了保证单点故障下的数据可用性，Redis Cluster引入了Master节点和Slave节点。 在Redis Cluster中，每个Master节点都会有对应的两个用于冗余的Slave节点。 这样在整个集群中，任意两个节点的宕机都不会导致数据的不可用。当Master节点退出后，集群会自动选择一个Slave节点成为新的Master节点。 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[grep统计某个单词的个数，行数等]]></title>
    <url>%2F2018%2F03%2F12%2Fgrep%E7%BB%9F%E8%AE%A1%E6%9F%90%E4%B8%AA%E5%8D%95%E8%AF%8D%E7%9A%84%E4%B8%AA%E6%95%B0%EF%BC%8C%E8%A1%8C%E6%95%B0%E7%AD%89%2F</url>
    <content type="text"><![CDATA[文件内容 1234567j@j:~$ cat hello.c#include&lt;stdio.h&gt;//include includeint main()&#123; printf(&quot;hello world/n&quot;); exit(0);&#125; grep命令查看include关键词 1234j@j:~$ grep -o include hello.cincludeincludeinclude 如上所示， 所有的include都显示出来了， 包括有两个include的行。 命令1grep -c -o include hello.c 的结果是2， -c参数只能统计行。如果要统计个数， 可以再来个重定向： 1grep -o include hello.c| grep -c include 得到include的个数 3 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用meerkat进行服务监控和服务降级]]></title>
    <url>%2F2018%2F01%2F29%2F%E4%BD%BF%E7%94%A8meerkat%E8%BF%9B%E8%A1%8C%E6%9C%8D%E5%8A%A1%E7%9B%91%E6%8E%A7%E5%92%8C%E6%9C%8D%E5%8A%A1%E9%99%8D%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[meerkat进行服务监控和服务降级meerkat 是用于服务监控以及服务降级基础组件，主要为了解决调用外部接口的时候进行成功率，响应时间，QPS指标的监控，同时在成功率下降到预设的阈值以下的时候自动切断外部接口的调用，外部接口成功率恢复后自动恢复请求。本文将对使用方式以及进阶特性进行介绍。 项目主页： https://github.com/ChanningBJ… 为什么要进行监控和熔断在我们的Java服务中，经常会调用外部的一些接口进行数据的获取操作，当这些外部接口的成功率比较低的时候会直接影响到服务本身的成功率，因此我们添加了对外部接口调用的成功率和响应时间监控，这样可以在造成大量用户影响之前预先发现并解决问题。同时，对于接口中的非关键数据，我们采取了更具成功率判断进行触发熔断的方式，当成功率下降到预定的阀值以下的时候自动停止对这个外部接口的访问以便保证关键数据能够正常提供，当成功率恢复以后自动恢复请求。 meerkat主要功能监控：监控Java内部操作的成功率以及响应时间指标上报：log文件和Grafhite两种监控指标上报方式，支持扩展其他的上报方式熔断：（可选功能）成功率下降到预设的阈值以下触发熔断保护，暂定对外部接口的访问，成功率恢复以后自动恢复访问 基本使用12345&lt;dependency&gt; &lt;groupId&gt;com.github.channingbj&lt;/groupId&gt; &lt;artifactId&gt;meerkat&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt;&lt;/dependency&gt; 定义受监控的操作假设我们的服务中需要从HTTP接口查询一个节目的播放次数，为了防止这个HTTP接口大量超时影响我们自身服务的质量，可以定义一个查询Command：123456789101112131415161718192021public class GetPlayCountCommand extends FusingCommand&lt;Long&gt; &#123; private final Long videoID; public GetPlayCountCommand(Long videoID) &#123; this.videoID = videoID; &#125; protected Optional&lt;Long&gt; run() &#123; Long result = 0l; // 调用HTTP接口获取视频的播放次数信息 // 如果调用失败，返回 null 或者抛出异常，会将这次操作记录为失败 // 如果ID非法，返回 Optional.absent(),会将这次操作记录为成功 return Optional.fromNullable(result); &#125;&#125;执行查询：//获取视频ID为123的视频的播放次数GetPlayCountCommand command = new GetPlayCountCommand(123l);Long result = command.execute(); // 执行查询操作，如果执行失败或者处于熔断状态，返回 null 配置监控上报在服务初始化的时候需要对监控上报进行设置。下面的例子中开启了监控数据向日志文件的打印123MeterCenter.INSTANCE .enableReporter(new EnablingLogReporter(&quot;org.apache.log4j.RollingFileAppender&quot;)) .init(); 查看统计结果统计结果会以熔断命令类名为进行分组。例如前面我们定义的 GetPlayCountCommand 类,package name 是 com.test，那么在日志中的输出将会是这个样子： 123type=GAUGE, name=com.test.GetPlayCountCommand.normal-rate, value=0.0type=GAUGE, name=com.test.GetPlayCountCommand.success-rate, value=61.0type=TIMER, name=com.test.GetPlayCountCommand.time, count=25866500, min=0.0, max=0.001, mean=3.963926781047921E-5, stddev=1.951102156677818E-4, median=0.0, p75=0.0, p95=0.0, p98=0.001, p99=0.001, p999=0.001, mean_rate=649806.0831335272, m1=1665370.7316699813, m5=2315813.300713087, m15=2446572.324069477, rate_unit=events/second, duration_unit=milliseconds 单独使用监控功能如果不想使用熔断功能，只是想监控Java方法调用的耗时和成功率，可以直接使用 OperationMeter 进行实现，只需要在函数调用的前后添加开始和结束的调用即可：123456789101112//创建一个操作的计数器 OperationMeter meter = MeterCenter.INSTANCE.getOrCreateMeter(OperationMeterTest.class, OperationMeter.class); //模拟成功率60% for(int k=0; k&lt;100; k++)&#123; Timer.Context context = meter.startOperation(); if(k%10&lt;6)&#123; meter.endOperation(context, OperationMeter.Result.SUCCESS); &#125; else &#123; meter.endOperation(context, OperationMeter.Result.FAILURE); &#125; &#125; 开启熔断并配置阀值和持续时间首先创建一个接口，继承自FusingConfig，用于指定配置文件的加载路径，同时还可以设定配置文件的刷新时间，具体定义方法请参照 ++owner++ 文档123456@Config.Sources(&quot;classpath:app_config.properties&quot;)@Config.HotReload( value = 1, unit = java.util.concurrent.TimeUnit.MINUTES, type = Config.HotReloadType.ASYNC)public interface APPFusingConfig extends FusingConfig &#123;&#125; 创建查询Command的时候在构造函数中传入1234567891011121314151617public class GetPlayCountCommand extends FusingCommand&lt;Long&gt; &#123; private final Long videoID; public GetPlayCountCommand(Long videoID) &#123; super( APPFusingConfig.class); //设定配置文件 this.videoID = videoID; &#125; protected Optional&lt;Long&gt; run() &#123; Long result = 0l; // 调用HTTP接口获取视频的播放次数信息 // 如果调用失败，返回 null 或者抛出异常，会将这次操作记录为失败 // 如果ID非法，返回 Optional.absent(),会将这次操作记录为成功 return Optional.fromNullable(result); &#125;&#125; 配置文件内容如下： 监控项 含义 默认值 fusing.[CommandClassName].mode 熔断模式：FORCE_NORMAL－关闭熔断功能;AUTO_FUSING－自动进入熔断模式;FORCE_NORMAL－强制进行熔断 FORCE_NORMAL fusing.[CommandClassName].duration 触发一次熔断以后持续的时间，支持ms,sec,min 单位。例如 10sec 50sec fusing.[CommandClassName].success_rate_threshold 触发熔断的成功率阀值，降低到这个成功率以下将触发熔断，例如0.9表示成功率90% 0.9 配置文件中的 CommandClassName 是每个操作类的名称，可以为每个操作单独设置上述参数。同时，这个配置文件支持动态加载，乐意通过修改fusing.[CommandClassName].mode 手工触发或者关闭熔断。 监控指标上报Graphite我们的服务中使用的是Metric+Graphite+Gafana进行监控数据的采集存储和展现，下面将介绍如何配置监控数据上报Grafana，关于Graphite+Grafana的配置，可以参考文章：使用graphite和grafana进行应用程序监控 定义配置文件首先定义一个接口，继承自GraphiteReporterConfig，通过这个接口定义配置文件的加载路径。配置文件路径的定义方法请参照 ++owner++ 文档, 下面是一个例子：123@Config.Sources(&quot;classpath:config.properties&quot;)public interface MyConfig extends GraphiteReporterConfig &#123;&#125; 配置文件中需要定义下列内容： 配置项 含义 meter.reporter.enabled.hosts 开启监控上报的服务器列表 meter.reporter.perfix 上报使用的前缀 meter.reporter.carbon.host grafana(carbon-cache) 的 IP 地址，用于存储监控数据 meter.reporter.carbon.port grafana(carbon-cache) 的端口 下面这个例子是在192.168.0.0.1和192.168.0.0.2两台服务器上开启监控数据上报，上报监控指标的前缀是12345project_name.dc：meter.reporter.enabled.hosts = 192.168.0.0.1,192.168.0.0.2meter.reporter.perfix = project_name.dcmeter.reporter.carbon.host = hostname.graphite 由于相同机房的不同服务器对外部接口的访问情况一般比较类似，所以仅选取部分机器上报，也是为了节省资源。仅选择部分机器上报不影响熔断效果。 初始化配置上报在服务初始化的时候需要对监控上报进行设置。下面的例子中开启了监控数据向日志文件的打印，同时通过MyConfig指定的配置文件加载Graphite配置信息。1234MeterCenter.INSTANCE .enableReporter(new EnablingLogReporter(&quot;org.apache.log4j.RollingFileAppender&quot;)) .enableReporter(new EnablingGraphiteReporter(MyConfig.class)) //监控数据上报Grafana .init(); 查看统计结果统计结果会以熔断命令类名为进行分组。例如前面我们定义的 GetPlayCountCommand 类,package name 是 com.test，那么在日志中的输出将会是这个样子：123type=GAUGE, name=com.test.GetPlayCountCommand.normal-rate, value=0.0type=GAUGE, name=com.test.GetPlayCountCommand.success-rate, value=61.0type=TIMER, name=com.test.GetPlayCountCommand.time, count=25866500, min=0.0, max=0.001, mean=3.963926781047921E-5, stddev=1.951102156677818E-4, median=0.0, p75=0.0, p95=0.0, p98=0.001, p99=0.001, p999=0.001, mean_rate=649806.0831335272, m1=1665370.7316699813, m5=2315813.300713087, m15=2446572.324069477, rate_unit=events/second, duration_unit=milliseconds 监控项 含义 [classname].success-rate 成功率 [classname].time.m1 QPS [classname].time.mean 平均响应时间 [classname].normal-rate 过去1分钟内处于正常访问（非熔断）的时间比例 在Grafanna中可以看到下面的监控图： 自定义监控上报meerkat使用++Metrics++进行监控数据的统计，因此可以使用Metrics支持的所有reporter进行上报。添加一种上报的时候，只需要实现 EnablingReporter 并在 MeterCenter 初始化之前进行调用即可。下面是log reporter的实现，可以作为参考12345678910111213141516public class EnablingLogReporter implements EnablingReporter &#123; private String loggername; public EnablingLogReporter(String loggername) &#123; this.loggername = loggername; &#125; @Override public void invoke(MetricRegistry metricRegistry, long period, TimeUnit timeUnit) &#123; Slf4jReporter.forRegistry(metricRegistry) .outputTo(LoggerFactory.getLogger(loggername)) .convertRatesTo(java.util.concurrent.TimeUnit.SECONDS) .convertDurationsTo(java.util.concurrent.TimeUnit.MILLISECONDS) .build().start(period, timeUnit); &#125;&#125; MeterCenter 初始化的时候开启reporter123MeterCenter.INSTANCE .enableReporter(new EnablingLogReporter(&quot;org.apache.log4j.RollingFileAppender&quot;)) .init(); 多实例监控多实例监控主要是为了解决一个被监控操作的实现类需要根据输入参数的不同分别进行监控和熔断的情况，通过定义实例的名称进行实现。例如获取视频播放次数的例子，获取视频播放次数的接口对于不同的视频类型而言请求逻辑是一样的，所以使用同一个类进行实现；但是对于不同的视频类型，接口实现的复杂程度不同导致成功率不同，当用户上传的视频的播次接口大量失败的时候我们不希望同时熔断电影电视剧这类视频的播放次数获取，这时就需要使用多实例这种特性进行监控和熔断。 下面是一个单实例的实现：1234567891011121314151617public class GetPlayCountCommand extends FusingCommand&lt;Long&gt; &#123; private final Long videoID; public GetPlayCountCommand(Long videoID) &#123; super( APPFusingConfig.class); this.videoID = videoID; &#125; protected Optional&lt;Long&gt; run() &#123; Long result = 0l; // 调用HTTP接口获取视频的播放次数信息 // 如果调用失败，返回 null 或者抛出异常，会将这次操作记录为失败 // 如果ID非法，返回 Optional.absent(),会将这次操作记录为成功 return Optional.fromNullable(result); &#125;&#125; 假设业务上我们可以根据视频ID判断视频类型，可以在类初始化的时候根据类型创建多种监控实例，添加了多实例支持的实现如下：1234567891011121314151617181920public class GetPlayCountCommand extends FusingCommand&lt;Long&gt; &#123; private final Long videoID; public GetPlayCountCommand(Long videoID) &#123; super( getVideoType(videoID), APPFusingConfig.class); this.videoID = videoID; &#125; private static String getVideoType(Long videoID)&#123; return &quot;PGC&quot;; //根据videoID进行判断，返回 &quot;PGC&quot; 或者 &quot;UGC&quot; 这两个类别 &#125; protected Optional&lt;Long&gt; run() &#123; Long result = 0l; // 调用HTTP接口获取视频的播放次数信息 // 如果调用失败，返回 null 或者抛出异常，会将这次操作记录为失败 // 如果ID非法，返回 Optional.absent(),会将这次操作记录为成功 return Optional.fromNullable(result); &#125; 由于每个实例独享一个监控指标，日志中的监控个结果是这个样子： 123456type=GAUGE, name=com.test.GetPlayCountCommand.PGC.normal-rate, value=100.0type=GAUGE, name=com.test.GetPlayCountCommand.PGC.success-rate, value=100.0type=GAUGE, name=com.test.GetPlayCountCommand.UGC.normal-rate, value=100.0type=GAUGE, name=com.test.GetPlayCountCommand.UGC.success-rate, value=60.0type=TIMER, name=com.test.GetPlayCountCommand.PGC.time, count=100, min=0.0, max=0.509, mean=0.00635, stddev=0.05052135687013958, median=0.001, p75=0.002, p95=0.002, p98=0.003, p99=0.003, p999=0.509, mean_rate=1.6680162586215173, m1=8.691964170141569, m5=16.929634497812284, m15=18.919189378135307, rate_unit=events/second, duration_unit=millisecondstype=TIMER, name=com.test.GetPlayCountCommand.UGC.time, count=100, min=0.0, max=0.027, mean=0.00132, stddev=0.0026939933184772376, median=0.001, p75=0.001, p95=0.002, p98=0.005, p99=0.006, p999=0.027, mean_rate=1.6715904477699361, m1=8.691964170141569, m5=16.929634497812284, m15=18.919189378135307, rate_unit=events/second, duration_unit=milliseconds 相应的，对熔断阀值以及持续时间的配置也需要明确指出实例的名字： 1234567fusing.GetPlayCountCommand.UGC.mode = AUTO_FUSINGfusing.GetPlayCountCommand.UGC.duration = 50secfusing.GetPlayCountCommand.UGC.success_rate_threshold = 0.9fusing.GetPlayCountCommand.PGC.mode = AUTO_FUSINGfusing.GetPlayCountCommand.PGC.duration = 50secfusing.GetPlayCountCommand.PGC.success_rate_threshold = 0.9 参考资料https://segmentfault.com/a/1190000009730789 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>熔断</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell 输出重定向（> >> 2>&1）]]></title>
    <url>%2F2018%2F01%2F10%2Fshell%20%E8%BE%93%E5%87%BA%E9%87%8D%E5%AE%9A%E5%90%91%EF%BC%88%3E%20%3E%3E%202%3E%261%EF%BC%89%2F</url>
    <content type="text"><![CDATA[shell - 输出重定向（&gt; &gt;&gt; 2&gt;&amp;1） 一、bash中0，1，2bash中0，1，2三个数字分别代表STDIN_FILENO、STDOUT_FILENO、STDERR_FILENO，即标准输入（一般是键盘），标准输出（一般是显示屏，准确的说是用户终端控制台），标准错误（出错信息输出） 二、输入输出重定向所谓重定向输入就是在命令中指定具体的输入来源，譬如 cat &lt; test.c 将test.c重定向为cat命令的输入源。输出重定向是指定具体的输出目标以替换默认的标准输出，譬如ls &gt; 1.txt将ls的结果从标准输出重定向为1.txt文本。有时候会看到如 ls &gt;&gt; 1.txt这类的写法，&gt; 和 &gt;&gt; 的区别在于：&gt; 用于新建而&gt;&gt;用于追加。即ls &gt; 1.txt会新建一个1.txt文件并且将ls的内容输出到新建的1.txt中，而ls &gt;&gt; 1.txt则用在1.txt已经存在，而我们只是想将ls的内容追加到1.txt文本中的时候。 默认输入只有一个（0，STDIN_FILENO），而默认输出有两个（标准输出1 STDOUT_FILENO，标准错误2 STDERR_FILENO）。因此默认情况下，shell输出的错误信息会被输出到2，而普通输出信息会输出到1。但是某些情况下，我们希望在一个终端下看到所有的信息（包括标准输出信息和错误信息），要怎么办呢？对了，你可以使用我们上面讲到的输出重定向。思路有了，怎么写呢？ 非常直观的想法就是2&gt;1（将2重定向到1嘛），行不行呢？试一试就知道了。我们进行以下测试步骤： 123451）mkdir test &amp;&amp; cd test; 创建test文件夹并进入test目录2）touch a.txt b.c c; 创建a.txt b.c c 三个文件3）ls &gt; 1; 按我们的猜测，这句应该是将ls的结果重定向到标准输出，因此效果和直接ls应该一样。但是实际这句执行后，标准输出中并没有任何信息。4）ls; 执行3之后再次ls，则会看到test文件夹中多了一个文件15）cat 1 ; 查看文件1的内容，实际结果为：1 a.txt b.c c 可见步骤3中 ls &gt; 1并不是将ls的结果重定向为标准输出，而是将结果重定向到了一个文件1中。即1在此处不被解释为STDOUT_FILENO，而是文件1。 4、到了此时，你应该也能猜到2&gt;&amp;1的用意了。不错，2&gt;&amp;1就是用来将标准错误2重定向到标准输出1中的。此处1前面的&amp;就是为了让bash将1解释成标准输出而不是文件1。至于最后一个&amp;，则是让bash在后台执行 三、例子🌰1.输出重定向 1.1把标准输出重定向到文件 1234567[~]# echo &quot;hello&quot; &gt; test.sh[~]# cat test.shhello&apos;&gt;&apos;输出方式默认等价&apos;1&gt;&apos;[~]# echo &quot;hello&quot; 1&gt; test.sh[~]# cat test.shhello 输入、输出及标准错误输出主要用于 I/O 的重定向，就是说需要改变他们的默认设置。先看这个例子：1$ ls &gt; ls_result $ ls -l &gt;&gt; ls_result 上面这两个命令分别将 ls 命令的结果输出重定向到 ls_result 文件中和追加到 ls_result 文件中，而不是输出到屏幕上。”&gt;”就是输出（标准输出和标准错误输出）重定向的代表符号，连续两个 “&gt;” 符号，即 “&gt;&gt;” 则表示不清除原来的而追加输出。下面再来看一个稍微复杂的例子：1$ find /home -name lost* 2&gt; err_result 这个命令在 “&gt;” 符号之前多了一个 “2”，”2&gt;” 表示将标准错误输出重定向。由于 /home 目录下有些目录由于权限限制不能访问，因此会产生一些标准错误输出被存放在 err_result 文件中。 大家可以设想一下 find /home -name lost 2&gt;&gt;err_result 命令会产生什么结果？如果直接执行 find /home -name lost &gt; all_result ，其结果是只有标准输出被存入 all_result 文件中，要想让标准错误输出和标准输入一样都被存入到文件中，那该怎么办呢？看下面这个例子：1$ find /home -name lost* &gt; all_result 2&gt;&amp; 1 上面这个例子中将首先将标准错误输出也重定向到标准输出中，再将标准输出重定向到 all_result 这个文件中。这样我们就可以将所有的输出都存储到文件中了。为实现上述功能，还有一种简便的写法如下：1$ find /home -name lost* &gt;&amp; all_result 如果那些出错信息并不重要，下面这个命令可以让你避开众多无用出错信息的干扰：1$ find /home -name lost* 2&gt; /dev/null 同学们回去后还可以再试验一下如下几种重定向方式，看看会出什么结果，为什么？1$ find /home -name lost* &gt; all_result 1&gt;&amp; 2 $ find /home -name lost* 2&gt; all_result 1&gt;&amp; 2 $ find /home -name lost* 2&gt;&amp; 1 &gt; all_result 另外一个非常有用的重定向操作符是 “-“，请看下面这个例子1$ (cd /source/directory &amp;&amp; tar cf - . ) | (cd /dest/directory &amp;&amp; tar xvfp -) 该命令表示把 /source/directory 目录下的所有文件通过压缩和解压，快速的全部移动到 /dest/directory 目录下去，这个命令在 /source/directory 和 /dest/directory 不处在同一个文件系统下时将显示出特别的优势。下面还几种不常见的用法：n&lt;&amp;- 表示将 n 号输入关闭 &lt;&amp;- 表示关闭标准输入（键盘） n&gt;&amp;- 表示将 n 号输出关闭 &gt;&amp;- 表示将标准输出关闭 参考资料https://qinqianshan.com/shell-output-redirection-21/ https://blog.csdn.net/feng27156/article/details/38980543 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell中条件判断if中的-a到-z的意思]]></title>
    <url>%2F2018%2F01%2F10%2Fshell%E4%B8%AD%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%ADif%E4%B8%AD%E7%9A%84-a%E5%88%B0-z%E7%9A%84%E6%84%8F%E6%80%9D%2F</url>
    <content type="text"><![CDATA[shell中条件判断if中的-a到-z的意思 [-a file] 如果file存在则为真 [-b file] 如果file存在且是一个块特殊文件则为真 [-c file] 如果file存在且是一个字特殊文件则为真 [-d file] 如果file文件存在且是一个目录则为真-d前的!是逻辑非例如：1if [ ! -d $lcd_path/$par_date ] 表示后面的那个目录不存在，则执行后面的then操作 [-e file] 如果file文件存在则为真 [-f file] 如果file存在且是一个普通文件则为真 [-g file] 如果file存在且已经设置了SGID则为真（SUID 是 Set User ID, SGID 是 Set Group ID的意思） [-h file] 如果file存在且是一个符号连接则为真 [-k file] 如果file存在且已经设置粘制位则为真当一个目录被设置为”粘制位”(用chmod a+t),则该目录下的文件只能由一、超级管理员删除二、该目录的所有者删除三、该文件的所有者删除也就是说,即便该目录是任何人都可以写,但也只有文件的属主才可以删除文件。具体例子如下：123#ls -dl /tmp drwxrwxrwt 4 root root ......... 注意other位置的t，这便是粘连位。 [-p file] 如果file存在且是一个名字管道（F如果O）则为真管道是linux里面进程间通信的一种方式，其他的还有像信号（signal）、信号量、消息队列、共享内存、套接字（socket）等。 [-r file] 如果file存在且是可读的则为真 [-s file] 如果file存在且大小不为0则为真 [-t FD] 如果文件描述符FD打开且指向一个终端则为真 [-u file] 如果file存在且设置了SUID（set userID）则为真 [-w file] 如果file存在且是可写的则为真 [-x file] 如果file存在且是可执行的则为真 [-O file] 如果file存在且属有效用户ID则为真 [-G file] 如果file存在且属有效用户组则为真 [-L file] 如果file存在且是一个符号连接则为真 [-N file] 如果file存在and has been mod如果ied since it was last read则为真 [-S file] 如果file存在且是一个套接字则为真 [file1 –nt file2] 如果file1 has been changed more recently than file2或者file1 exists and file2 does not则为真 [file1 –ot file2] 如果file1比file2要老，或者file2存在且file1不存在则为真 [file1 –ef file2] 如果file1和file2指向相同的设备和节点号则为真 [-o optionname] 如果shell选项“optionname”开启则为真 [-z string] “string”的长度为零则为真 [-n string] or [string] “string”的长度为非零non-zero则为真 [sting1==string2] 如果2个字符串相同。“=”may be used instead of “==”for strict posix compliance则为真 [string1!=string2] 如果字符串不相等则为真[string1&lt;string2] 如果“string1”sorts before“string2”lexicographically in the current locale则为真 [arg1 OP arg2] “OP”is one of –eq,-ne,-lt,-le,-gt or –ge.These arithmetic binary oprators return true if “arg1”is equal to,not equal to,less than,less than or equal to,greater than,or greater than or equal to“agr2”,respectively.“arg1”and “agr2”are integers. 参考资料https://blog.csdn.net/vergilgeekopen/article/details/69493321 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux中date命令的各种实用方法]]></title>
    <url>%2F2018%2F01%2F10%2FLinux%E4%B8%ADdate%E5%91%BD%E4%BB%A4%E7%9A%84%E5%90%84%E7%A7%8D%E5%AE%9E%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Linux中date命令的各种实用方法 用法：date [选项]… [+格式] 或：date [-u|–utc|–universal] [MMDDhhmm[[CC]YY][.ss]]以给定的格式显示当前时间，或是设置系统日期。 -d,–date=字符串 显示指定字符串所描述的时间，而非当前时间 -f,–file=日期文件 类似–date，从日期文件中按行读入时间描述 -r, –reference=文件 显示文件指定文件的最后修改时间 -s, –set=字符串 设置指定字符串来分开时间 -u, –utc, –universal 输出或者设置协调的通用时间 –help 显示此帮助信息并退出 –version 显示版本信息并退出 读者可以设定特定的格式，格式设定规则：一个加号后接数个标记，每个标记中都有%，其中可用的标记列表和说明如下: %n : 下一行 %t : 跳格 %H : 小时(00..23) %I : 小时(01..12) %k : 小时(0..23) %l : 小时(1..12) %M : 分钟(00..59) %p : 显示本地 AM 或 PM %r : 直接显示时间 (12 小时制，格式为 hh:mm:ss [AP]M) %s : 从 1970 年 1 月 1 日 00:00:00 UTC 到目前为止的秒数%S : 秒(00..59) %T : 直接显示时间 (24 小时制) %X : 相当于 %H:%M:%S %Z : 显示时区 %a : 星期几 (Sun..Sat) %A : 星期几(Sunday..Saturday) %b : 月份 (Jan..Dec) %B : 月份 (January..December) %c : 直接显示日期与时间 %d : 日 (01..31) %D : 直接显示日期 (mm/dd/yy) %h : 同 %b %j : 一年中的第几天 (001..366) %m : 月份 (01..12) %U : 一年中的第几周 (00..53) (以 Sunday 为一周的第一天的情形) %w : 一周中的第几天 (0..6) %W : 一年中的第几周 (00..53) (以 Monday 为一周的第一天的情形) %x : 直接显示日期 (mm/dd/yy) %y : 年份的最后两位数字 (00.99) %Y : 完整年份 (0000..9999) ##1.按照特定的格式输出时间：短接符”-“、空格” “和冒号”:” 为分隔符，其中空格前面加了转义符号”\” 12[root@RHEL601 tmp]# date +%Y-%m-%d\ %H:%M:%S2012-07-19 21:10:28 在当前时间的基础上往前推或者往后推三天1234[root@RHEL601 tmp]# date -d &quot;+3 day&quot; 2012年 07月 22日 星期日 20:12:08 CST[root@RHEL601 tmp]# date -d &quot;-3 day&quot; 2012年 07月 16日 星期一 20:12:12 CST 在当前时间的基础上往前推或者往后推三个月1234[root@RHEL601 tmp]# date -d &quot;-3 month&quot; 2012年 04月 19日 星期四 20:12:39 CST[root@RHEL601 tmp]# date -d &quot;+3 month&quot; 2012年 10月 19日 星期五 20:12:48 CST 在当前时间的基础上往前推或者往后推三年1234[root@RHEL601 tmp]# date -d &quot;+3 year&quot; 2015年 07月 19日 星期日 20:13:06 CST[root@RHEL601 tmp]# date -d &quot;-3 year&quot; 2009年 07月 19日 星期日 20:13:11 CST 在当前时间的基础上往前推或者往后推三小时1234[root@RHEL601 tmp]# date -d &quot;-3 hour&quot; 2012年 07月 19日 星期四 17:13:20 CST[root@RHEL601 tmp]# date -d &quot;+3 hour&quot; 2012年 07月 19日 星期四 23:13:24 CST 在当前时间的基础上往前推或者往后推三分钟1234[root@RHEL601 tmp]# date -d &quot;+3 minute&quot; 2012年 07月 19日 星期四 20:16:56 CST[root@RHEL601 tmp]# date -d &quot;-3 minute&quot; 2012年 07月 19日 星期四 20:10:59 CST 在当前时间的基础上往前推或者往后推三十秒123456[root@RHEL601 tmp]# date &amp;&amp; date -d &quot;-30 second&quot; 2012年 07月 19日 星期四 20:14:24 CST2012年 07月 19日 星期四 20:13:54 CST[root@RHEL601 tmp]# date &amp;&amp; date -d &quot;+30 second&quot; 2012年 07月 19日 星期四 20:14:29 CST2012年 07月 19日 星期四 20:14:59 CST ##2、接下来的范例说明如何用date来表示各种各样的时间，表示的都是某一天的零点时间，也可以在当前的时分秒的基础上表示时间，(特别注意中间用到了反单引号`)参见范例12345678910111213141516171819202122232425262728293031323334353637383940date -d `date +%y%m01` #本月第一天[root@RHEL601 tmp]# date -d `date +%y%m01`2012年 07月 01日 星期日 00:00:00 CSTdate -d `date +%y%m01`&quot;-1 day&quot; #上个月最后一天[root@RHEL601 tmp]# date -d `date +%y%m01`&quot;-1 day&quot;2012年 06月 30日 星期六 00:00:00 CSTdate -d `date -d &quot;-3 month&quot; +%y%m01`&quot;-1 day&quot; #4个月前的第一天[root@RHEL601 tmp]# date -d `date -d &quot;-3 month&quot; +%y%m01`&quot;-1 day&quot;2012年 03月 31日 星期六 00:00:00 CSTdate -d `date -d &quot;+12 month&quot; +%y%m01`&quot;-1 day&quot; #第11个月后的第一天[root@RHEL601 tmp]# date -d `date -d &quot;+12 month&quot; +%y%m01`&quot;-1 day&quot;2013年 06月 30日 星期日 00:00:00 CSTdate -d `date -d &quot;-1 month&quot; +%y%m01` #上个月第一天[root@RHEL601 tmp]# date -d `date -d &quot;-1 month&quot; +%y%m01`2012年 06月 01日 星期五 00:00:00 CSTdate -d `date -d &quot;+12 month&quot; +%y%m01` #第12个月后的第一天[root@RHEL601 tmp]# date -d `date -d &quot;+12 month&quot; +%y%m01`2013年 07月 01日 星期一 00:00:00 CSTdate -d `date -d &quot;-1 day&quot; +%Y%m%d` #前一天零点时间[root@RHEL601 tmp]# date -d `date -d &quot;-1 day&quot; +%Y%m%d`2012年 07月 18日 星期三 00:00:00 CSTdate -d `date -d &quot;-3 day&quot; +%Y%m%d` #前三天零点时间[root@RHEL601 tmp]# date -d `date -d &quot;-3 day&quot; +%Y%m%d` 2012年 07月 16日 星期一 00:00:00 CSTdate -d `date -d &quot;+1 day&quot; +%Y%m%d` #明天零点时间[root@RHEL601 tmp]# date -d `date -d &quot;+1 day&quot; +%Y%m%d` 2012年 07月 20日 星期五 00:00:00 CSTdate -d `date -d &quot;+3 day&quot; +%Y%m%d` #往后推三天零点时间[root@RHEL601 tmp]# date -d `date -d &quot;+3 day&quot; +%Y%m%d` 2012年 07月 22日 星期日 00:00:00 CST ##3、以下简单示范了字母大小写在date命令中的区别Y #代表完整的年份，例如:2012年 将显示 2012y #代表缩写年份，例如：2012年 缩写为 121234[root@RHEL601 tmp]# date +&quot;%y%m01%H%M%S&quot;121201121556[root@RHEL601 tmp]# date +&quot;%Y%m01%H%M%S&quot;20121201121610 ##4、以下范例说明如何调整服务器的时间1ntpdate 210.72.145.44 将服务器时间调整为正常时间，210.72.145.44 是国家授时中心服务器IP地址12[root@RHEL601 tmp]# ntpdate 210.72.145.4419 Jul 13:07:07 ntpdate[15150]: adjust time server 210.72.145.44 offset -0.020920 sec date 121212122012 将时间设置为2012年 12月 12日 星期三 12:12:00，date后面的数字代表月日时分年，还可以加秒，需要后面跟英文状态下的句号字符”.”，例如：121212122012.1212345678910111213141516171819[root@RHEL601 tmp]# date 1212121220122012年 12月 12日 星期三 12:12:00 CST[root@RHEL601 tmp]# date 121212122012.122012年 12月 12日 星期三 12:12:12 CST[root@RHEL601 tmp]# date `date -d &quot;1 day ago&quot; +%m%d%H%M%Y.%S`2012年 07月 18日 星期三 20:13:04 CST[root@RHEL601 tmp]# date2012年 07月 18日 星期三 20:13:10 CST[root@RHEL601 tmp]# date `date -d &quot;3 days ago&quot; +%m%d%H%M%Y.%S`2012年 07月 15日 星期日 20:13:18 CST[root@RHEL601 tmp]# date `date -d &quot;5 days ago&quot; +%m%d%H%M%Y.%S`2012年 07月 10日 星期二 20:13:28 CST[root@RHEL601 tmp]# date `date -d &quot;$((3600*24)) seconds ago&quot; +%m%d%H%M%Y.%S`2012年 07月 09日 星期一 20:13:39 CST[root@RHEL601 tmp]# date `date -d &quot;$((3600*24)) seconds ago&quot; +%m%d%H%M%Y.%S`2012年 07月 08日 星期日 20:14:01 CST[root@RHEL601 tmp]# ntpdate 210.72.145.4419 Jul 20:14:15 ntpdate[26846]: step time server 210.72.145.44 offset 950404.037565 sec 参考资料https://blog.chinaunix.net/uid-22823163-id-3293784.html 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sublime注册码破解及关闭自动更新]]></title>
    <url>%2F2017%2F09%2F30%2Fsublime%E7%A0%B4%E8%A7%A3%E5%8F%8A%E5%85%B3%E9%97%AD%E8%87%AA%E5%8A%A8%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[sublime - 丰富的文本编辑器，使用技巧 破解方法话不多说，先分享三个注册码！点击最右侧 help add license，添加注册码即可123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354—– BEGIN LICENSE —–Michael BarnesSingle User LicenseEA7E-8213858A353C41 872A0D5C DF9B2950 AFF6F667C458EA6D 8EA3C286 98D1D650 131A97ABAA919AEC EF20E143 B361B1E7 4C8B7F04B085E65E 2F5F5360 8489D422 FB8FC1AA93F6323C FD7F7544 3F39C318 D95E6480FCCC7561 8A4A1741 68FA4223 ADCEDE07200C25BE DBBC4855 C4CFB774 C5EC138C0FEC1CEF D9DCECEC D3A5DAD1 01316C36—— END LICENSE ———– BEGIN LICENSE —–Nicolas HennionSingle User LicenseEA7E-8660758A01AA83 1D668D24 4484AEBC 3B04512C827B0DE5 69E9B07A A39ACCC0 F95F5410729D5639 4C37CECB B2522FB3 8D37FDC172899363 BBA441AC A5F47F08 6CD3B3FECEFB3783 B2E1BA96 71AAF7B4 AFB61B1D0CC513E7 52FF2333 9F726D2C CDE53B4A810C0D4F E1F419A3 CDA0832B 8440565A35BF00F6 4CA9F869 ED10E245 469C233E—— END LICENSE ———– BEGIN LICENSE —–Anthony SansoneSingle User LicenseEA7E-87856328B9A648 42B99D8A F2E3E9E0 16DE076EE218B3DC F3606379 C33C1526 E8B58964B2CB3F63 BDF901BE D31424D2 082891B5F7058694 55FA46D8 EFC11878 0868F093B17CAFE7 63A78881 86B78E38 0F146238BAE22DBB D4EC71A1 0EC2E701 C7F9C6485CF29CA3 1CB14285 19A46991 E9A9867614FD4777 2D8A0AB6 A444EE0D CA009B54—— END LICENSE ———– BEGIN LICENSE —–Alexey PlutalovSingle User LicenseEA7E-8607763DC19CC1 134CDF23 504DC871 2DE5CE55585DC8A6 253BB0D9 637C87A2 D8D0BA85AAE574AD BA7D6DA9 2B9773F2 324C5DEF17830A4E FBCF9D1D 182406E9 F883EA87E585BBA1 2538C270 E2E857C2 194283CA7234FF9E D0392F93 1D16E021 F191491763909E12 203C0169 3F08FFC8 86D06EA873DDAEF0 AC559F30 A6A67947 B60104C6—— END LICENSE —— 关闭自动更新如图打开,添加如下代码12345// Settings in here override those in &quot;Default/Preferences.sublime-settings&quot;,// and are overridden in turn by file type specific settings.&#123; &quot;update_check&quot; : false&#125; 注意：博主添加代码时发现自动更新提醒还是会弹出来，后来发现是没有破解，破解之后就不会弹出来了 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>软件破解</category>
      </categories>
      <tags>
        <tag>sublime</tag>
        <tag>破解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[零基础搭建Hexo炫酷静态页面博客]]></title>
    <url>%2F2017%2F09%2F29%2F%E9%9B%B6%E5%9F%BA%E7%A1%80%E6%90%AD%E5%BB%BAHexo%E7%82%AB%E9%85%B7%E9%9D%99%E6%80%81%E9%A1%B5%E9%9D%A2%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[Hexo - 快速、简洁且高效的博客框架 GitHubPages + hexo 简介最近花了两天的时间搭建了一个博客，使用GitHubPages + hexo 为什么选用GitHubPages + hexo 优点如下： 1.超快速度Node.js 所带来的超快生成速度，让上百个页面在几秒内瞬间完成渲染。 2.支持 MarkdownHexo 支持 GitHub Flavored Markdown 的所有功能，甚至可以整合 Octopress 的大多数插件。 3.一键部署只需一条指令即可部署到 GitHub Pages, Heroku 或其他网站。 4.丰富的插件Hexo 拥有强大的插件系统，安装插件可以让 Hexo 支持 Jade, CoffeeScript。 博客地址 前期准备github账号node.jsnpmhexo注意：（node.js 集成带有npm，因此只要下载 node.js 就可以了） github上创建GitHubPages仓库GitHubPages官方参考地址 注意： 创建仓库的时候仓库名一定严格按照 git用户名.github.io 来命名创建仓库完成之后，在本地创建一个站点文件夹 git用户名.github.io/blog 下载node.js我是在mac系统上搭建的，下载用的终端brew命令，如果其他系统或者没有翻墙，可能会有问题，如果下载失败可以移步Node.js官网，下载最新版本一路安装即可。检测安装是否成功 终端输入 node -v ,npm -v 成功则显示版本号123➜ blog git:(master) ✗ npm -v5.3.0➜ blog git:(master) ✗ 下载 hexohexo官方 有详细的windows和mac用户的安装文档，如果因为防火墙等原因安装失败，请使用下面命令安装，sudo赋予命令最高权限，避免权限不足 1➜ blog git:(master) sudo npm install hexo --no-optional 下载git文档说明，上面有各个平台的git下载安装步骤，按照步骤安装即可 本地关联github仓库git下载安装完成之后，需要跟你的github仓库关联起来，你需要一个私钥和公钥，首先查看本地有没有1234➜ ~ git:(master) ✗ cd ~/.ssh ➜ .ssh git:(master) ✗ lsconfig id_rsa id_rsa.pub known_hosts➜ .ssh git:(master) ✗ 如果没有id_rsa（私钥）和id_rsa.pub（公钥）就需要手动生成一个，执行命令1➜ ~ git:(master) ✗ ssh-keygen 生成双钥，然后把公钥放到github仓库上，点击头像选择settingsd左侧找到SSH，然后点击New SSH key，把公钥的文本内容粘贴进去，就可以了 部署hexo命令介绍准备工作完成，就可以部署看一下博客界面效果了,先介绍一下hexo常用命令 hexo clean 清除编辑后生成的静态文件，一般部署前都会执行一遍，防止上次部署结果影响新的文件内容 hexo generate 编辑生成静态页面等 hexo server 启动服务 hexo develop 部署到远程github，需要修改配置文件，稍后介绍 基本上用到的就是这些命令，为了方便可以设置快捷指令，执行如下命令，会在跟目录生成一个.bash_profile文件，同时打开.bash_profile进入编辑模式12➜ ~ git:(master) ✗ cd ~➜ ~ git:(master) ✗ vi .bash_profile 然后输入以下内容,进行保存，linux命令123456789101112131415161718192021222324alias hexo clean=hexo calias hexo generate=hexo galias hexo server=hexo salias hexo develop=hexo d~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ -- INSERT -- 执行如下命令，让文件生效1➜ ~ git:(master) ✗ source .bash_profile 至此，快捷指令设置完成 本地部署然后cd进入本地博客根目录，执行命令12345➜ ~ git:(master) ✗ hexo c➜ ~ git:(master) ✗ hexo g➜ ~ git:(master) ✗ hexo sINFO Start processingINFO Hexo is running at https://localhost:4000/. Press Ctrl+C to stop. 看到如下信息就可以在本地访问https://localhost:4000/ 查看博客主页了 远程部署部署到元辰github上需要修改一下本地博客配置文件，找到根目录的_config.yml文件，修改以下内容123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:你的github名/你的github名.github.io.git branch: master 然后就可以执行如下命令部署到github1➜ ~ git:(master) ✗ hexo d 看到如下信息说明部署完成，就可以通过https://你的github名.github.io/ 访问你的主页了1INFO Deploy done: git 部署完成至此，一个博客模板搭建完成，hexo还支持丰富的插件，包括RSS订阅，评论系统的接入，文章阅读量，打赏功能，第三方链接，如微博，发邮件等。 接入以上功能，让自己的博客更加炫酷！ 有问题可以联系博主。转载请注明出处，谢谢！]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
</search>
