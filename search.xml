<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[判断一个单链表是否有环]]></title>
    <url>%2F2018%2F03%2F26%2F%E5%88%A4%E6%96%AD%E4%B8%80%E4%B8%AA%E5%8D%95%E9%93%BE%E8%A1%A8%E6%98%AF%E5%90%A6%E6%9C%89%E7%8E%AF%2F</url>
    <content type="text"><![CDATA[三类情况：（1）（2）（3）1、遇到这个问题，首先想到的是遍历链表，寻找是否有相同地址，借此判断链表中是否有环。123456789101112131415listnode_ptr current =head-&gt;next;while(current)&#123; if(current==head) &#123; printf(&quot;有环！\n&quot;); return 0; &#125; else &#123; current=current-&gt;next; &#125;&#125;printf(&quot;无环！\n&quot;);return 0; 这段代码满足了（1）（链表无环）、（2）（链表头尾相连)两类情况，却没有将（3）情况考虑在内，如果出现（3）类情况，程序会进入死循环。 2、将（3）考虑在内，首先想到我们可能需要一块空间来存储指针，遍历新指针时将其和储存的旧指针比对，若有相同指针，则该链表有环，否则将这个新指针存下来后继续往下读取，直到遇见NULL，这说明这个链表无环。 上述方法虽然可行，可是否还有更简便的算法？ 3、假设有两个学生A和B在跑道上跑步，两人从相同起点出发，假设A的速度为2m/s，B的速度为1m/s,结果会发生什么？ 答案很简单，A绕了跑道一圈之后会追上B！ 将这个问题延伸到链表中，跑道就是链表，我们可以设置两个指针，a跑的快，b跑的慢，如果链表有环，那么当程序执行到某一状态时，a==b。如果链表没有环，程序会执行到a==NULL，结束。 1234567891011121314151617181920212223242526listnode_ptr fast=head-&gt;next; listnode_ptr slow=head;while(fast)&#123; if(fast==slow) &#123; printf(&quot;环！\n&quot;); return 0; &#125; else &#123; fast=fast-&gt;next; if(!fast) &#123; printf(&quot;无环！\n&quot;); return 0; &#125; else &#123; fast=fast-&gt;next; slow=slow-&gt;next; &#125; &#125;&#125;printf(&quot;无环！\n&quot;); return 0; 4、关于算法复杂度： 如图，链表长度为n，环节点个数为m，则循环 t=n-m 次时，slow进入环中，此时，我们假设fast与slow相距x个节点，那么，经过t’次循环，二者相遇时，有：2t’=t’+（m-x） -&gt; t’=m-x -&gt; t’&lt;=m.因此，总共循环了T=t+t’ &lt;= n. 算法复杂度为O(n). 有问题可以联系博主 转载请注明出处，谢谢！]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 和 Memcached比较]]></title>
    <url>%2F2018%2F03%2F26%2FRedis%E5%92%8CMemcached%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[1. MySql+Memcached架构的问题 实际MySQL是适合进行海量数据存储的，通过Memcached将热点数据加载到cache，加速访问，很多公司都曾经使用过这样的架构，但随着业务数据量的不断增加，和访问量的持续增长，我们遇到了很多问题： 1.MySQL需要不断进行拆库拆表，Memcached也需不断跟着扩容，扩容和维护工作占据大量开发时间。 2.Memcached与MySQL数据库数据一致性问题。 3.Memcached数据命中率低或down机，大量访问直接穿透到DB，MySQL无法支撑。 4.跨机房cache同步问题。 众多NoSQL百花齐放，如何选择 最近几年，业界不断涌现出很多各种各样的NoSQL产品，那么如何才能正确地使用好这些产品，最大化地发挥其长处，是我们需要深入研究和思考的问题，实际归根结底最重要的是了解这些产品的定位，并且了解到每款产品的tradeoffs，在实际应用中做到扬长避短，总体上这些NoSQL主要用于解决以下几种问题 1.少量数据存储，高速读写访问。此类产品通过数据全部in-momery 的方式来保证高速访问，同时提供数据落地的功能，实际这正是Redis最主要的适用场景。 2.海量数据存储，分布式系统支持，数据一致性保证，方便的集群节点添加/删除。 3.这方面最具代表性的是dynamo和bigtable 2篇论文所阐述的思路。前者是一个完全无中心的设计，节点之间通过gossip方式传递集群信息，数据保证最终一致性，后者是一个中心化的方案设计，通过类似一个分布式锁服务来保证强一致性,数据写入先写内存和redo log，然后定期compat归并到磁盘上，将随机写优化为顺序写，提高写入性能。 4.Schema free，auto-sharding等。比如目前常见的一些文档数据库都是支持schema-free的，直接存储json格式数据，并且支持auto-sharding等功能，比如mongodb。 面对这些不同类型的NoSQL产品,我们需要根据我们的业务场景选择最合适的产品。 Redis最适合所有数据in-momory的场景，虽然Redis也提供持久化功能，但实际更多的是一个disk-backed的功能，跟传统意义上的持久化有比较大的差别，那么可能大家就会有疑问，似乎Redis更像一个加强版的Memcached，那么何时使用Memcached,何时使用Redis呢? 如果简单地比较Redis与Memcached的区别，大多数都会得到以下观点： 1 、Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。2 、Redis支持数据的备份，即master-slave模式的数据备份。3 、Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。 2. Redis常用数据类型Redis最为常用的数据类型主要有以下：String Hash List Set Sorted set pub/sub Transactions 在具体描述这几种数据类型之前，我们先通过一张图了解下Redis内部内存管理中是如何描述这些不同数据类型的： 首先Redis内部使用一个redisObject对象来表示所有的key和value,redisObject最主要的信息如上图所示： type代表一个value对象具体是何种数据类型， encoding是不同数据类型在redis内部的存储方式， 比如：type=string代表value存储的是一个普通字符串，那么对应的encoding可以是raw或者是int,如果是int则代表实际redis内部是按数值型类存储和表示这个字符串的，当然前提是这个字符串本身可以用数值表示，比如:”123” “456”这样的字符串。 这里需要特殊说明一下vm字段，只有打开了Redis的虚拟内存功能，此字段才会真正的分配内存，该功能默认是关闭状态的，该功能会在后面具体描述。 通过上图我们可以发现Redis使用redisObject来表示所有的key/value数据是比较浪费内存的，当然这些内存管理成本的付出主要也是为了给Redis不同数据类型提供一个统一的管理接口，实际作者也提供了多种方法帮助我们尽量节省内存使用，我们随后会具体讨论。 3. 各种数据类型应用和实现方式 下面我们先来逐一的分析下这7种数据类型的使用和内部实现方式: String:Strings数据结构是简单的key-value类型，value其实不仅是String，也可以是数字.常用命令: set,get,decr,incr,mget 等。 应用场景：String是最常用的一种数据类型，普通的key/ value 存储都可以归为此类.即可以完全实现目前 Memcached 的功能，并且效率更高。还可以享受Redis的定时持久化，操作日志及 Replication等功能。 除了提供与 Memcached 一样的get、set、incr、decr 等操作外，Redis还提供了下面一些操作： 获取字符串长度往字符串append内容 设置和获取字符串的某一段内容 设置及获取字符串的某一位（bit） 批量设置一系列字符串的内容 实现方式： String在redis内部存储默认就是一个字符串，被redisObject所引用，当遇到incr,decr等操作时会转成数值型进行计算，此时redisObject的encoding字段为int。 Hash常用命令：hget,hset,hgetall 等。 应用场景：在Memcached中，我们经常将一些结构化的信息打包成HashMap，在客户端序列化后存储为一个字符串的值，比如用户的昵称、年龄、性别、积分等，这时候在需要修改其中某一项时，通常需要将所有值取出反序列化后，修改某一项的值，再序列化存储回去。这样不仅增大了开销，也不适用于一些可能并发操作的场合（比如两个并发的操作都需要修改积分）。而Redis的Hash结构可以使你像在数据库中Update一个属性一样只修改某一项属性值。 我们简单举个实例来描述下Hash的应用场景，比如我们要存储一个用户信息对象数据，包含以下信息：用户ID为查找的key，存储的value用户对象包含姓名，年龄，生日等信息，如果用普通的key/value结构来存储，主要有以下2种存储方式： 第一种方式将用户ID作为查找key,把其他信息封装成一个对象以序列化的方式存储，这种方式的缺点是，增加了序列化/反序列化的开销，并且在需要修改其中一项信息时，需要把整个对象取回，并且修改操作需要对并发进行保护，引入CAS等复杂问题。 第二种方法是这个用户信息对象有多少成员就存成多少个key-value对儿，用用户ID+对应属性的名称作为唯一标识来取得对应属性的值，虽然省去了序列化开销和并发问题，但是用户ID为重复存储，如果存在大量这样的数据，内存浪费还是非常可观的。那么Redis提供的Hash很好的解决了这个问题，Redis的Hash实际是内部存储的Value为一个HashMap，并提供了直接存取这个Map成员的接口，如下图： 也就是说，Key仍然是用户ID, value是一个Map，这个Map的key是成员的属性名，value是属性值，这样对数据的修改和存取都可以直接通过其内部Map的Key(Redis里称内部Map的key为field), 也就是通过 key(用户ID) + field(属性标签) 就可以操作对应属性数据了，既不需要重复存储数据，也不会带来序列化和并发修改控制的问题。很好的解决了问题。 这里同时需要注意，Redis提供了接口(hgetall)可以直接取到全部的属性数据,但是如果内部Map的成员很多，那么涉及到遍历整个内部Map的操作，由于Redis单线程模型的缘故，这个遍历操作可能会比较耗时，而另其它客户端的请求完全不响应，这点需要格外注意。 实现方式：上面已经说到Redis Hash对应Value内部实际就是一个HashMap，实际这里会有2种不同实现，这个Hash的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，对应的value redisObject的encoding为zipmap,当成员数量增大时会自动转成真正的HashMap,此时encoding为ht。 List常用命令：lpush,rpush,lpop,rpop,lrange等。应用场景：Redis list的应用场景非常多，也是Redis最重要的数据结构之一，比如twitter的关注列表，粉丝列表等都可以用Redis的list结构来实现。 Lists 就是链表，相信略有数据结构知识的人都应该能理解其结构。使用Lists结构，我们可以轻松地实现最新消息排行等功能。Lists的另一个应用就是消息队列，可以利用Lists的PUSH操作，将任务存在Lists中，然后工作线程再用POP操作将任务取出进行执行。Redis还提供了操作Lists中某一段的api，你可以直接查询，删除Lists中某一段的元素。 实现方式：Redis list的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销，Redis内部的很多实现，包括发送缓冲队列等也都是用的这个数据结构。 Set常用命令：sadd,spop,smembers,sunion 等。应用场景：Redis set对外提供的功能与list类似是一个列表的功能，特殊之处在于set是可以自动排重的，当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的。 Sets 集合的概念就是一堆不重复值的组合。利用Redis提供的Sets数据结构，可以存储一些集合性的数据，比如在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。 Redis还为集合提供了求交集、并集、差集等操作，可以非常方便的实现如共同关注、共同喜好、二度好友等功能，对上面的所有集合操作，你还可以使用不同的命令选择将结果返回给客户端还是存集到一个新的集合中。 实现方式：set 的内部实现是一个 value永远为null的HashMap，实际就是通过计算hash的方式来快速排重的，这也是set能提供判断一个成员是否在集合内的原因。 Sorted Set常用命令：zadd,zrange,zrem,zcard等使用场景：Redis sorted set的使用场景与set类似，区别是set不是自动有序的，而sorted set可以通过用户额外提供一个优先级(score)的参数来为成员排序，并且是插入有序的，即自动排序。 当你需要一个有序的并且不重复的集合列表，那么可以选择sorted set数据结构，比如twitter 的public timeline可以以发表时间作为score来存储，这样获取时就是自动按时间排好序的。 另外还可以用Sorted Sets来做带权重的队列，比如普通消息的score为1，重要消息的score为2，然后工作线程可以选择按score的倒序来获取工作任务。让重要的任务优先执行。 实现方式：Redis sorted set的内部使用HashMap和跳跃表(SkipList)来保证数据的存储和有序，HashMap里放的是成员到score的映射，而跳跃表里存放的是所有的成员，排序依据是HashMap里存的score,使用跳跃表的结构可以获得比较高的查找效率，并且在实现上比较简单。 Pub/SubPub/Sub从字面上理解就是发布（Publish）与订阅（Subscribe），在Redis中，你可以设定对某一个key值进行消息发布及消息订阅，当一个key值上进行了消息发布后，所有订阅它的客户端都会收到相应的消息。 这一功能最明显的用法就是用作实时消息系统，比如普通的即时聊天，群聊等功能。 Transactions谁说NoSQL都不支持事务，虽然Redis的Transactions提供的并不是严格的ACID的事务（比如一串用EXEC提交执行的命令，在执行中服务器宕机，那么会有一部分命令执行了，剩下的没执行），但是这个Transactions还是提供了基本的命令打包执行的功能（在服务器不出问题的情况下，可以保证一连串的命令是顺序在一起执行的，中间有会有其它客户端命令插进来执行）。 Redis还提供了一个Watch功能，你可以对一个key进行Watch，然后再执行Transactions，在这过程中，如果这个Watched的值进行了修改，那么这个Transactions会发现并拒绝执行。 4. Redis实际应用场景 Redis在很多方面与其他数据库解决方案不同：它使用内存提供主存储支持，而仅使用硬盘做持久性的存储； 它的数据模型非常独特，用的是单线程。另一个大区别在于，你可以在开发环境中使用Redis的功能，但却不需要转到Redis。转向Redis当然也是可取的，许多开发者从一开始就把Redis作为首选数据库；但设想如果你的开发环境已经搭建好，应用已经在上面运行了，那么更换数据库框架显然不那么容易。另外在一些需要大容量数据集的应用，Redis也并不适合，因为它的数据集不会超过系统可用的内存。所以如果你有大数据应用，而且主要是读取访问模式，那么Redis并不是正确的选择。 然而我喜欢Redis的一点就是你可以把它融入到你的系统中来，这就能够解决很多问题，比如那些你现有的数据库处理起来感到缓慢的任务。这些你就可以通过Redis来进行优化，或者为应用创建些新的功能。在本文中，我就想探讨一些怎样将Redis加入到现有的环境中，并利用它的原语命令等功能来解决 传统环境中碰到的一些常见问题。 在这些例子中，Redis都不是作为首选数据库。 1、显示最新的项目列表下面这个语句常用来显示最新项目，随着数据多了，查询毫无疑问会越来越慢。 1SELECT * FROM foo WHERE ... ORDER BY time DESC LIMIT 10 在Web应用中，“列出最新的回复”之类的查询非常普遍，这通常会带来可扩展性问题。这令人沮丧，因为项目本来就是按这个顺序被创建的，但要输出这个顺序却不得不进行排序操作。 类似的问题就可以用Redis来解决。比如说，我们的一个Web应用想要列出用户贴出的最新20条评论。在最新的评论边上我们有一个“显示全部”的链接，点击后就可以获得更多的评论。 我们假设数据库中的每条评论都有一个唯一的递增的ID字段。 我们可以使用分页来制作主页和评论页，使用Redis的模板，每次新评论发表时，我们会将它的ID添加到一个Redis列表：1LPUSH latest.comments &lt;ID&gt; 我们将列表裁剪为指定长度，因此Redis只需要保存最新的5000条评论： 1LTRIM latest.comments 0 5000 每次我们需要获取最新评论的项目范围时，我们调用一个函数来完成（使用伪代码）： 1234567 FUNCTION get_latest_comments(start, num_items): id_list = redis.lrange(&quot;latest.comments&quot;,start,start+num_items - 1) IF id_list.length &lt; num_items id_list = SQL_DB(&quot;SELECT ... ORDER BY time LIMIT ...&quot;) END RETURN id_list END 这里我们做的很简单。在Redis中我们的最新ID使用了常驻缓存，这是一直更新的。但是我们做了限制不能超过5000个ID，因此我们的获取ID函数会一直询问Redis。只有在start/count参数超出了这个范围的时候，才需要去访问数据库。 我们的系统不会像传统方式那样“刷新”缓存，Redis实例中的信息永远是一致的。SQL数据库（或是硬盘上的其他类型数据库）只是在用户需要获取“很远”的数据时才会被触发，而主页或第一个评论页是不会麻烦到硬盘上的数据库了。 2、删除与过滤我们可以使用LREM来删除评论。如果删除操作非常少，另一个选择是直接跳过评论条目的入口，报告说该评论已经不存在。 有些时候你想要给不同的列表附加上不同的过滤器。如果过滤器的数量受到限制，你可以简单的为每个不同的过滤器使用不同的Redis列表。毕竟每个列表只有5000条项目，但Redis却能够使用非常少的内存来处理几百万条项目。 3、排行榜相关另一个很普遍的需求是各种数据库的数据并非存储在内存中，因此在按得分排序以及实时更新这些几乎每秒钟都需要更新的功能上数据库的性能不够理想。 典型的比如那些在线游戏的排行榜，比如一个Facebook的游戏，根据得分你通常想要： 列出前100名高分选手 列出某用户当前的全球排名 这些操作对于Redis来说小菜一碟，即使你有几百万个用户，每分钟都会有几百万个新的得分。 模式是这样的，每次获得新得分时，我们用这样的代码：1ZADD leaderboard &lt;score&gt; &lt;username&gt; 你可能用userID来取代username，这取决于你是怎么设计的。 得到前100名高分用户很简单：1ZREVRANGE leaderboard 0 99 用户的全球排名也相似，只需要：1ZRANK leaderboard &lt;username&gt; 4、按照用户投票和时间排序排行榜的一种常见变体模式就像Reddit或Hacker News用的那样，新闻按照类似下面的公式根据得分来排序： score = points / time^alpha 因此用户的投票会相应的把新闻挖出来，但时间会按照一定的指数将新闻埋下去。下面是我们的模式，当然算法由你决定。 模式是这样的，开始时先观察那些可能是最新的项目，例如首页上的1000条新闻都是候选者，因此我们先忽视掉其他的，这实现起来很简单。 每次新的新闻贴上来后，我们将ID添加到列表中，使用LPUSH + LTRIM，确保只取出最新的1000条项目。 有一项后台任务获取这个列表，并且持续的计算这1000条新闻中每条新闻的最终得分。计算结果由ZADD命令按照新的顺序填充生成列表，老新闻则被清除。这里的关键思路是排序工作是由后台任务来完成的。 5、处理过期项目另一种常用的项目排序是按照时间排序。我们使用unix时间作为得分即可。 模式如下： 每次有新项目添加到我们的非Redis数据库时，我们把它加入到排序集合中。这时我们用的是时间属性，current_time和time_to_live。 另一项后台任务使用ZRANGE…SCORES查询排序集合，取出最新的10个项目。如果发现unix时间已经过期，则在数据库中删除条目。 6、计数Redis是一个很好的计数器，这要感谢INCRBY和其他相似命令。 我相信你曾许多次想要给数据库加上新的计数器，用来获取统计或显示新信息，但是最后却由于写入敏感而不得不放弃它们。 好了，现在使用Redis就不需要再担心了。有了原子递增（atomic increment），你可以放心的加上各种计数，用GETSET重置，或者是让它们过期。 例如这样操作：1INCR user:&lt;id&gt; EXPIRE user:&lt;id&gt; 60 你可以计算出最近用户在页面间停顿不超过60秒的页面浏览量，当计数达到比如20时，就可以显示出某些条幅提示，或是其它你想显示的东西。 7、特定时间内的特定项目另一项对于其他数据库很难，但Redis做起来却轻而易举的事就是统计在某段特点时间里有多少特定用户访问了某个特定资源。比如我想要知道某些特定的注册用户或IP地址，他们到底有多少访问了某篇文章。 每次我获得一次新的页面浏览时我只需要这样做：1SADD page:day1:&lt;page_id&gt; &lt;user_id&gt; 当然你可能想用unix时间替换day1，比如time()-(time()%3600*24)等等。 想知道特定用户的数量吗？只需要使用1SCARD page:day1:&lt;page_id&gt; 需要测试某个特定用户是否访问了这个页面？1SISMEMBER page:day1:&lt;page_id&gt; 8、实时分析正在发生的情况用于数据统计与防止垃圾邮件等 我们只做了几个例子，但如果你研究Redis的命令集，并且组合一下，就能获得大量的实时分析方法，有效而且非常省力。使用Redis原语命令，更容易实施垃圾邮件过滤系统或其他实时跟踪系统。 9、Pub/SubRedis的Pub/Sub非常非常简单，运行稳定并且快速。支持模式匹配，能够实时订阅与取消频道。 10、队列你应该已经注意到像list push和list pop这样的Redis命令能够很方便的执行队列操作了，但能做的可不止这些：比如Redis还有list pop的变体命令，能够在列表为空时阻塞队列。 现代的互联网应用大量地使用了消息队列（Messaging）。消息队列不仅被用于系统内部组件之间的通信，同时也被用于系统跟其它服务之间的交互。 消息队列的使用可以增加系统的可扩展性、灵活性和用户体验。 非基于消息队列的系统，其运行速度取决于系统中最慢的组件的速度（注：短板效应）。而基于消息队列可以将系统中各组件解除耦合，这样系统就不再受最慢组件的束缚，各组件可以异步运行从而得以更快的速度完成各自的工作。 此外，当服务器处在高并发操作的时候，比如频繁地写入日志文件。可以利用消息队列实现异步处理。从而实现高性能的并发操作。 11、缓存Redis的缓存部分值得写一篇新文章，我这里只是简单的说一下。Redis能够替代memcached，让你的缓存从只能存储数据变得能够更新数据，因此你不再需要每次都重新生成数据了。 数据持久化支持Redis虽然是基于内存的存储系统，但是它本身是支持内存数据的持久化的，而且提供两种主要的持久化策略：RDB快照和AOF日志。 而memcached是不支持数据持久化操作的。 1）RDB快照Redis支持将当前数据的快照存成一个数据文件的持久化机制，即RDB快照。但是一个持续写入的数据库如何生成快照呢？ Redis借助了fork命令的copy on write机制。在生成快照时，将当前进程fork出一个子进程，然后在子进程中循环所有的数据，将数据写成为RDB文件。 我们可以通过Redis的save指令来配置RDB快照生成的时机，比如配置10分钟就生成快照，也可以配置有1000次写入就生成快照，也可以多个规则一起实施。 这些规则的定义就在Redis的配置文件中，你也可以通过Redis的CONFIG SET命令在Redis运行时设置规则，不需要重启Redis。 Redis的RDB文件不会坏掉，因为其写操作是在一个新进程中进行的，当生成一个新的RDB文件时，Redis生成的子进程会先将数据写到一个临时文件中，然后通过原子性rename系统调用将临时文件重命名为RDB文件，这样在任何时候出现故障，Redis的RDB文件都总是可用的。 同时，Redis的RDB文件也是Redis主从同步内部实现中的一环。RDB有他的不足，就是一旦数据库出现问题，那么我们的RDB文件中保存的数据并不是全新的，从上次RDB文件生成到Redis停机这段时间的数据全部丢掉了。在某些业务下，这是可以忍受的。 2）AOF日志AOF日志的全称是append only file，它是一个追加写入的日志文件。与一般数据库的binlog不同的是，AOF文件是可识别的纯文本，它的内容就是一个个的Redis标准命令。只有那些会导致数据发生修改的命令才会追加到AOF文件。每一条修改数据的命令都生成一条日志。 AOF文件会越来越大，所以Redis又提供了一个功能，叫做AOF rewrite。其功能就是重新生成一份AOF文件，新的AOF文件中一条记录的操作只会有一次，而不像一份老文件那样，可能记录了对同一个值的多次操作。其生成过程和RDB类似，也是fork一个进程，直接遍历数据，写入新的AOF临时文件。 在写入新文件的过程中，所有的写操作日志还是会写到原来老的AOF文件中，同时还会记录在内存缓冲区中。当重完操作完成后，会将所有缓冲区中的日志一次性写入到临时文件中。然后调用原子性的rename命令用新的AOF文件取代老的AOF文件。 AOF是一个写文件操作，其目的是将操作日志写到磁盘上，所以它也同样会遇到我们上面说的写操作的流程。在Redis中对AOF调用write写入后，通过appendfsync选项来控制调用fsync将其写到磁盘上的时间，下面appendfsync的三个设置项，安全强度逐渐变强。 appendfsync no 当设置appendfsync为no的时候，Redis不会主动调用fsync去将AOF日志内容同步到磁盘，所以这一切就完全依赖于操作系统的调试了。对大多数Linux操作系统，是每30秒进行一次fsync，将缓冲区中的数据写到磁盘上。 appendfsync everysec 当设置appendfsync为everysec的时候，Redis会默认每隔一秒进行一次fsync调用，将缓冲区中的数据写到磁盘。但是当这一次的fsync调用时长超过1秒时。Redis会采取延迟fsync的策略，再等一秒钟。也就是在两秒后再进行fsync，这一次的fsync就不管会执行多长时间都会进行。这时候由于在fsync时文件描述符会被阻塞，所以当前的写操作就会阻塞。 所以结论就是，在绝大多数情况下，Redis会每隔一秒进行一次fsync。在最坏的情况下，两秒钟会进行一次fsync操作。这一操作在大多数数据库系统中被称为group commit，就是组合多次写操作的数据，一次性将日志写到磁盘。 appednfsync always 当设置appendfsync为always时，每一次写操作都会调用一次fsync，这时数据是最安全的，当然，由于每次都会执行fsync，所以其性能也会受到影响。 对于一般性的业务需求，建议使用RDB的方式进行持久化，原因是RDB的开销并相比AOF日志要低很多，对于那些无法忍数据丢失的应用，建议使用AOF日志。 集群管理的不同Memcached是全内存的数据缓冲系统，Redis虽然支持数据的持久化，但是全内存毕竟才是其高性能的本质。 作为基于内存的存储系统来说，机器物理内存的大小就是系统能够容纳的最大数据量。如果需要处理的数据量超过了单台机器的物理内存大小，就需要构建分布式集群来扩展存储能力。 Memcached本身并不支持分布式，因此只能在客户端通过像一致性哈希这样的分布式算法来实现Memcached的分布式存储。 当客户端向Memcached集群发送数据之前，首先会通过内置的分布式算法计算出该条数据的目标节点，然后数据会直接发送到该节点上存储。但客户端查询数据时，同样要计算出查询数据所在的节点，然后直接向该节点发送查询请求以获取数据。 相较于Memcached只能采用客户端实现分布式存储，Redis更偏向于在服务器端构建分布式存储。最新版本的Redis已经支持了分布式存储功能。 Redis Cluster是一个实现了分布式且允许单点故障的Redis高级版本，它没有中心节点，具有线性可伸缩的功能。 Redis Cluster将整个key的数值域分成4096个哈希槽，每个节点上可以存储一个或多个哈希槽，也就是说当前Redis Cluster支持的最大节点数就是4096。 Redis Cluster使用的分布式算法也很简单：crc16( key ) % HASH_SLOTS_NUMBER。 为了保证单点故障下的数据可用性，Redis Cluster引入了Master节点和Slave节点。 在Redis Cluster中，每个Master节点都会有对应的两个用于冗余的Slave节点。 这样在整个集群中，任意两个节点的宕机都不会导致数据的不可用。当Master节点退出后，集群会自动选择一个Slave节点成为新的Master节点。 有问题可以联系博主。 转载请注明出处，谢谢！]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[grep统计某个单词的个数，行数等]]></title>
    <url>%2F2018%2F03%2F12%2Fgrep%E7%BB%9F%E8%AE%A1%E6%9F%90%E4%B8%AA%E5%8D%95%E8%AF%8D%E7%9A%84%E4%B8%AA%E6%95%B0%EF%BC%8C%E8%A1%8C%E6%95%B0%E7%AD%89%2F</url>
    <content type="text"><![CDATA[文件内容 1234567j@j:~$ cat hello.c#include&lt;stdio.h&gt;//include includeint main()&#123; printf(&quot;hello world/n&quot;); exit(0);&#125; grep命令查看include关键词 1234j@j:~$ grep -o include hello.cincludeincludeinclude 如上所示， 所有的include都显示出来了， 包括有两个include的行。 命令1grep -c -o include hello.c 的结果是2， -c参数只能统计行。如果要统计个数， 可以再来个重定向： 1grep -o include hello.c| grep -c include 得到include的个数 3 有问题可以联系博主，博客主页有邮箱，微博，文章底部有微信公众号等联系方式。 转载请注明出处，谢谢！]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用meerkat进行服务监控和服务降级]]></title>
    <url>%2F2018%2F01%2F29%2F%E4%BD%BF%E7%94%A8meerkat%E8%BF%9B%E8%A1%8C%E6%9C%8D%E5%8A%A1%E7%9B%91%E6%8E%A7%E5%92%8C%E6%9C%8D%E5%8A%A1%E9%99%8D%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[meerkat进行服务监控和服务降级meerkat 是用于服务监控以及服务降级基础组件，主要为了解决调用外部接口的时候进行成功率，响应时间，QPS指标的监控，同时在成功率下降到预设的阈值以下的时候自动切断外部接口的调用，外部接口成功率恢复后自动恢复请求。本文将对使用方式以及进阶特性进行介绍。 项目主页： https://github.com/ChanningBJ… 为什么要进行监控和熔断在我们的Java服务中，经常会调用外部的一些接口进行数据的获取操作，当这些外部接口的成功率比较低的时候会直接影响到服务本身的成功率，因此我们添加了对外部接口调用的成功率和响应时间监控，这样可以在造成大量用户影响之前预先发现并解决问题。同时，对于接口中的非关键数据，我们采取了更具成功率判断进行触发熔断的方式，当成功率下降到预定的阀值以下的时候自动停止对这个外部接口的访问以便保证关键数据能够正常提供，当成功率恢复以后自动恢复请求。 meerkat主要功能监控：监控Java内部操作的成功率以及响应时间指标上报：log文件和Grafhite两种监控指标上报方式，支持扩展其他的上报方式熔断：（可选功能）成功率下降到预设的阈值以下触发熔断保护，暂定对外部接口的访问，成功率恢复以后自动恢复访问 基本使用12345&lt;dependency&gt; &lt;groupId&gt;com.github.channingbj&lt;/groupId&gt; &lt;artifactId&gt;meerkat&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt;&lt;/dependency&gt; 定义受监控的操作假设我们的服务中需要从HTTP接口查询一个节目的播放次数，为了防止这个HTTP接口大量超时影响我们自身服务的质量，可以定义一个查询Command：123456789101112131415161718192021public class GetPlayCountCommand extends FusingCommand&lt;Long&gt; &#123; private final Long videoID; public GetPlayCountCommand(Long videoID) &#123; this.videoID = videoID; &#125; protected Optional&lt;Long&gt; run() &#123; Long result = 0l; // 调用HTTP接口获取视频的播放次数信息 // 如果调用失败，返回 null 或者抛出异常，会将这次操作记录为失败 // 如果ID非法，返回 Optional.absent(),会将这次操作记录为成功 return Optional.fromNullable(result); &#125;&#125;执行查询：//获取视频ID为123的视频的播放次数GetPlayCountCommand command = new GetPlayCountCommand(123l);Long result = command.execute(); // 执行查询操作，如果执行失败或者处于熔断状态，返回 null 配置监控上报在服务初始化的时候需要对监控上报进行设置。下面的例子中开启了监控数据向日志文件的打印123MeterCenter.INSTANCE .enableReporter(new EnablingLogReporter(&quot;org.apache.log4j.RollingFileAppender&quot;)) .init(); 查看统计结果统计结果会以熔断命令类名为进行分组。例如前面我们定义的 GetPlayCountCommand 类,package name 是 com.test，那么在日志中的输出将会是这个样子： 123type=GAUGE, name=com.test.GetPlayCountCommand.normal-rate, value=0.0type=GAUGE, name=com.test.GetPlayCountCommand.success-rate, value=61.0type=TIMER, name=com.test.GetPlayCountCommand.time, count=25866500, min=0.0, max=0.001, mean=3.963926781047921E-5, stddev=1.951102156677818E-4, median=0.0, p75=0.0, p95=0.0, p98=0.001, p99=0.001, p999=0.001, mean_rate=649806.0831335272, m1=1665370.7316699813, m5=2315813.300713087, m15=2446572.324069477, rate_unit=events/second, duration_unit=milliseconds 单独使用监控功能如果不想使用熔断功能，只是想监控Java方法调用的耗时和成功率，可以直接使用 OperationMeter 进行实现，只需要在函数调用的前后添加开始和结束的调用即可：123456789101112//创建一个操作的计数器 OperationMeter meter = MeterCenter.INSTANCE.getOrCreateMeter(OperationMeterTest.class, OperationMeter.class); //模拟成功率60% for(int k=0; k&lt;100; k++)&#123; Timer.Context context = meter.startOperation(); if(k%10&lt;6)&#123; meter.endOperation(context, OperationMeter.Result.SUCCESS); &#125; else &#123; meter.endOperation(context, OperationMeter.Result.FAILURE); &#125; &#125; 开启熔断并配置阀值和持续时间首先创建一个接口，继承自FusingConfig，用于指定配置文件的加载路径，同时还可以设定配置文件的刷新时间，具体定义方法请参照 ++owner++ 文档123456@Config.Sources(&quot;classpath:app_config.properties&quot;)@Config.HotReload( value = 1, unit = java.util.concurrent.TimeUnit.MINUTES, type = Config.HotReloadType.ASYNC)public interface APPFusingConfig extends FusingConfig &#123;&#125; 创建查询Command的时候在构造函数中传入1234567891011121314151617public class GetPlayCountCommand extends FusingCommand&lt;Long&gt; &#123; private final Long videoID; public GetPlayCountCommand(Long videoID) &#123; super( APPFusingConfig.class); //设定配置文件 this.videoID = videoID; &#125; protected Optional&lt;Long&gt; run() &#123; Long result = 0l; // 调用HTTP接口获取视频的播放次数信息 // 如果调用失败，返回 null 或者抛出异常，会将这次操作记录为失败 // 如果ID非法，返回 Optional.absent(),会将这次操作记录为成功 return Optional.fromNullable(result); &#125;&#125; 配置文件内容如下： 监控项 含义 默认值 fusing.[CommandClassName].mode 熔断模式：FORCE_NORMAL－关闭熔断功能;AUTO_FUSING－自动进入熔断模式;FORCE_NORMAL－强制进行熔断 FORCE_NORMAL fusing.[CommandClassName].duration 触发一次熔断以后持续的时间，支持ms,sec,min 单位。例如 10sec 50sec fusing.[CommandClassName].success_rate_threshold 触发熔断的成功率阀值，降低到这个成功率以下将触发熔断，例如0.9表示成功率90% 0.9 配置文件中的 CommandClassName 是每个操作类的名称，可以为每个操作单独设置上述参数。同时，这个配置文件支持动态加载，乐意通过修改fusing.[CommandClassName].mode 手工触发或者关闭熔断。 监控指标上报Graphite我们的服务中使用的是Metric+Graphite+Gafana进行监控数据的采集存储和展现，下面将介绍如何配置监控数据上报Grafana，关于Graphite+Grafana的配置，可以参考文章：使用graphite和grafana进行应用程序监控 定义配置文件首先定义一个接口，继承自GraphiteReporterConfig，通过这个接口定义配置文件的加载路径。配置文件路径的定义方法请参照 ++owner++ 文档, 下面是一个例子：123@Config.Sources(&quot;classpath:config.properties&quot;)public interface MyConfig extends GraphiteReporterConfig &#123;&#125; 配置文件中需要定义下列内容： 配置项 含义 meter.reporter.enabled.hosts 开启监控上报的服务器列表 meter.reporter.perfix 上报使用的前缀 meter.reporter.carbon.host grafana(carbon-cache) 的 IP 地址，用于存储监控数据 meter.reporter.carbon.port grafana(carbon-cache) 的端口 下面这个例子是在192.168.0.0.1和192.168.0.0.2两台服务器上开启监控数据上报，上报监控指标的前缀是12345project_name.dc：meter.reporter.enabled.hosts = 192.168.0.0.1,192.168.0.0.2meter.reporter.perfix = project_name.dcmeter.reporter.carbon.host = hostname.graphite 由于相同机房的不同服务器对外部接口的访问情况一般比较类似，所以仅选取部分机器上报，也是为了节省资源。仅选择部分机器上报不影响熔断效果。 初始化配置上报在服务初始化的时候需要对监控上报进行设置。下面的例子中开启了监控数据向日志文件的打印，同时通过MyConfig指定的配置文件加载Graphite配置信息。1234MeterCenter.INSTANCE .enableReporter(new EnablingLogReporter(&quot;org.apache.log4j.RollingFileAppender&quot;)) .enableReporter(new EnablingGraphiteReporter(MyConfig.class)) //监控数据上报Grafana .init(); 查看统计结果统计结果会以熔断命令类名为进行分组。例如前面我们定义的 GetPlayCountCommand 类,package name 是 com.test，那么在日志中的输出将会是这个样子：123type=GAUGE, name=com.test.GetPlayCountCommand.normal-rate, value=0.0type=GAUGE, name=com.test.GetPlayCountCommand.success-rate, value=61.0type=TIMER, name=com.test.GetPlayCountCommand.time, count=25866500, min=0.0, max=0.001, mean=3.963926781047921E-5, stddev=1.951102156677818E-4, median=0.0, p75=0.0, p95=0.0, p98=0.001, p99=0.001, p999=0.001, mean_rate=649806.0831335272, m1=1665370.7316699813, m5=2315813.300713087, m15=2446572.324069477, rate_unit=events/second, duration_unit=milliseconds 监控项 含义 [classname].success-rate 成功率 [classname].time.m1 QPS [classname].time.mean 平均响应时间 [classname].normal-rate 过去1分钟内处于正常访问（非熔断）的时间比例 在Grafanna中可以看到下面的监控图： 自定义监控上报meerkat使用++Metrics++进行监控数据的统计，因此可以使用Metrics支持的所有reporter进行上报。添加一种上报的时候，只需要实现 EnablingReporter 并在 MeterCenter 初始化之前进行调用即可。下面是log reporter的实现，可以作为参考12345678910111213141516public class EnablingLogReporter implements EnablingReporter &#123; private String loggername; public EnablingLogReporter(String loggername) &#123; this.loggername = loggername; &#125; @Override public void invoke(MetricRegistry metricRegistry, long period, TimeUnit timeUnit) &#123; Slf4jReporter.forRegistry(metricRegistry) .outputTo(LoggerFactory.getLogger(loggername)) .convertRatesTo(java.util.concurrent.TimeUnit.SECONDS) .convertDurationsTo(java.util.concurrent.TimeUnit.MILLISECONDS) .build().start(period, timeUnit); &#125;&#125; MeterCenter 初始化的时候开启reporter123MeterCenter.INSTANCE .enableReporter(new EnablingLogReporter(&quot;org.apache.log4j.RollingFileAppender&quot;)) .init(); 多实例监控多实例监控主要是为了解决一个被监控操作的实现类需要根据输入参数的不同分别进行监控和熔断的情况，通过定义实例的名称进行实现。例如获取视频播放次数的例子，获取视频播放次数的接口对于不同的视频类型而言请求逻辑是一样的，所以使用同一个类进行实现；但是对于不同的视频类型，接口实现的复杂程度不同导致成功率不同，当用户上传的视频的播次接口大量失败的时候我们不希望同时熔断电影电视剧这类视频的播放次数获取，这时就需要使用多实例这种特性进行监控和熔断。 下面是一个单实例的实现：1234567891011121314151617public class GetPlayCountCommand extends FusingCommand&lt;Long&gt; &#123; private final Long videoID; public GetPlayCountCommand(Long videoID) &#123; super( APPFusingConfig.class); this.videoID = videoID; &#125; protected Optional&lt;Long&gt; run() &#123; Long result = 0l; // 调用HTTP接口获取视频的播放次数信息 // 如果调用失败，返回 null 或者抛出异常，会将这次操作记录为失败 // 如果ID非法，返回 Optional.absent(),会将这次操作记录为成功 return Optional.fromNullable(result); &#125;&#125; 假设业务上我们可以根据视频ID判断视频类型，可以在类初始化的时候根据类型创建多种监控实例，添加了多实例支持的实现如下：1234567891011121314151617181920public class GetPlayCountCommand extends FusingCommand&lt;Long&gt; &#123; private final Long videoID; public GetPlayCountCommand(Long videoID) &#123; super( getVideoType(videoID), APPFusingConfig.class); this.videoID = videoID; &#125; private static String getVideoType(Long videoID)&#123; return &quot;PGC&quot;; //根据videoID进行判断，返回 &quot;PGC&quot; 或者 &quot;UGC&quot; 这两个类别 &#125; protected Optional&lt;Long&gt; run() &#123; Long result = 0l; // 调用HTTP接口获取视频的播放次数信息 // 如果调用失败，返回 null 或者抛出异常，会将这次操作记录为失败 // 如果ID非法，返回 Optional.absent(),会将这次操作记录为成功 return Optional.fromNullable(result); &#125; 由于每个实例独享一个监控指标，日志中的监控个结果是这个样子： 123456type=GAUGE, name=com.test.GetPlayCountCommand.PGC.normal-rate, value=100.0type=GAUGE, name=com.test.GetPlayCountCommand.PGC.success-rate, value=100.0type=GAUGE, name=com.test.GetPlayCountCommand.UGC.normal-rate, value=100.0type=GAUGE, name=com.test.GetPlayCountCommand.UGC.success-rate, value=60.0type=TIMER, name=com.test.GetPlayCountCommand.PGC.time, count=100, min=0.0, max=0.509, mean=0.00635, stddev=0.05052135687013958, median=0.001, p75=0.002, p95=0.002, p98=0.003, p99=0.003, p999=0.509, mean_rate=1.6680162586215173, m1=8.691964170141569, m5=16.929634497812284, m15=18.919189378135307, rate_unit=events/second, duration_unit=millisecondstype=TIMER, name=com.test.GetPlayCountCommand.UGC.time, count=100, min=0.0, max=0.027, mean=0.00132, stddev=0.0026939933184772376, median=0.001, p75=0.001, p95=0.002, p98=0.005, p99=0.006, p999=0.027, mean_rate=1.6715904477699361, m1=8.691964170141569, m5=16.929634497812284, m15=18.919189378135307, rate_unit=events/second, duration_unit=milliseconds 相应的，对熔断阀值以及持续时间的配置也需要明确指出实例的名字： 1234567fusing.GetPlayCountCommand.UGC.mode = AUTO_FUSINGfusing.GetPlayCountCommand.UGC.duration = 50secfusing.GetPlayCountCommand.UGC.success_rate_threshold = 0.9fusing.GetPlayCountCommand.PGC.mode = AUTO_FUSINGfusing.GetPlayCountCommand.PGC.duration = 50secfusing.GetPlayCountCommand.PGC.success_rate_threshold = 0.9 参考资料https://segmentfault.com/a/1190000009730789 有问题可以联系博主，博客主页有邮箱，微博，文章底部有微信公众号等联系方式。 转载请注明出处，谢谢！]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>熔断</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux中date命令的各种实用方法]]></title>
    <url>%2F2018%2F01%2F10%2FLinux%E4%B8%ADdate%E5%91%BD%E4%BB%A4%E7%9A%84%E5%90%84%E7%A7%8D%E5%AE%9E%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Linux中date命令的各种实用方法 用法：date [选项]… [+格式] 或：date [-u|–utc|–universal] [MMDDhhmm[[CC]YY][.ss]]以给定的格式显示当前时间，或是设置系统日期。 -d,–date=字符串 显示指定字符串所描述的时间，而非当前时间 -f,–file=日期文件 类似–date，从日期文件中按行读入时间描述 -r, –reference=文件 显示文件指定文件的最后修改时间 -s, –set=字符串 设置指定字符串来分开时间 -u, –utc, –universal 输出或者设置协调的通用时间 –help 显示此帮助信息并退出 –version 显示版本信息并退出 读者可以设定特定的格式，格式设定规则：一个加号后接数个标记，每个标记中都有%，其中可用的标记列表和说明如下: %n : 下一行 %t : 跳格 %H : 小时(00..23) %I : 小时(01..12) %k : 小时(0..23) %l : 小时(1..12) %M : 分钟(00..59) %p : 显示本地 AM 或 PM %r : 直接显示时间 (12 小时制，格式为 hh:mm:ss [AP]M) %s : 从 1970 年 1 月 1 日 00:00:00 UTC 到目前为止的秒数%S : 秒(00..59) %T : 直接显示时间 (24 小时制) %X : 相当于 %H:%M:%S %Z : 显示时区 %a : 星期几 (Sun..Sat) %A : 星期几(Sunday..Saturday) %b : 月份 (Jan..Dec) %B : 月份 (January..December) %c : 直接显示日期与时间 %d : 日 (01..31) %D : 直接显示日期 (mm/dd/yy) %h : 同 %b %j : 一年中的第几天 (001..366) %m : 月份 (01..12) %U : 一年中的第几周 (00..53) (以 Sunday 为一周的第一天的情形) %w : 一周中的第几天 (0..6) %W : 一年中的第几周 (00..53) (以 Monday 为一周的第一天的情形) %x : 直接显示日期 (mm/dd/yy) %y : 年份的最后两位数字 (00.99) %Y : 完整年份 (0000..9999) ##1.按照特定的格式输出时间：短接符”-“、空格” “和冒号”:” 为分隔符，其中空格前面加了转义符号”\” 12[root@RHEL601 tmp]# date +%Y-%m-%d\ %H:%M:%S2012-07-19 21:10:28 在当前时间的基础上往前推或者往后推三天1234[root@RHEL601 tmp]# date -d &quot;+3 day&quot; 2012年 07月 22日 星期日 20:12:08 CST[root@RHEL601 tmp]# date -d &quot;-3 day&quot; 2012年 07月 16日 星期一 20:12:12 CST 在当前时间的基础上往前推或者往后推三个月1234[root@RHEL601 tmp]# date -d &quot;-3 month&quot; 2012年 04月 19日 星期四 20:12:39 CST[root@RHEL601 tmp]# date -d &quot;+3 month&quot; 2012年 10月 19日 星期五 20:12:48 CST 在当前时间的基础上往前推或者往后推三年1234[root@RHEL601 tmp]# date -d &quot;+3 year&quot; 2015年 07月 19日 星期日 20:13:06 CST[root@RHEL601 tmp]# date -d &quot;-3 year&quot; 2009年 07月 19日 星期日 20:13:11 CST 在当前时间的基础上往前推或者往后推三小时1234[root@RHEL601 tmp]# date -d &quot;-3 hour&quot; 2012年 07月 19日 星期四 17:13:20 CST[root@RHEL601 tmp]# date -d &quot;+3 hour&quot; 2012年 07月 19日 星期四 23:13:24 CST 在当前时间的基础上往前推或者往后推三分钟1234[root@RHEL601 tmp]# date -d &quot;+3 minute&quot; 2012年 07月 19日 星期四 20:16:56 CST[root@RHEL601 tmp]# date -d &quot;-3 minute&quot; 2012年 07月 19日 星期四 20:10:59 CST 在当前时间的基础上往前推或者往后推三十秒123456[root@RHEL601 tmp]# date &amp;&amp; date -d &quot;-30 second&quot; 2012年 07月 19日 星期四 20:14:24 CST2012年 07月 19日 星期四 20:13:54 CST[root@RHEL601 tmp]# date &amp;&amp; date -d &quot;+30 second&quot; 2012年 07月 19日 星期四 20:14:29 CST2012年 07月 19日 星期四 20:14:59 CST ##2、接下来的范例说明如何用date来表示各种各样的时间，表示的都是某一天的零点时间，也可以在当前的时分秒的基础上表示时间，(特别注意中间用到了反单引号`)参见范例12345678910111213141516171819202122232425262728293031323334353637383940date -d `date +%y%m01` #本月第一天[root@RHEL601 tmp]# date -d `date +%y%m01`2012年 07月 01日 星期日 00:00:00 CSTdate -d `date +%y%m01`&quot;-1 day&quot; #上个月最后一天[root@RHEL601 tmp]# date -d `date +%y%m01`&quot;-1 day&quot;2012年 06月 30日 星期六 00:00:00 CSTdate -d `date -d &quot;-3 month&quot; +%y%m01`&quot;-1 day&quot; #4个月前的第一天[root@RHEL601 tmp]# date -d `date -d &quot;-3 month&quot; +%y%m01`&quot;-1 day&quot;2012年 03月 31日 星期六 00:00:00 CSTdate -d `date -d &quot;+12 month&quot; +%y%m01`&quot;-1 day&quot; #第11个月后的第一天[root@RHEL601 tmp]# date -d `date -d &quot;+12 month&quot; +%y%m01`&quot;-1 day&quot;2013年 06月 30日 星期日 00:00:00 CSTdate -d `date -d &quot;-1 month&quot; +%y%m01` #上个月第一天[root@RHEL601 tmp]# date -d `date -d &quot;-1 month&quot; +%y%m01`2012年 06月 01日 星期五 00:00:00 CSTdate -d `date -d &quot;+12 month&quot; +%y%m01` #第12个月后的第一天[root@RHEL601 tmp]# date -d `date -d &quot;+12 month&quot; +%y%m01`2013年 07月 01日 星期一 00:00:00 CSTdate -d `date -d &quot;-1 day&quot; +%Y%m%d` #前一天零点时间[root@RHEL601 tmp]# date -d `date -d &quot;-1 day&quot; +%Y%m%d`2012年 07月 18日 星期三 00:00:00 CSTdate -d `date -d &quot;-3 day&quot; +%Y%m%d` #前三天零点时间[root@RHEL601 tmp]# date -d `date -d &quot;-3 day&quot; +%Y%m%d` 2012年 07月 16日 星期一 00:00:00 CSTdate -d `date -d &quot;+1 day&quot; +%Y%m%d` #明天零点时间[root@RHEL601 tmp]# date -d `date -d &quot;+1 day&quot; +%Y%m%d` 2012年 07月 20日 星期五 00:00:00 CSTdate -d `date -d &quot;+3 day&quot; +%Y%m%d` #往后推三天零点时间[root@RHEL601 tmp]# date -d `date -d &quot;+3 day&quot; +%Y%m%d` 2012年 07月 22日 星期日 00:00:00 CST ##3、以下简单示范了字母大小写在date命令中的区别Y #代表完整的年份，例如:2012年 将显示 2012y #代表缩写年份，例如：2012年 缩写为 121234[root@RHEL601 tmp]# date +&quot;%y%m01%H%M%S&quot;121201121556[root@RHEL601 tmp]# date +&quot;%Y%m01%H%M%S&quot;20121201121610 ##4、以下范例说明如何调整服务器的时间1ntpdate 210.72.145.44 将服务器时间调整为正常时间，210.72.145.44 是国家授时中心服务器IP地址12[root@RHEL601 tmp]# ntpdate 210.72.145.4419 Jul 13:07:07 ntpdate[15150]: adjust time server 210.72.145.44 offset -0.020920 sec date 121212122012 将时间设置为2012年 12月 12日 星期三 12:12:00，date后面的数字代表月日时分年，还可以加秒，需要后面跟英文状态下的句号字符”.”，例如：121212122012.1212345678910111213141516171819[root@RHEL601 tmp]# date 1212121220122012年 12月 12日 星期三 12:12:00 CST[root@RHEL601 tmp]# date 121212122012.122012年 12月 12日 星期三 12:12:12 CST[root@RHEL601 tmp]# date `date -d &quot;1 day ago&quot; +%m%d%H%M%Y.%S`2012年 07月 18日 星期三 20:13:04 CST[root@RHEL601 tmp]# date2012年 07月 18日 星期三 20:13:10 CST[root@RHEL601 tmp]# date `date -d &quot;3 days ago&quot; +%m%d%H%M%Y.%S`2012年 07月 15日 星期日 20:13:18 CST[root@RHEL601 tmp]# date `date -d &quot;5 days ago&quot; +%m%d%H%M%Y.%S`2012年 07月 10日 星期二 20:13:28 CST[root@RHEL601 tmp]# date `date -d &quot;$((3600*24)) seconds ago&quot; +%m%d%H%M%Y.%S`2012年 07月 09日 星期一 20:13:39 CST[root@RHEL601 tmp]# date `date -d &quot;$((3600*24)) seconds ago&quot; +%m%d%H%M%Y.%S`2012年 07月 08日 星期日 20:14:01 CST[root@RHEL601 tmp]# ntpdate 210.72.145.4419 Jul 20:14:15 ntpdate[26846]: step time server 210.72.145.44 offset 950404.037565 sec 参考资料http://blog.chinaunix.net/uid-22823163-id-3293784.html 有问题可以联系博主，博客主页有邮箱，微博，文章底部有微信公众号等联系方式。 转载请注明出处，谢谢！]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell 输出重定向（> >> 2>&1）]]></title>
    <url>%2F2018%2F01%2F10%2Fshell%20%E8%BE%93%E5%87%BA%E9%87%8D%E5%AE%9A%E5%90%91%EF%BC%88%3E%20%3E%3E%202%3E%261%EF%BC%89%2F</url>
    <content type="text"><![CDATA[shell - 输出重定向（&gt; &gt;&gt; 2&gt;&amp;1） 一、bash中0，1，2bash中0，1，2三个数字分别代表STDIN_FILENO、STDOUT_FILENO、STDERR_FILENO，即标准输入（一般是键盘），标准输出（一般是显示屏，准确的说是用户终端控制台），标准错误（出错信息输出） 二、输入输出重定向所谓重定向输入就是在命令中指定具体的输入来源，譬如 cat &lt; test.c 将test.c重定向为cat命令的输入源。输出重定向是指定具体的输出目标以替换默认的标准输出，譬如ls &gt; 1.txt将ls的结果从标准输出重定向为1.txt文本。有时候会看到如 ls &gt;&gt; 1.txt这类的写法，&gt; 和 &gt;&gt; 的区别在于：&gt; 用于新建而&gt;&gt;用于追加。即ls &gt; 1.txt会新建一个1.txt文件并且将ls的内容输出到新建的1.txt中，而ls &gt;&gt; 1.txt则用在1.txt已经存在，而我们只是想将ls的内容追加到1.txt文本中的时候。 默认输入只有一个（0，STDIN_FILENO），而默认输出有两个（标准输出1 STDOUT_FILENO，标准错误2 STDERR_FILENO）。因此默认情况下，shell输出的错误信息会被输出到2，而普通输出信息会输出到1。但是某些情况下，我们希望在一个终端下看到所有的信息（包括标准输出信息和错误信息），要怎么办呢？对了，你可以使用我们上面讲到的输出重定向。思路有了，怎么写呢？ 非常直观的想法就是2&gt;1（将2重定向到1嘛），行不行呢？试一试就知道了。我们进行以下测试步骤： 123451）mkdir test &amp;&amp; cd test; 创建test文件夹并进入test目录2）touch a.txt b.c c; 创建a.txt b.c c 三个文件3）ls &gt; 1; 按我们的猜测，这句应该是将ls的结果重定向到标准输出，因此效果和直接ls应该一样。但是实际这句执行后，标准输出中并没有任何信息。4）ls; 执行3之后再次ls，则会看到test文件夹中多了一个文件15）cat 1 ; 查看文件1的内容，实际结果为：1 a.txt b.c c 可见步骤3中 ls &gt; 1并不是将ls的结果重定向为标准输出，而是将结果重定向到了一个文件1中。即1在此处不被解释为STDOUT_FILENO，而是文件1。 4、到了此时，你应该也能猜到2&gt;&amp;1的用意了。不错，2&gt;&amp;1就是用来将标准错误2重定向到标准输出1中的。此处1前面的&amp;就是为了让bash将1解释成标准输出而不是文件1。至于最后一个&amp;，则是让bash在后台执行 三、例子🌰1.输出重定向 1.1把标准输出重定向到文件 1234567[~]# echo &quot;hello&quot; &gt; test.sh[~]# cat test.shhello&apos;&gt;&apos;输出方式默认等价&apos;1&gt;&apos;[~]# echo &quot;hello&quot; 1&gt; test.sh[~]# cat test.shhello 输入、输出及标准错误输出主要用于 I/O 的重定向，就是说需要改变他们的默认设置。先看这个例子：1$ ls &gt; ls_result $ ls -l &gt;&gt; ls_result 上面这两个命令分别将 ls 命令的结果输出重定向到 ls_result 文件中和追加到 ls_result 文件中，而不是输出到屏幕上。”&gt;”就是输出（标准输出和标准错误输出）重定向的代表符号，连续两个 “&gt;” 符号，即 “&gt;&gt;” 则表示不清除原来的而追加输出。下面再来看一个稍微复杂的例子：1$ find /home -name lost* 2&gt; err_result 这个命令在 “&gt;” 符号之前多了一个 “2”，”2&gt;” 表示将标准错误输出重定向。由于 /home 目录下有些目录由于权限限制不能访问，因此会产生一些标准错误输出被存放在 err_result 文件中。 大家可以设想一下 find /home -name lost 2&gt;&gt;err_result 命令会产生什么结果？如果直接执行 find /home -name lost &gt; all_result ，其结果是只有标准输出被存入 all_result 文件中，要想让标准错误输出和标准输入一样都被存入到文件中，那该怎么办呢？看下面这个例子：1$ find /home -name lost* &gt; all_result 2&gt;&amp; 1 上面这个例子中将首先将标准错误输出也重定向到标准输出中，再将标准输出重定向到 all_result 这个文件中。这样我们就可以将所有的输出都存储到文件中了。为实现上述功能，还有一种简便的写法如下：1$ find /home -name lost* &gt;&amp; all_result 如果那些出错信息并不重要，下面这个命令可以让你避开众多无用出错信息的干扰：1$ find /home -name lost* 2&gt; /dev/null 同学们回去后还可以再试验一下如下几种重定向方式，看看会出什么结果，为什么？1$ find /home -name lost* &gt; all_result 1&gt;&amp; 2 $ find /home -name lost* 2&gt; all_result 1&gt;&amp; 2 $ find /home -name lost* 2&gt;&amp; 1 &gt; all_result 另外一个非常有用的重定向操作符是 “-“，请看下面这个例子1$ (cd /source/directory &amp;&amp; tar cf - . ) | (cd /dest/directory &amp;&amp; tar xvfp -) 该命令表示把 /source/directory 目录下的所有文件通过压缩和解压，快速的全部移动到 /dest/directory 目录下去，这个命令在 /source/directory 和 /dest/directory 不处在同一个文件系统下时将显示出特别的优势。下面还几种不常见的用法：n&lt;&amp;- 表示将 n 号输入关闭 &lt;&amp;- 表示关闭标准输入（键盘） n&gt;&amp;- 表示将 n 号输出关闭 &gt;&amp;- 表示将标准输出关闭 参考资料http://qinqianshan.com/shell-output-redirection-21/ http://blog.csdn.net/feng27156/article/details/38980543 有问题可以联系博主，博客主页有邮箱，微博，文章底部有微信公众号等联系方式。 转载请注明出处，谢谢！]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell中条件判断if中的-a到-z的意思]]></title>
    <url>%2F2018%2F01%2F10%2Fshell%E4%B8%AD%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%ADif%E4%B8%AD%E7%9A%84-a%E5%88%B0-z%E7%9A%84%E6%84%8F%E6%80%9D%2F</url>
    <content type="text"><![CDATA[shell中条件判断if中的-a到-z的意思 [-a file] 如果file存在则为真 [-b file] 如果file存在且是一个块特殊文件则为真 [-c file] 如果file存在且是一个字特殊文件则为真 [-d file] 如果file文件存在且是一个目录则为真-d前的!是逻辑非例如：1if [ ! -d $lcd_path/$par_date ] 表示后面的那个目录不存在，则执行后面的then操作 [-e file] 如果file文件存在则为真 [-f file] 如果file存在且是一个普通文件则为真 [-g file] 如果file存在且已经设置了SGID则为真（SUID 是 Set User ID, SGID 是 Set Group ID的意思） [-h file] 如果file存在且是一个符号连接则为真 [-k file] 如果file存在且已经设置粘制位则为真当一个目录被设置为”粘制位”(用chmod a+t),则该目录下的文件只能由一、超级管理员删除二、该目录的所有者删除三、该文件的所有者删除也就是说,即便该目录是任何人都可以写,但也只有文件的属主才可以删除文件。具体例子如下：123#ls -dl /tmp drwxrwxrwt 4 root root ......... 注意other位置的t，这便是粘连位。 [-p file] 如果file存在且是一个名字管道（F如果O）则为真管道是linux里面进程间通信的一种方式，其他的还有像信号（signal）、信号量、消息队列、共享内存、套接字（socket）等。 [-r file] 如果file存在且是可读的则为真 [-s file] 如果file存在且大小不为0则为真 [-t FD] 如果文件描述符FD打开且指向一个终端则为真 [-u file] 如果file存在且设置了SUID（set userID）则为真 [-w file] 如果file存在且是可写的则为真 [-x file] 如果file存在且是可执行的则为真 [-O file] 如果file存在且属有效用户ID则为真 [-G file] 如果file存在且属有效用户组则为真 [-L file] 如果file存在且是一个符号连接则为真 [-N file] 如果file存在and has been mod如果ied since it was last read则为真 [-S file] 如果file存在且是一个套接字则为真 [file1 –nt file2] 如果file1 has been changed more recently than file2或者file1 exists and file2 does not则为真 [file1 –ot file2] 如果file1比file2要老，或者file2存在且file1不存在则为真 [file1 –ef file2] 如果file1和file2指向相同的设备和节点号则为真 [-o optionname] 如果shell选项“optionname”开启则为真 [-z string] “string”的长度为零则为真 [-n string] or [string] “string”的长度为非零non-zero则为真 [sting1==string2] 如果2个字符串相同。“=”may be used instead of “==”for strict posix compliance则为真 [string1!=string2] 如果字符串不相等则为真[string1&lt;string2] 如果“string1”sorts before“string2”lexicographically in the current locale则为真 [arg1 OP arg2] “OP”is one of –eq,-ne,-lt,-le,-gt or –ge.These arithmetic binary oprators return true if “arg1”is equal to,not equal to,less than,less than or equal to,greater than,or greater than or equal to“agr2”,respectively.“arg1”and “agr2”are integers. 参考资料http://blog.csdn.net/vergilgeekopen/article/details/69493321 有问题可以联系博主，博客主页有邮箱，微博，文章底部有微信公众号等联系方式。 转载请注明出处，谢谢！]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sublime注册码破解及关闭自动更新]]></title>
    <url>%2F2017%2F09%2F30%2Fsublime%E7%A0%B4%E8%A7%A3%E5%8F%8A%E5%85%B3%E9%97%AD%E8%87%AA%E5%8A%A8%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[sublime - 丰富的文本编辑器，使用技巧 破解方法话不多说，先分享三个注册码！点击最右侧 help add license，添加注册码即可123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354—– BEGIN LICENSE —–Michael BarnesSingle User LicenseEA7E-8213858A353C41 872A0D5C DF9B2950 AFF6F667C458EA6D 8EA3C286 98D1D650 131A97ABAA919AEC EF20E143 B361B1E7 4C8B7F04B085E65E 2F5F5360 8489D422 FB8FC1AA93F6323C FD7F7544 3F39C318 D95E6480FCCC7561 8A4A1741 68FA4223 ADCEDE07200C25BE DBBC4855 C4CFB774 C5EC138C0FEC1CEF D9DCECEC D3A5DAD1 01316C36—— END LICENSE ———– BEGIN LICENSE —–Nicolas HennionSingle User LicenseEA7E-8660758A01AA83 1D668D24 4484AEBC 3B04512C827B0DE5 69E9B07A A39ACCC0 F95F5410729D5639 4C37CECB B2522FB3 8D37FDC172899363 BBA441AC A5F47F08 6CD3B3FECEFB3783 B2E1BA96 71AAF7B4 AFB61B1D0CC513E7 52FF2333 9F726D2C CDE53B4A810C0D4F E1F419A3 CDA0832B 8440565A35BF00F6 4CA9F869 ED10E245 469C233E—— END LICENSE ———– BEGIN LICENSE —–Anthony SansoneSingle User LicenseEA7E-87856328B9A648 42B99D8A F2E3E9E0 16DE076EE218B3DC F3606379 C33C1526 E8B58964B2CB3F63 BDF901BE D31424D2 082891B5F7058694 55FA46D8 EFC11878 0868F093B17CAFE7 63A78881 86B78E38 0F146238BAE22DBB D4EC71A1 0EC2E701 C7F9C6485CF29CA3 1CB14285 19A46991 E9A9867614FD4777 2D8A0AB6 A444EE0D CA009B54—— END LICENSE ———– BEGIN LICENSE —–Alexey PlutalovSingle User LicenseEA7E-8607763DC19CC1 134CDF23 504DC871 2DE5CE55585DC8A6 253BB0D9 637C87A2 D8D0BA85AAE574AD BA7D6DA9 2B9773F2 324C5DEF17830A4E FBCF9D1D 182406E9 F883EA87E585BBA1 2538C270 E2E857C2 194283CA7234FF9E D0392F93 1D16E021 F191491763909E12 203C0169 3F08FFC8 86D06EA873DDAEF0 AC559F30 A6A67947 B60104C6—— END LICENSE —— 关闭自动更新如图打开,添加如下代码12345// Settings in here override those in &quot;Default/Preferences.sublime-settings&quot;,// and are overridden in turn by file type specific settings.&#123; &quot;update_check&quot; : false&#125; 注意：博主添加代码时发现自动更新提醒还是会弹出来，后来发现是没有破解，破解之后就不会弹出来了 转载请注明出处，谢谢！]]></content>
      <categories>
        <category>软件破解</category>
      </categories>
      <tags>
        <tag>sublime</tag>
        <tag>破解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[零基础搭建Hexo炫酷静态页面博客]]></title>
    <url>%2F2017%2F09%2F29%2F%E9%9B%B6%E5%9F%BA%E7%A1%80%E6%90%AD%E5%BB%BAHexo%E7%82%AB%E9%85%B7%E9%9D%99%E6%80%81%E9%A1%B5%E9%9D%A2%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[Hexo - 快速、简洁且高效的博客框架 GitHubPages + hexo 简介最近花了两天的时间搭建了一个博客，使用GitHubPages + hexo 为什么选用GitHubPages + hexo 优点如下： 1.超快速度Node.js 所带来的超快生成速度，让上百个页面在几秒内瞬间完成渲染。 2.支持 MarkdownHexo 支持 GitHub Flavored Markdown 的所有功能，甚至可以整合 Octopress 的大多数插件。 3.一键部署只需一条指令即可部署到 GitHub Pages, Heroku 或其他网站。 4.丰富的插件Hexo 拥有强大的插件系统，安装插件可以让 Hexo 支持 Jade, CoffeeScript。 博客地址 前期准备github账号node.jsnpmhexo注意：（node.js 集成带有npm，因此只要下载 node.js 就可以了） github上创建GitHubPages仓库GitHubPages官方参考地址 注意： 创建仓库的时候仓库名一定严格按照 git用户名.github.io 来命名创建仓库完成之后，在本地创建一个站点文件夹 git用户名.github.io/blog 下载node.js我是在mac系统上搭建的，下载用的终端brew命令，如果其他系统或者没有翻墙，可能会有问题，如果下载失败可以移步Node.js官网，下载最新版本一路安装即可。检测安装是否成功 终端输入 node -v ,npm -v 成功则显示版本号123➜ blog git:(master) ✗ npm -v5.3.0➜ blog git:(master) ✗ 下载 hexohexo官方 有详细的windows和mac用户的安装文档，如果因为防火墙等原因安装失败，请使用下面命令安装，sudo赋予命令最高权限，避免权限不足 1➜ blog git:(master) sudo npm install hexo --no-optional 下载git文档说明，上面有各个平台的git下载安装步骤，按照步骤安装即可 本地关联github仓库git下载安装完成之后，需要跟你的github仓库关联起来，你需要一个私钥和公钥，首先查看本地有没有1234➜ ~ git:(master) ✗ cd ~/.ssh ➜ .ssh git:(master) ✗ lsconfig id_rsa id_rsa.pub known_hosts➜ .ssh git:(master) ✗ 如果没有id_rsa（私钥）和id_rsa.pub（公钥）就需要手动生成一个，执行命令1➜ ~ git:(master) ✗ ssh-keygen 生成双钥，然后把公钥放到github仓库上，点击头像选择settings左侧找到SSH，然后点击New SSH key，把公钥的文本内容粘贴进去，就可以了 部署hexo命令介绍准备工作完成，就可以部署看一下博客界面效果了,先介绍一下hexo常用命令 hexo clean 清除编辑后生成的静态文件，一般部署前都会执行一遍，防止上次部署结果影响新的文件内容 hexo generate 编辑生成静态页面等 hexo server 启动服务 hexo develop 部署到远程github，需要修改配置文件，稍后介绍 基本上用到的就是这些命令，为了方便可以设置快捷指令，执行如下命令，会在跟目录生成一个.bash_profile文件，同时打开.bash_profile进入编辑模式12➜ ~ git:(master) ✗ cd ~➜ ~ git:(master) ✗ vi .bash_profile 然后输入以下内容,进行保存，linux命令123456789101112131415161718192021222324alias hexo clean=hexo calias hexo generate=hexo galias hexo server=hexo salias hexo develop=hexo d~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ -- INSERT -- 执行如下命令，让文件生效1➜ ~ git:(master) ✗ source .bash_profile 至此，快捷指令设置完成 本地部署然后cd进入本地博客根目录，执行命令12345➜ ~ git:(master) ✗ hexo c➜ ~ git:(master) ✗ hexo d➜ ~ git:(master) ✗ hexo sINFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 看到如下信息就可以在本地访问http://localhost:4000/ 查看博客主页了 远程部署部署到元辰github上需要修改一下本地博客配置文件，找到根目录的_config.yml文件，修改以下内容123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:你的github名/你的github名.github.io.git branch: master 然后就可以执行如下命令部署到github1➜ ~ git:(master) ✗ hexo d 看到如下信息说明部署完成，就可以通过https://你的github名.github.io/ 访问你的主页了1INFO Deploy done: git 部署完成至此，一个博客模板搭建完成，hexo还支持丰富的插件，包括RSS订阅，评论系统的接入，文章阅读量，打赏功能，第三方链接，如微博，发邮件等。 接入以上功能，让自己的博客更加炫酷！可以联系博主，博客主页有邮箱，微博，文章底部有微信公众号等联系方式。 转载请注明出处，谢谢！]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
</search>
